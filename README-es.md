[游쥟릖](/README.md "English")
[游뻟릖쓯(/README-cn.md "Simplified Chinese")
[游游엫(/README-ja.md "Japanese")
[游쉻릖젷(/README-it.md "Italian")
[游썷릖윻(/README-ko.md "Korean")
[游游죟(/README-ru.md "Russian")
[游游윻(/README-pt.md "Portuguese")
[游쀯릖](/README-es.md "Spanish")

[![license](https://img.shields.io/badge/license-BSD--3--Clause-blue.svg)](https://img.shields.io/badge/license-BSD--3--Clause-blue.svg)

# Introducci칩n

Esto es una breve introducci칩n a la tecnolog칤a de v칤deo, principalmente dirigido a desarrolladores o ingenieros de software. La intenci칩n es que sea **f치cil de aprender para cualquier persona**. La idea naci칩 durante un [breve taller para personas nuevas en v칤deo](https://docs.google.com/presentation/d/17Z31kEkl_NGJ0M66reqr9_uTG6tI5EDDVXpdPKVuIrs/edit#slide=id.p).

El objetivo es introducir algunos conceptos de v칤deo digital con un **vocabulario simple, muchas figuras y ejemplos pr치cticos** en cuanto sea posible, y dejar disponible este conocimiento. Por favor, si칠ntete libre de enviar correcciones, sugerencias y mejoras.

Habr치n **pr치cticas** que requiere tener instalado **docker** y este repositorio clonado. 

```bash
git clone https://github.com/leandromoreira/digital_video_introduction.git
cd digital_video_introduction
./setup.sh
```
> **ADVERTENCIA**: cuando observes el comando `./s/ffmpeg` o `./s/mediainfo`, significa que estamos ejecutando la **versi칩n contenerizada** del mismo, que a su vez incluye todos los requerimientos necesarios.

Todas las **pr치cticas deber치n ser ejecutadas desde el directorio donde has clonado** este repositorio. Para ejecutar los **ejemplos de jupyter** deber치s primero iniciar el servicio `./s/start_jupyter.sh`, copiar la URL y pegarla en tu navegador.

# Control de cambios

* Detalles sobre sistemas DRM
* Versi칩n 1.0.0 liberada
* Traducci칩n a Chino Simplificado
* Ejemplo de FFmpeg oscilloscope filter
* Traducci칩n a Portugu칠s (Brasil)
* Traducci칩n a Espa침ol

# Tabla de contenidos

- [Introducci칩n](#introducci칩n)
- [Tabla de contenidos](#tabla-de-contenidos)
- [Terminolog칤a B치sica](#terminolog칤a-b치sica)
  * [Otras formas de codificar una imagen a color](#otras-formas-de-codificar-una-imagen-a-color)
  * [Pr치ctica: juguemos con una imagen y colores](#pr치ctica-juguemos-con-una-imagen-y-colores)
  * [DVD es DAR 4:3](#dvd-es-dar-43)
  * [Pr치ctica: Verifiquemos las propiedades del v칤deo](#pr치ctica-verifiquemos-las-propiedades-del-v칤deo)
- [Eliminaci칩n de Redundancias](#eliminaci칩n-de-redundancias)
  * [Colores, Luminancia y nuestros ojos](#colores-luinancia-y-nuestros-ojos)
    + [Modelo de color](#modelo-de-color)
    + [Conversiones entre YCbCr y RGB](#conversiones-entre-ycbcr-y-rgb)
    + [Chroma subsampling](#chroma-subsampling)
    + [Pr치ctica: Observemos el histograma YCbCr](#pr치ctica-observemos-el-histograma-ycbcr)
    + [Color, luma, luminancia, gamma](#color-luma-luminancia-gamma)
    + [Pr치ctica: Observa la intensidad YCbCr](#pr치ctica-observa-la-intensidad-ycbcr)
  * [Tipos de fotogramas](#tipos-de-fotogramas)
    + [I Frame (intra, keyframe)](#i-frame-intra-keyframe)
    + [P Frame (predicted)](#p-frame-predicted)
      - [Pr치ctica: Un v칤deo con un solo I-frame](#pr치ctica-un-v칤deo-con-un-solo-i-frame)
    + [B Frame (bi-predictive)](#b-frame-bi-predictive)
      - [Pr치ctica: Compara v칤deos con B-frame](#pr치ctica-compara-videos-con-b-frame)
    + [Resumen](#resumen)
  * [Redundancia Temporal (inter prediction)](#redundancia-temporal-inter-prediction)
      - [Pr치ctica: Observa los vectores de movimiento](#pr치ctica-observa-los-vectores-de-movimiento)
  * [Redundancia Espacial (intra prediction)](#redundancia-espacial-intra-prediction)
      - [Pr치ctica: Observa las intra predictions](#pr치ctica-observa-las-intra-predictions)
- [쮺칩mo funciona un c칩dec de v칤deo?](#c칩mo-funciona-un-c칩dec-de-v칤deo)
  * [쯈u칠? 쯇or qu칠? 쮺칩mo?](#qu칠-por-qu칠-c칩mo)
  * [Historia](#historia)
    + [El nacimiento de AV1](#el-nacimiento-de-av1)
  * [Un c칩dec gen칠rico](#un-c칩dec-gen칠rico)
  * [1st step - picture partitioning](#1st-step---picture-partitioning)
    + [Hands-on: Check partitions](#hands-on-check-partitions)
  * [2nd step - predictions](#2nd-step---predictions)
  * [3rd step - transform](#3rd-step---transform)
    + [Hands-on: throwing away different coefficients](#hands-on-throwing-away-different-coefficients)
  * [4th step - quantization](#4th-step---quantization)
    + [Hands-on: quantization](#hands-on-quantization)
  * [5th step - entropy coding](#5th-step---entropy-coding)
    + [VLC coding](#vlc-coding)
    + [Arithmetic coding](#arithmetic-coding)
    + [Hands-on: CABAC vs CAVLC](#hands-on-cabac-vs-cavlc)
  * [6th step - bitstream format](#6th-step---bitstream-format)
    + [H.264 bitstream](#h264-bitstream)
    + [Hands-on: Inspect the H.264 bitstream](#hands-on-inspect-the-h264-bitstream)
  * [Review](#review)
  * [How does H.265 achieve a better compression ratio than H.264?](#how-does-h265-achieve-a-better-compression-ratio-than-h264)
- [Online streaming](#online-streaming)
  * [General architecture](#general-architecture)
  * [Progressive download and adaptive streaming](#progressive-download-and-adaptive-streaming)
  * [Content protection](#content-protection)
- [How to use jupyter](#how-to-use-jupyter)
- [Conferences](#conferences)
- [References](#references)

# Terminolog칤a B치sica

Una **imagen** puede ser pensada como una **matriz 2D**. Si pensamos en **colores**, podemos extrapolar este concepto a que una imagen es una **matriz 3D** donde la **dimensi칩n adicional** es usada para indicar la **informaci칩n de color**.

Si elegimos representar esos colores usando los [colores primarios (rojo, verde y azul)](https://es.wikipedia.org/wiki/Color_primario), definimos tres planos: el primero para el **rojo**, el segundo para el **verde**, y el 칰ltimo para el **azul**.

![una imagen es una matriz 3D RGB](/i/image_3d_matrix_rgb.png "Una imagen es una matriz 3D")

Llamaremos a cada punto en esta matriz **un p칤xel** (del ingl칠s, *picture element*). Un p칤xel representa la **intensidad** (usualmente un valor num칠rico) de un color dado. Por ejemplo, un **p칤xel rojo** significa 0 de verde, 0 de azul y el m치ximo de rojo. El **p칤xel de color rosa** puede formarse de la combinaci칩n de estos tres colores. Usando la representaci칩n num칠rica con un rango desde 0 a 255, el p칤xel rosa es definido como **Rojo=255, Verde=192 y Azul=203**.

> #### Otras formas de codificar una imagen a color
> Otros modelos pueden ser utilizados para representar los colores que forman una imagen. Por ejemplo, podemos usar una paleta indexada donde necesitaremos solamente un byte para representar cada p칤xel en vez de los 3 necesarios cuando usamos el modelo RGB. En un modelo como este, podemos utilizar una matriz 2D para representar nuestros colores, esto nos ayudar칤a a ahorrar memoria aunque tendr칤amos menos colores para representar.
>
> ![NES palette](/i/nes-color-palette.png "NES palette")

Observemos la figura que se encuentra a continuaci칩n. La primer cara muestra todos los colores utilizados. Mientras las dem치s muestran a intencidad de cada plano rojo, verde y azul (representado en escala de grises).

![Intensidad de los canales RGB](/i/rgb_channels_intensity.png "Intensidad de los canales RGB")

Podemos ver que el **color rojo** es el que **contribuye m치s** (las partes m치s brillantes en la segunda cara) mientras que en la cuarta cara observamos que el **color azul** su contribuci칩n se **limita a los ojos de Mario** y parte de su ropa. Tambi칠n podemos observar como **todos los planos no contribuyen demasiado** (partes oscuras) al **bigote de Mario**.

Cada intensidad de color requiere de una cantidad de bits para ser representada, esa cantidad es conocida como **bit depth**. Digamos que utilizamos **8 bits** (aceptando valores entre 0 y 255) por color (plano), entonces tendremos una **color depth** (profundidad de color) de **24 bits** (8 bits * 3 planos R/G/B), por lo que podemos inferir que tenemos disponibles 2^24 colores diferentes.

> **Es asombroso** aprender [como una imagen es capturada desde el Mundo a los bits](http://www.cambridgeincolour.com/tutorials/camera-sensors.htm).

Otra propiedad de una images es la **resoluci칩n**, que es el n칰mero de p칤xeles en una dimensi칩n. Frecuentemente es presentado como ancho 칑 alto (*width 칑 height*), por ejemplo, la siguiente imagen es **4칑4**.

![image resolution](/i/resolution.png "image resolution")

> #### Pr치ctica: juguemos con una imagen y colores
> Puedes [jugar con una imagen y colores](/image_as_3d_array.ipynb) usando [jupyter](#how-to-use-jupyter) (python, numpy, matplotlib, etc).
>
> Tambi칠n puedes aprender [c칩mo funcionan los filtros de im치genes (edge detection, sharpen, blur...)](/filters_are_easy.ipynb).

Trabajando con im치genes o v칤deos, te encontrar치s con otra propiedad llamada **aspect ratio** (relaci칩n de aspecto) que simplemente describe la relaci칩n proporcional que existe entre el ancho y alto de una imagen o p칤xel.

Cuando las personas dicen que una pel칤cula o imagen es **16x9** usualmente se est치n refiriendo a la relaci칩n de aspecto de la pantalla (**Display Aspect Ratio (DAR)**), sin embargo podemos tener diferentes formas para cada p칤xel, a ello le llamamos relaci칩n de aspecto del p칤xel (**Pixel Aspect Ratio (PAR)**).

![display aspect ratio](/i/DAR.png "display aspect ratio")

![pixel aspect ratio](/i/PAR.png "pixel aspect ratio")

> #### DVD es DAR 4:3
> La resoluci칩n del formato de DVD es 704x480, igualmente mantiente una relaci칩n de aspecto (DAR) de 4:3 porque tiene PAR de 10:11 (704x10/480x11)

Finalmente, podemos definir a un **v칤deo** como una **sucesi칩n de *n* fotogramas** en el **tiempo**, la cual la podemos definir como otra dimensi칩n, *n* es el *frame rate* o fotogramas por segundo (**FPS**, del ingl칠s *frames per second*).

![video](/i/video.png "video")

El n칰mero necesario de bits por segundo para mostrar un v칤deo es llamado **bit rate**.

> bit rate = ancho * alto * bit depth * FPS

Por ejemplo, un v칤deo de 30 fotogramas por segundo, 24 bits por p칤xel, 480x240 de resoluci칩n necesitar칤a de **82,944,000 bits por segundo** o 82.944 Mbps (30x480x240x24) si no queremos aplicar ning칰n tipo de compresi칩n.

Cuando el **bit rate** es casi constante es llamado bit rate constante (**CBR**, del ingl칠s *constant bit rate*), pero tambi칠n puede ser variable por lo que lo llamaremos bit rate variable (**VBR**, del ingl칠s *variable bit rate*).

> La siguiente gr치fica muestra como utilizando VBR no se necesita gastar demasiados bits para un fotograma es negro.
>
> ![constrained vbr](/i/vbr.png "constrained vbr")

En otros tiempos, unos ingenieros idearon una t칠cnica para duplicar la cantidad de fotogramas por segundo percividos en una pantalla **sin consumir ancho de banda adicional**. Esta t칠cnica es conocida como **v칤deo entrelazado** (*interlaced video*); b치sicamente env칤a la mitad de la pantalla en un "fotograma" y la otra mitad en el siguiente "fotograma".  

Hoy en d칤a, las pantallas representan im치genes principalmente utilizando la **t칠cnica de escaneo progresivo** (*progressive v칤deo*). El v칤deo progresivo es una forma de mostrar, almacenar o transmitir im치genes en movimiento en la que todas las l칤neas de cada fotograma se dibujan en secuencia.

![entrelazado vs. progresivo](/i/interlaced_vs_progressive.png "entrelazado vs. progresivo")

Ahora ya tenemos una idea acerca de c칩mo una **imagen** es representada digitalmente, c칩mo sus **colores** son codificados, cu치ntos **bits por segundo** necesitamos para mostrar un v칤deo, si es constante (CBR) o variable (VBR), con una **resoluci칩n** dada a determinado **frame rate** y otros t칠rminos como entrelazado, PAR, etc.

> #### Pr치ctica: Verifiquemos las propiedades del v칤deo
> Puedes [verificar la mayor칤a de las propiedades mencionadas con ffmpeg o mediainfo.](https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#inspect-stream)

# Eliminaci칩n de Redundancias

Aprendimos que no es factible utilizar v칤deo sin ninguna compresi칩n; **un solo v칤deo de una hora** a una resoluci칩n de 720p y 30fps **requerir칤a 278GB<sup>*</sup>**. Dado que **utilizar 칰nicamente algoritmos de compresi칩n de datos sin p칠rdida** como DEFLATE (utilizado en PKZIP, Gzip y PNG), **no disminuir칤a** suficientemente el ancho de banda necesario, debemos encontrar otras formas de comprimir el v칤deo.

> <sup>*</sup> Encontramos este n칰mero multiplicando 1280 x 720 x 24 x 30 x 3600 (ancho, alto, bits por p칤xel, fps y tiempo en segundos).

Para hacerlo, podemos **aprovechar c칩mo funciona nuestra visi칩n**. Somos mejores para distinguir el brillo que los colores, las **repeticiones en el tiempo**, un v칤deo contiene muchas im치genes con pocos cambios, y las **repeticiones dentro de la imagen**, cada fotograma tambi칠n contiene muchas 치reas que utilizan colores iguales o similares.

## Colores, Luminancia y nuestros ojos
Nuestros ojos son [m치s sensibles al brillo que a los colores](http://vanseodesign.com/web-design/color-luminance/), puedes comprobarlo por ti mismo, mira esta imagen.

![luminance vs color](/i/luminance_vs_color.png "luminance vs color")

Si no puedes ver que los colores de los **cuadrados A y B son id칠nticos** en el lado izquierdo, no te preocupes, es nuestro cerebro jug치ndonos una pasada para que **prestemos m치s atenci칩n a la luz y la oscuridad que al color**. Hay un conector, del mismo color, en el lado derecho para que nosotros (nuestro cerebro) podamos identificar f치cilmente que, de hecho, son del mismo color.

> **Explicaci칩n simplista de c칩mo funcionan nuestros ojos**
> El [ojo es un 칩rgano complejo](http://www.biologymad.com/nervoussystem/eyenotes.htm), compuesto por muchas partes, pero principalmente nos interesan las c칠lulas de conos y bastones. El ojo [contiene alrededor de 120 millones de c칠lulas de bastones y 6 millones de c칠lulas de conos](https://en.wikipedia.org/wiki/Photoreceptor_cell).
>
> Para **simplificar absurdamente**, intentemos relacionar los colores y el brillo con las funciones de las partes del ojo. Los **[bastones](https://es.wikipedia.org/wiki/Bast%C3%B3n_(c%C3%A9lula)) son principalmente responsables del brillo**, mientras que los **[conos](https://es.wikipedia.org/wiki/Cono_(c%C3%A9lula)) son responsables del color**. Hay tres tipos de conos, cada uno con un pigmento diferente, a saber: [Tipo S (azul), Tipo M (verde) y Tipo L (rojos)](https://upload.wikimedia.org/wikipedia/commons/1/1e/Cones_SMJ2_E.svg).
>
> Dado que tenemos muchas m치s c칠lulas de bastones (brillo) que c칠lulas de conos (color), se puede inferir que somos m치s capaces de distinguir el contraste entre la luz y la oscuridad que los colores.
>
> ![eyes composition](/i/eyes.jpg "eyes composition")
>
> **Funciones de sensibilidad al contraste**
>
> Los investigadores de psicolog칤a experimental y muchas otras disciplinas han desarrollado varias teor칤as sobre la visi칩n humana. Una de ellas se llama funciones de sensibilidad al contraste. Est치n relacionadas con el espacio y el tiempo de la luz y su valor se presenta en una luz inicial dada, cu치nto cambio se requiere antes de que un observador informe que ha habido un cambio. Observa el plural de la palabra "funci칩n", esto se debe a que podemos medir las funciones de sensibilidad al contraste no solo con blanco y negro, sino tambi칠n con colores. El resultado de estos experimentos muestra que en la mayor칤a de los casos nuestros ojos son m치s sensibles al brillo que al color.

Una vez que sabemos que somos m치s sensibles a la **luminancia** (*luma*, el brillo en una imagen), podemos aprovecharlo.
### Modelo de color

Primero aprendimos [c칩mo funcionan las im치genes en color](#terminolog칤a-b치sica) utilizando el **modelo RGB**, pero existen otros modelos tambi칠n. De hecho, hay un modelo que separa la luminancia (brillo) de la crominancia (colores) y se conoce como **YCbCr**<sup>*</sup>.

> <sup>*</sup> hay m치s modelos que hacen la misma separaci칩n.

Este modelo de color utiliza **Y** para representar el brillo y dos canales de color, **Cb** (croma azul) y **Cr** (croma rojo). El [YCbCr](https://es.wikipedia.org/wiki/YCbCr) se puede derivar a partir de RGB y tambi칠n se puede convertir de nuevo a RGB. Utilizando este modelo, podemos crear im치genes a todo color, como podemos ver a continuaci칩n.

![ycbcr example](/i/ycbcr.png "ycbcr example")

### Conversiones entre YCbCr y RGB

Algunos pueden preguntarse, 쯖칩mo podemos producir **todos los colores sin usar el verde**?

Para responder a esta pregunta, vamos a realizar una conversi칩n de RGB a YCbCr. Utilizaremos los coeficientes del **[est치ndar BT.601](https://en.wikipedia.org/wiki/Rec._601)** que fue recomendado por el **[grupo ITU-R<sup>*</sup>](https://en.wikipedia.org/wiki/ITU-R)**. El primer paso es **calcular la luminancia**, utilizaremos las constantes sugeridas por la ITU y sustituiremos los valores RGB.

```
Y = 0.299R + 0.587G + 0.114B
```

Una vez obtenida la luminancia, podemos **separar los colores** (croma azul y croma rojo):

```
Cb = 0.564(B - Y)
Cr = 0.713(R - Y)
```

Y tambi칠n podemos **covertirlo de vuelta** e incluse obtener el **verde utilizando YCbCr**.

```
R = Y + 1.402Cr
B = Y + 1.772Cb
G = Y - 0.344Cb - 0.714Cr
```

> <sup>*</sup> Los grupos y est치ndares son comunes en el v칤deo digital y suelen definir cu치les son los est치ndares, por ejemplo, [쯤u칠 es 4K? 쯤u칠 FPS debemos usar? resoluci칩n? 쯠odelo de color?](https://en.wikipedia.org/wiki/Rec._2020)

Generalmente, las **pantallas** (monitores, televisores, etc.) utilizan **solo el modelo RGB**, organizado de diferentes maneras, como se muestra a continuaci칩n:

![pixel geometry](/i/new_pixel_geometry.jpg "pixel geometry")

### Chroma subsampling

Con la imagen representada en componentes de luminancia y crominancia, podemos aprovechar la mayor sensibilidad del sistema visual humano a la resoluci칩n de luminancia en lugar de la crominancia para eliminar selectivamente informaci칩n. El **chroma subsampling** es la t칠cnica de codificar im치genes utilizando **menor resoluci칩n para la crominancia que para la luminancia**.

![ycbcr subsampling resolutions](/i/ycbcr_subsampling_resolution.png "ycbcr subsampling resolutions")


쮺u치nto debemos reducir la resoluci칩n de la crominancia? Resulta que ya existen algunos esquemas que describen c칩mo manejar la resoluci칩n y la combinaci칩n (`color final = Y + Cb + Cr`).

Estos esquemas se conocen como sistemas de *subsampling* y se expresan como una relaci칩n de 3 partes: `a:x:y`, que define la resoluci칩n de la crominancia en relaci칩n con un bloque `a x 2` de p칤xeles de luminancia.

 * `a` es la referencia de muestreo horizontal (generalmente 4).
 * `x` es el n칰mero de muestras de crominancia en la primera fila de `a` p칤xeles (resoluci칩n horizontal en relaci칩n con `a`).
 * `y` es el n칰mero de cambios de muestras de crominancia entre la primera y segunda filas de a p칤xeles.

> Una excepci칩n a esto es el 4:1:0, que proporciona una sola muestra de crominancia dentro de cada bloque de `4 x 4` p칤xeles de resoluci칩n de luminancia.

Los esquemas comunes utilizados en c칩decs modernos son: **4:4:4** *(sin subsampling)*, **4:2:2, 4:1:1, 4:2:0, 4:1:0 y 3:1:1**.

> Puedes seguir algunas discusiones para [aprender m치s sobre el Chroma Subsampling](https://github.com/leandromoreira/digital_video_introduction/issues?q=YCbCr).

> **Fusi칩n YCbCr 4:2:0**
>
> Aqu칤 tienes una parte fusionada de una imagen utilizando YCbCr 4:2:0, observa que solo utilizamos 12 bits por p칤xel.
>
> ![YCbCr 4:2:0 merge](/i/ycbcr_420_merge.png "YCbCr 4:2:0 merge")

Puedes ver la misma imagen codificada por los principales tipos de *chroma subsampling*, las im치genes en la primera fila son las YCbCr finales, mientras que la 칰ltima fila de im치genes muestra la resoluci칩n de crominancia. Es realmente una gran ganancia con una p칠rdida tan peque침a.

![chroma subsampling examples](/i/chroma_subsampling_examples.jpg "chroma subsampling examples")

Anteriormente hab칤amos calculado que necesit치bamos [278 GB de almacenamiento para mantener un archivo de video con una hora de resoluci칩n de 720p y 30 fps](#eliminaci칩n-de-redundancias). Si usamos **YCbCr 4:2:0**, podemos reducir **este tama침o a la mitad (139 GB)**<sup>*</sup>, pero a칰n est치 lejos del ideal.

> <sup>*</sup> encontramos este valor multiplicando el ancho, alto, bits por p칤xel y fps. Anteriormente necesit치bamos 24 bits, ahora solo necesitamos 12.

<br/>

> ### Pr치ctica: Observemos el histograma YCbCr
> Puedes [observar el histograma YCbCr con ffmpeg](/encoding_pratical_examples.md#generates-yuv-histogram). Esta excena tiene una mayor contribuci칩n de azul la cual se muestra en el [histograma](https://es.wikipedia.org/wiki/Histograma).
>
> ![ycbcr color histogram](/i/yuv_histogram.png "ycbcr color histogram")

### Color, luma, luminancia, gamma

Mira este incre칤ble video que explica qu칠 es la luma y aprende sobre luminancia, gamma y color.
**V칤deo en Ingl칠s**
[![Analog Luma - A history and explanation of video](http://img.youtube.com/vi/Ymt47wXUDEU/0.jpg)](http://www.youtube.com/watch?v=Ymt47wXUDEU)

> ### Pr치ctica: Observa la intensidad YCbCr
> Puedes visualizar la intensidad Y (luma) para una l칤nea espec칤fica de un v칤deo utilizando el [filtro de osciloscopio de FFmpeg](https://ffmpeg.org/ffmpeg-filters.html#oscilloscope).
> ```bash
> ffplay -f lavfi -i 'testsrc2=size=1280x720:rate=30000/1001,format=yuv420p' -vf oscilloscope=x=0.5:y=200/720:s=1:c=1
> ```
> ![y color oscilloscope](/i/ffmpeg_oscilloscope.png "y color oscilloscope")

## Tipos de fotogramas

Ahora podemos continuar y tratar de eliminar la **redundancia en el tiempo**, pero antes de hacerlo, establezcamos algunas terminolog칤as b치sicas. Supongamos que tenemos una pel칤cula con 30 fps. Aqu칤 est치n sus primeros 4 fotogramas.

![ball 1](/i/smw_background_ball_1.png "ball 1") ![ball 2](/i/smw_background_ball_2.png "ball 2") ![ball 3](/i/smw_background_ball_3.png "ball 3")
![ball 4](/i/smw_background_ball_4.png "ball 4")

Podemos ver **muchas repeticiones** dentro de los fotogramas, como **el fondo azul**, que no cambia del fotograma 0 al fotograma 3. Para abordar este problema, podemos **categorizarlos abstractamente** en tres tipos de fotogramas.

### I Frame (intra, keyframe)

Un *I-frame* (*reference*, *keyframe*, *intra*, en espa침ol fotograma o cuadro de referencia) es un **fotograma aut칩nomo**. No depende de nada para ser renderizado, un *I-frame* se parece a una fotograf칤a est치tica. Por lo general, el primer fotograma es un *I-frame*, pero veremos *I-frames* insertados regularmente entre otros tipos de fotogramas.

![ball 1](/i/smw_background_ball_1.png "ball 1")

### P Frame (predicted)

Un *P-frame* (en espa침ol, fotograma o cuadro predictivo) aprovecha el hecho de que casi siempre l**a imagen actual se puede renderizar utilizando el fotograma anterior**. Por ejemplo, en el segundo fotograma, el 칰nico cambio fue que la pelota se movi칩 hacia adelante. Podemos **reconstruir el fotograma 1, utilizando solo la diferencia y haciendo referencia al fotograma anterior**.

![ball 1](/i/smw_background_ball_1.png "ball 1") <-  ![ball 2](/i/smw_background_ball_2_diff.png "ball 2")

> #### Pr치ctica: Un v칤deo con un solo I-frame
> Dado que un fotograma P utiliza menos datos, 쯣or qu칠 no podemos codificar un [video entero con un solo *I-frame* y todos los dem치s siendo *P-frames*?](/encoding_pratical_examples.md#1-i-frame-and-the-rest-p-frames)
>
> Despu칠s de codificar este v칤deo, comienza a verlo y **busca una parte avanzada** del v칤deo; notar치s que **toma algo de tiempo** llegar realmente a esa parte. Esto se debe a que un ***P-frame* necesita un fotograma de referencia** (como un *I-frame*, por ejemplo) para renderizarse.
>
> Otra prueba r치pida que puedes hacer es codificar un v칤deo utilizando un solo *I-frame* y luego [codificarlo insertando un *I-frame* cada 2 segundos](/encoding_pratical_examples.md#1-i-frames-per-second-vs-05-i-frames-per-second) y **comprobar el tama침o de cada versi칩n**.

### B Frame (bi-predictive)

쯈u칠 pasa si hacemos referencia a los fotogramas pasados y futuros para proporcionar una compresi칩n a칰n mejor? Eso es b치sicamente lo que hace un *B-frame*.

![ball 1](/i/smw_background_ball_1.png "ball 1") <-  ![ball 2](/i/smw_background_ball_2_diff.png "ball 2") -> ![ball 3](/i/smw_background_ball_3.png "ball 3")

> #### Pr치ctica: Compara v칤deos con B-frame
> Puedes generar dos versiones, una con *B-frames* y otra sin ning칰n [*B-frame* en absoluto](/encoding_pratical_examples.md#no-b-frames-at-all) y verificar el tama침o del archivo y la calidad.

### Resumen

Estos tipos de fotogramas se utilizan para **proporcionar una mejor compresi칩n**. Veremos c칩mo sucede esto en la pr칩xima secci칩n, pero por ahora podemos pensar en un ***I-frame* como costoso, un *P-frame* como m치s econ칩mico y el m치s econ칩mico es el *B-frame*.**

![frame types example](/i/frame_types.png "frame types example")

## Redundancia temporal (inter prediction)

Vamos a explorar las opciones que tenemos para reducir las **repeticiones en el tiempo**, este tipo de redundancia se puede resolver con t칠cnicas de **interpredicci칩n**.

Intentaremos **utilizar menos bits** para codificar la secuencia de los fotogramas 0 y 1.

![original frames](/i/original_frames.png "original frames")

Una cosa que podemos hacer es una resta, simplemente **restamos el fotograma 1 del fotograma 0** y obtenemos lo que necesitamos para **codificar el residual**.

![delta frames](/i/difference_frames.png "delta frames")

Pero, 쯤u칠 pasa si te digo que hay un **m칠todo mejor** que utiliza incluso menos bits? Primero, tratemos el `fotograma 0` como una colecci칩n de particiones bien definidas y luego intentaremos emparejar los bloques del `fotograma 0` en el `fotograma 1`. Podemos pensar en ello como una **estimaci칩n de movimiento**.

> ### Wikipedia - compensaci칩n de movimiento por bloques
> "La **compensaci칩n de movimiento por bloques** divide el fotograma actual en bloques no superpuestos, y el vector de compensaci칩n de movimiento **indica de d칩nde provienen esos bloques** (una idea err칩nea com칰n es que el fotograma anterior se divide en bloques no superpuestos y los vectores de compensaci칩n de movimiento indican hacia d칩nde se mueven esos bloques). Los bloques de origen suelen superponerse en el fotograma fuente. Algunos algoritmos de compresi칩n de v칤deo ensamblan el fotograma actual a partir de piezas de varios fotogramas previamente transmitidos."

![delta frames](/i/original_frames_motion_estimation.png "delta frames")

Podr칤amos estimar que la pelota se movi칩 de `x=0, y=25` a `x=6, y=26`, los valores de **x** e **y** son los **vectores de movimiento**. Un **paso adicional** que podemos dar para ahorrar bits es **codificar solo la diferencia del vector de movimiento** entre la 칰ltima posici칩n del bloque y la predicci칩n, por lo que el vector de movimiento final ser칤a `x=6 (6-0), y=1 (26-25)`.

> En una situaci칩n del mundo real, esta **pelota se dividir칤a en n particiones**, pero el proceso es el mismo.

Los objetos en el fotograma se **mueven de manera tridimensional**, la pelota puede volverse m치s peque침a cuando se mueve al fondo. Es normal que no encontremos una coincidencia perfecta con el bloque que intentamos encontrar. Aqu칤 hay una vista superpuesta de nuestra estimaci칩n frente a la imagen real.

![motion estimation](/i/motion_estimation.png "motion estimation")

Pero podemos ver que cuando aplicamos la **estimaci칩n de movimiento**, los **datos a codificar son m치s peque침os** que cuando simplemente usamos t칠cnicas de fotograma delta.

![motion estimation vs delta ](/i/comparison_delta_vs_motion_estimation.png "motion estimation delta")

> ### C칩mo se ver칤a la compensaci칩n de movimiento real
> Esta t칠cnica se aplica a todos los bloques, muy a menudo una pelota se dividir칤a en m치s de un bloque.
>  ![real world motion compensation](/i/real_world_motion_compensation.png "real world motion compensation")
> Fuente: https://web.stanford.edu/class/ee398a/handouts/lectures/EE398a_MotionEstimation_2012.pdf

Puedes [experimentar con estos conceptos utilizando jupyter](/frame_difference_vs_motion_estimation_plus_residual.ipynb).

> #### Pr치ctica: Observa los vectores de movimiento
> Podemos [generar un v칤deo con la interpredicci칩n (vectores de movimiento) con ffmpeg.](/encoding_pratical_examples.md#generate-debug-video)
>
> ![inter prediction (motion vectors) with ffmpeg](/i/motion_vectors_ffmpeg.png "inter prediction (motion vectors) with ffmpeg")
>
> O podemos usar el [Intel Video Pro Analyzer](https://software.intel.com/en-us/intel-video-pro-analyzer) (que es de pago, pero hay una versi칩n de prueba gratuita que te limita a trabajar solo con los primeros 10 fotogramas).
>
> ![inter prediction intel video pro analyzer](/i/inter_prediction_intel_video_pro_analyzer.png "inter prediction intel video pro analyzer")

## Redundancia Espacial (intra prediction)

Si analizamos **cada fotograma** en un v칤deo, veremos que tambi칠n hay **muchas 치reas que est치n correlacionadas**.

![](/i/repetitions_in_space.png)

Vamos a trav칠s de un ejemplo. Esta escena est치 compuesta principalmente por colores azules y blancos.

![](/i/smw_bg.png)

Este es un `I-frame` y **no podemos usar fotogramas anteriores** para hacer una predicci칩n, pero a칰n podemos comprimirlo. Codificaremos la selecci칩n de bloques en rojo. Si **observamos a sus vecinos**, podemos **estimar** que hay una **tendencia de colores a su alrededor**.

![](/i/smw_bg_block.png)

Vamos a **predecir** que el fotograma continuar치 **extendiendo los colores verticalmente**, lo que significa que los colores de los **p칤xeles desconocidos mantendr치n los valores de sus vecinos**.

![](/i/smw_bg_prediction.png)

Nuestra **predicci칩n puede ser incorrecta**, por esa raz칩n necesitamos aplicar esta t칠cnica (**intra prediction**) y luego **restar los valores reales**, lo que nos da el bloque residual, lo que resulta en una matriz mucho m치s compresible en comparaci칩n con la original.

![](/i/smw_residual.png)

Existen muchos tipos diferentes de este tipo de predicci칩n. La que se muestra aqu칤 es una forma de predicci칩n plana recta, donde los p칤xeles de la fila superior del bloque se copian fila por fila dentro del bloque. La predicci칩n plana tambi칠n puede tener una componente angular, donde se utilizan p칤xeles tanto de la izquierda como de la parte superior para ayudar a predecir el bloque actual. Y tambi칠n existe la predicci칩n DC, que implica tomar el promedio de las muestras justo arriba y a la izquierda del bloque.

> #### Pr치ctica: Observa las intra predictions
> Puedes [generar con ffmpeg un v칤deo con macrobloques y sus predicciones.](/encoding_pratical_examples.md#generate-debug-video) Por favor verifica la documentaci칩n de ffmpeg para comprender el [significado de cada bloque de color](https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors#AnalyzingMacroblockTypes).
>
> ![intra prediction (macro blocks) with ffmpeg](/i/macro_blocks_ffmpeg.png "inter prediction (motion vectors) with ffmpeg")
>
> O puedes usar [Intel Video Pro Analyzer](https://software.intel.com/en-us/intel-video-pro-analyzer) (que es de pago, pero hay una versi칩n de prueba gratuita que te limita a trabajar solo con los primeros 10 fotogramas).
>
> ![intra prediction intel video pro analyzer](/i/intra_prediction_intel_video_pro_analyzer.png "intra prediction intel video pro analyzer")

# 쮺칩mo funciona un c칩dec de v칤deo?

## 쯈u칠? 쯇or qu칠? 쮺칩mo?

**쯈u칠?** Es un software o hardware que comprime o descomprime v칤deo digital. **쯇or qu칠?** El mercado y la sociedad demandan v칤deos de mayor calidad con ancho de banda o almacenamiento limitados. Recuerdas cuando [calculamos el ancho de banda necesario](#terminolog칤a-b치sica) para 30 fps, 24 bits por p칤xel, resoluci칩n de un v칤deo de 480x240? Fueron **82.944 Mbps** sin aplicar compresi칩n. Es la 칰nica forma de entregar HD/FullHD/4K en televisores e Internet. **쮺칩mo?** Echaremos un vistazo breve a las t칠cnicas principales aqu칤.

> **CODEC vs Contenedor**
>
> Un error com칰n que cometen los principiantes es confundir el CODEC de v칤deo digital y el [contenedor de v칤deo digital](https://en.wikipedia.org/wiki/Container_format). Podemos pensar en los **contenedores** como un formato que contiene metadatos del v칤deo (y posiblemente tambi칠n audio), y el **v칤deo comprimido** se puede ver como su contenido.
>
> Por lo general, la extensi칩n de un archivo de v칤deo define su contenedor de v칤deo. Por ejemplo, el archivo `video.mp4` probablemente es un contenedor **[MPEG-4 Part 14](https://en.wikipedia.org/wiki/MP4_file_format)** y un archivo llamado `video.mkv` probablemente es un **[matroska](https://en.wikipedia.org/wiki/Matroska)**. Para estar completamente seguro sobre el c칩dec y el formato del contenedor, podemos usar [ffmpeg o mediainfo](/encoding_pratical_examples.md#inspect-stream).

## Historia

Antes de adentrarnos en el funcionamiento interno de un c칩dec gen칠rico, echemos un vistazo atr치s para comprender un poco mejor algunos c칩decs de v칤deo antiguos.

El c칩dec de v칤deo [H.261](https://en.wikipedia.org/wiki/H.261) naci칩 en 1990 (t칠cnicamente en 1988) y fue dise침ado para trabajar con **tasas de datos de 64 kbit/s**. Ya utilizaba ideas como el *chroma subsampling*, el macrobloque, etc. En el a침o 1995, se public칩 el est치ndar de c칩dec de v칤deo **H.263** y continu칩 siendo extendido hasta 2001.

En 2003 se complet칩 la primera versi칩n de **H.264/AVC**. En el mismo a침o, **On2 Technologies** (anteriormente conocida como Duck Corporation) lanz칩 su c칩dec de v칤deo como una compresi칩n de v칤deo con p칠rdida **libre de regal칤as** llamada **VP3**. En 2008, **Google compr칩** esta empresa y lanz칩 **VP8** en el mismo a침o. En diciembre de 2012, Google lanz칩 **VP9**, el cual es **compatible con aproximadamente el  del mercado de navegadores** (incluyendo m칩viles).

  **[AV1](https://en.wikipedia.org/wiki/AOMedia_Video_1)** es un nuevo c칩dec de v칤deo **libre de regal칤as** y de c칩digo abierto que est치 siendo dise침ado por la [Alliance for Open Media (AOMedia)](http://aomedia.org/), la cual est치 compuesta por **empresas como Google, Mozilla, Microsoft, Amazon, Netflix, AMD, ARM, NVidia, Intel y Cisco**, entre otras. La **primera versi칩n**, 0.1.0 del c칩dec de referencia, **se public칩 el 7 de abril de 2016**.

![codec history timeline](/i/codec_history_timeline.png "codec history timeline")

> #### El nacimiento de AV1
>
> A principios de 2015, Google estaba trabajando en [VP10](https://en.wikipedia.org/wiki/VP9#Successor:_from_VP10_to_AV1), Xiph (Mozilla) estaba trabajando en [Daala](https://xiph.org/daala/) y Cisco lanz칩 su c칩dec de video libre de regal칤as llamado [Thor](https://tools.ietf.org/html/draft-fuldseth-netvc-thor-03).
>
> Entonces, MPEG LA anunci칩 por primera vez l칤mites anuales para HEVC (H.265) y tarifas 8 veces m치s altas que H.264, pero pronto cambiaron las reglas nuevamente:
> * **sin l칤mite anual**,
> * **tarifa por contenido** (0.5% de los ingresos) y
> * **tarifas por unidad aproximadamente 10 veces m치s altas que H.264**.
>
> La [Alliance for Open Media](http://aomedia.org/about/) fue creada por empresas de fabricantes de hardware (Intel, AMD, ARM, Nvidia, Cisco), distribuci칩n de contenido (Google, Netflix, Amazon), mantenimiento de navegadores (Google, Mozilla) y otros.
>
> Las empresas ten칤an un objetivo com칰n, un c칩dec de v칤deo libre de regal칤as, y as칤 naci칩 AV1 con una [licencia de patente mucho m치s simple](http://aomedia.org/license/patent/). **Timothy B. Terriberry** hizo una excelente presentaci칩n, que es la fuente de esta secci칩n, sobre la [concepci칩n de AV1, el modelo de licencia y su estado actual](https://www.youtube.com/watch?v=lzPaldsmJbk).
>
> Te sorprender치 saber que puedes **analizar el c칩dec AV1 a trav칠s de tu navegador**, visita https://arewecompressedyet.com/analyzer/
>
> ![av1 browser analyzer](/i/av1_browser_analyzer.png "av1 browser analyzer")
>
> PD: Si deseas aprender m치s sobre la historia de los c칩decs, debes comprender los conceptos b치sicos detr치s de las [patentes de compresi칩n de v칤deo](https://www.vcodex.com/video-compression-patents/).

## Un c칩dec gen칠rico

Vamos a presentar los **mecanismos principales detr치s de un c칩dec de v칤deo gen칠rico**, pero la mayor칤a de estos conceptos son 칰tiles y se utilizan en c칩decs modernos como VP9, AV1 y HEVC. Aseg칰rate de entender que vamos a simplificar las cosas MUCHO. A veces usaremos un ejemplo real (principalmente H.264) para demostrar una t칠cnica.

## 1st step - picture partitioning

The first step is to **divide the frame** into several **partitions, sub-partitions** and beyond.

![picture partitioning](/i/picture_partitioning.png "picture partitioning")

**But why?** There are many reasons, for instance, when we split the picture we can work the predictions more precisely, using small partitions for the small moving parts while using bigger partitions to a static background.

Usually, the CODECs **organize these partitions** into slices (or tiles), macro (or coding tree units) and many sub-partitions. The max size of these partitions varies, HEVC sets 64x64 while AVC uses 16x16 but the sub-partitions can reach sizes of 4x4.

Remember that we learned how **frames are typed**?! Well, you can **apply those ideas to blocks** too, therefore we can have I-Slice, B-Slice, I-Macroblock and etc.

> ### Hands-on: Check partitions
> We can also use the [Intel Video Pro Analyzer](https://software.intel.com/en-us/intel-video-pro-analyzer) (which is paid but there is a free trial version which limits you to only work with the first 10 frames). Here are [VP9 partitions](/encoding_pratical_examples.md#transcoding) analyzed.
>
> ![VP9 partitions view intel video pro analyzer ](/i/paritions_view_intel_video_pro_analyzer.png "VP9 partitions view intel video pro analyzer")

## 2nd step - predictions

Once we have the partitions, we can make predictions over them. For the [inter prediction](#temporal-redundancy-inter-prediction) we need **to send the motion vectors and the residual** and the [intra prediction](#spatial-redundancy-intra-prediction) we'll **send the prediction direction and the residual** as well.

## 3rd step - transform

After we get the residual block (`predicted partition - real partition`), we can **transform** it in a way that lets us know which **pixels we can discard** while keeping the **overall quality**. There are some transformations for this exact behavior.

Although there are [other transformations](https://en.wikipedia.org/wiki/List_of_Fourier-related_transforms#Discrete_transforms), we'll look more closely at the discrete cosine transform (DCT). The [**DCT**](https://en.wikipedia.org/wiki/Discrete_cosine_transform) main features are:

* **converts** blocks of **pixels** into  same-sized blocks of **frequency coefficients**.
* **compacts** energy, making it easy to eliminate spatial redundancy.
* is **reversible**, a.k.a. you can reverse to pixels.

> On 2 Feb 2017, Cintra, R. J. and Bayer, F. M have published their paper [DCT-like Transform for Image Compression
Requires 14 Additions Only](https://arxiv.org/abs/1702.00817).

Don't worry if you didn't understand the benefits from every bullet point, we'll try to make some experiments in order to see the real value from it.

Let's take the following **block of pixels** (8x8):

![pixel values matrix](/i/pixel_matrice.png "pixel values matrix")

Which renders to the following block image (8x8):

![pixel values matrix](/i/gray_image.png "pixel values matrix")

When we **apply the DCT** over this block of pixels and we get the **block of coefficients** (8x8):

![coefficients values](/i/dct_coefficient_values.png "coefficients values")

And if we render this block of coefficients, we'll get this image:

![dct coefficients image](/i/dct_coefficient_image.png "dct coefficients image")

As you can see it looks nothing like the original image, we might notice that the **first coefficient** is very different from all the others. This first coefficient is known as the DC coefficient which represents **all the samples** in the input array, something **similar to an average**.

This block of coefficients has an interesting property which is that it separates the high-frequency components from the low frequency.

![dct frequency coefficients property](/i/dctfrequ.jpg "dct frequency coefficients property")

In an image, **most of the energy** will be concentrated in the [**lower frequencies**](https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm), so if we transform an image into its frequency components and **throw away the higher frequency coefficients**, we can **reduce the amount of data** needed to describe the image without sacrificing too much image quality.

> frequency means how fast a signal is changing

Let's try to apply the knowledge we acquired in the test by converting the original image to its frequency (block of coefficients) using DCT and then throwing away part of the least important coefficients.

First, we convert it to its **frequency domain**.

![coefficients values](/i/dct_coefficient_values.png "coefficients values")

Next, we discard part (67%) of the coefficients, mostly the bottom right part of it.

![zeroed coefficients](/i/dct_coefficient_zeroed.png "zeroed coefficients")

Finally, we reconstruct the image from this discarded block of coefficients (remember, it needs to be reversible) and compare it to the original.

![original vs quantized](/i/original_vs_quantized.png "original vs quantized")

As we can see it resembles the original image but it introduced lots of differences from the original, we **throw away 67.1875%** and we still were able to get at least something similar to the original. We could more intelligently discard the coefficients to have a better image quality but that's the next topic.

> **Each coefficient is formed using all the pixels**
>
> It's important to note that each coefficient doesn't directly map to a single pixel but it's a weighted sum of all pixels. This amazing graph shows how the first and second coefficient is calculated, using weights which are unique for each index.
>
> ![dct calculation](/i/applicat.jpg "dct calculation")
>
> Source: https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm
>
> You can also try to [visualize the DCT by looking at a simple image](/dct_better_explained.ipynb) formation over the DCT basis. For instance, here's the [A character being formed](https://en.wikipedia.org/wiki/Discrete_cosine_transform#Example_of_IDCT) using each coefficient weight.
>
> ![](https://upload.wikimedia.org/wikipedia/commons/5/5e/Idct-animation.gif )




<br/>

> ### Hands-on: throwing away different coefficients
> You can play around with the [DCT transform](/uniform_quantization_experience.ipynb).

## 4th step - quantization

When we throw away some of the coefficients, in the last step (transform), we kinda did some form of quantization. This step is where we chose to lose information (the **lossy part**) or in simple terms, we'll **quantize coefficients to achieve compression**.

How can we quantize a block of coefficients? One simple method would be a uniform quantization, where we take a block, **divide it by a single value** (10) and round this value.

![quantize](/i/quantize.png "quantize")

How can we **reverse** (re-quantize) this block of coefficients? We can do that by **multiplying the same value** (10) we divide it first.

![re-quantize](/i/re-quantize.png "re-quantize")

This **approach isn't the best** because it doesn't take into account the importance of each coefficient, we could use a **matrix of quantizers** instead of a single value, this matrix can exploit the property of the DCT, quantizing most the bottom right and less the upper left, the [JPEG uses a similar approach](https://www.hdm-stuttgart.de/~maucher/Python/MMCodecs/html/jpegUpToQuant.html), you can check [source code to see this matrix](https://github.com/google/guetzli/blob/master/guetzli/jpeg_data.h#L40).

> ### Hands-on: quantization
> You can play around with the [quantization](/dct_experiences.ipynb).

## 5th step - entropy coding

After we quantized the data (image blocks/slices/frames) we still can compress it in a lossless way. There are many ways (algorithms) to compress data. We're going to briefly experience some of them, for a deeper understanding you can read the amazing book [Understanding Compression: Data Compression for Modern Developers](https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/).

### VLC coding:

Let's suppose we have a stream of the symbols: **a**, **e**, **r** and **t** and their probability (from 0 to 1) is represented by this table.

|             | a   | e   | r    | t   |
|-------------|-----|-----|------|-----|
| probability | 0.3 | 0.3 | 0.2 |  0.2 |

We can assign unique binary codes (preferable small) to the most probable and bigger codes to the least probable ones.

|             | a   | e   | r    | t   |
|-------------|-----|-----|------|-----|
| probability | 0.3 | 0.3 | 0.2 | 0.2 |
| binary code | 0 | 10 | 110 | 1110 |

Let's compress the stream **eat**, assuming we would spend 8 bits for each symbol, we would spend **24 bits** without any compression. But in case we replace each symbol for its code we can save space.

The first step is to encode the symbol **e** which is `10` and the second symbol is **a** which is added (not in a mathematical way) `[10][0]` and finally the third symbol **t** which makes our final compressed bitstream to be `[10][0][1110]` or `1001110` which only requires **7 bits** (3.4 times less space than the original).

Notice that each code must be a unique prefixed code [Huffman can help you to find these numbers](https://en.wikipedia.org/wiki/Huffman_coding). Though it has some issues there are [video codecs that still offers](https://en.wikipedia.org/wiki/Context-adaptive_variable-length_coding) this method and it's the  algorithm for many applications which requires compression.

Both encoder and decoder **must know** the symbol table with its code, therefore, you need to send the table too.

### Arithmetic coding:

Let's suppose we have a stream of the symbols: **a**, **e**, **r**, **s** and **t** and their probability is represented by this table.

|             | a   | e   | r    | s    | t   |
|-------------|-----|-----|------|------|-----|
| probability | 0.3 | 0.3 | 0.15 | 0.05 | 0.2 |

With this table in mind, we can build ranges containing all the possible symbols sorted by the most frequents.

![initial arithmetic range](/i/range.png "initial arithmetic range")

Now let's encode the stream **eat**, we pick the first symbol **e** which is located within the subrange **0.3 to 0.6** (but not included) and we take this subrange and split it again using the same proportions used before but within this new range.

![second sub range](/i/second_subrange.png "second sub range")

Let's continue to encode our stream **eat**, now we take the second symbol **a** which is within the new subrange **0.3 to 0.39** and then we take our last symbol **t** and we do the same process again and we get the last subrange **0.354 to 0.372**.

![final arithmetic range](/i/arithimetic_range.png "final arithmetic range")

We just need to pick a number within the last subrange **0.354 to 0.372**, let's choose **0.36** but we could choose any number within this subrange. With **only** this number we'll be able to recover our original stream **eat**. If you think about it, it's like if we were drawing a line within ranges of ranges to encode our stream.

![final range traverse](/i/range_show.png "final range traverse")

The **reverse process** (A.K.A. decoding) is equally easy, with our number **0.36** and our original range we can run the same process but now using this number to reveal the stream encoded behind this number.

With the first range, we notice that our number fits at the slice, therefore, it's our first symbol, now we split this subrange again, doing the same process as before, and we'll notice that **0.36** fits the symbol **a** and after we repeat the process we came to the last symbol **t** (forming our original encoded stream *eat*).

Both encoder and decoder **must know** the symbol probability table, therefore you need to send the table.

Pretty neat, isn't it? People are damn smart to come up with a such solution, some [video codecs use](https://en.wikipedia.org/wiki/Context-adaptive_binary_arithmetic_coding) this technique (or at least offer it as an option).

The idea is to lossless compress the quantized bitstream, for sure this article is missing tons of details, reasons, trade-offs and etc. But [you should learn more](https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/) as a developer. Newer codecs are trying to use different [entropy coding algorithms like ANS.](https://en.wikipedia.org/wiki/Asymmetric_Numeral_Systems)

> ### Hands-on: CABAC vs CAVLC
> You can [generate two streams, one with CABAC and other with CAVLC](https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#cabac-vs-cavlc) and **compare the time** it took to generate each of them as well as **the final size**.

## 6th step - bitstream format

After we did all these steps we need to **pack the compressed frames and context to these steps**. We need to explicitly inform to the decoder about **the decisions taken by the encoder**, such as bit depth, color space, resolution, predictions info (motion vectors, intra prediction direction), profile, level, frame rate, frame type, frame number and much more.

We're going to study, superficially, the H.264 bitstream. Our first step is to [generate a minimal  H.264 <sup>*</sup> bitstream](/encoding_pratical_examples.md#generate-a-single-frame-h264-bitstream), we can do that using our own repository and [ffmpeg](http://ffmpeg.org/).

```
./s/ffmpeg -i /files/i/minimal.png -pix_fmt yuv420p /files/v/minimal_yuv420.h264
```

> <sup>*</sup> ffmpeg adds, by default, all the encoding parameter as a **SEI NAL**, soon we'll define what is a NAL.

This command will generate a raw h264 bitstream with a **single frame**, 64x64, with color space yuv420 and using the following image as the frame.

> ![used frame to generate minimal h264 bitstream](/i/minimal.png "used frame to generate minimal h264 bitstream")

### H.264 bitstream

The AVC (H.264) standard defines that the information will be sent in **macro frames** (in the network sense), called **[NAL](https://en.wikipedia.org/wiki/Network_Abstraction_Layer)** (Network Abstraction Layer). The main goal of the NAL is the provision of a "network-friendly" video representation, this standard must work on TVs (stream based), the Internet (packet based) among others.

![NAL units H.264](/i/nal_units.png "NAL units H.264")

There is a **[synchronization marker](https://en.wikipedia.org/wiki/Frame_synchronization)** to define the boundaries of the NAL's units. Each synchronization marker holds a value of `0x00 0x00 0x01` except to the very first one which is `0x00 0x00 0x00 0x01`. If we run the **hexdump** on the generated h264 bitstream, we can identify at least three NALs in the beginning of the file.

![synchronization marker on NAL units](/i/minimal_yuv420_hex.png "synchronization marker on NAL units")

As we said before, the decoder needs to know not only the picture data but also the details of the video, frame, colors, used parameters, and others. The **first byte** of each NAL defines its category and **type**.

| NAL type id  | Description  |
|---  |---|
| 0  |  Undefined |
| 1  |  Coded slice of a non-IDR picture |
| 2  |  Coded slice data partition A |
| 3  |  Coded slice data partition B |
| 4  |  Coded slice data partition C |
| 5  |  **IDR** Coded slice of an IDR picture |
| 6  |  **SEI** Supplemental enhancement information |
| 7  |  **SPS** Sequence parameter set |
| 8  |  **PPS** Picture parameter set |
| 9  |  Access unit delimiter |
| 10 |  End of sequence |
| 11 |  End of stream |
| ... |  ... |

Usually, the first NAL of a bitstream is a **SPS**, this type of NAL is responsible for informing the general encoding variables like **profile**, **level**, **resolution** and others.

If we skip the first synchronization marker we can decode the **first byte** to know what **type of NAL** is the first one.

For instance the first byte after the synchronization marker is `01100111`, where the first bit (`0`) is to the field **forbidden_zero_bit**, the next 2 bits (`11`) tell us the field **nal_ref_idc** which indicates whether this NAL is a reference field or not and the rest 5 bits (`00111`) inform us the field **nal_unit_type**, in this case, it's a **SPS** (7) NAL unit.

The second byte (`binary=01100100, hex=0x64, dec=100`) of an SPS NAL is the field **profile_idc** which shows the profile that the encoder has used, in this case, we used the **[high profile](https://en.wikipedia.org/wiki/H.264/MPEG-4_AVC#Profiles)**. Also the third byte contains several flags which determine the exact profile (like constrained or progressive). But in our case the third byte is 0x00 and therefore the encoder has used just high profile.

![SPS binary view](/i/minimal_yuv420_bin.png "SPS binary view")

When we read the H.264 bitstream spec for an SPS NAL we'll find many values for the **parameter name**, **category** and a **description**, for instance, let's look at `pic_width_in_mbs_minus_1` and `pic_height_in_map_units_minus_1` fields.

| Parameter name  | Category  |  Description  |
|---  |---|---|
| pic_width_in_mbs_minus_1 |  0 | ue(v) |
| pic_height_in_map_units_minus_1 |  0 | ue(v) |

> **ue(v)**: unsigned integer [Exp-Golomb-coded](https://ghostarchive.org/archive/JBwdI)

If we do some math with the value of these fields we will end up with the **resolution**. We can represent a `1920 x 1080` using a `pic_width_in_mbs_minus_1` with the value of `119 ( (119 + 1) * macroblock_size = 120 * 16 = 1920) `, again saving space, instead of encode `1920` we did it with `119`.

If we continue to examine our created video with a binary view (ex: `xxd -b -c 11 v/minimal_yuv420.h264`), we can skip to the last NAL which is the frame itself.

![h264 idr slice header](/i/slice_nal_idr_bin.png "h264 idr slice header")

We can see its first 6 bytes values: `01100101 10001000 10000100 00000000 00100001 11111111`. As we already know the first byte tell us about what type of NAL it is, in this case, (`00101`) it's an **IDR Slice (5)** and we can further inspect it:

![h264 slice header spec](/i/slice_header.png "h264 slice header spec")

Using the spec info we can decode what type of slice (**slice_type**), the frame number (**frame_num**) among others important fields.

In order to get the values of some fields (`ue(v), me(v), se(v) or te(v)`) we need to decode it using a special decoder called [Exponential-Golomb](https://ghostarchive.org/archive/JBwdI), this method is **very efficient to encode variable values**, mostly when there are many default values.

> The values of **slice_type** and **frame_num** of this video are 7 (I slice) and 0 (the first frame).

We can see the **bitstream as a protocol** and if you want or need to learn more about this bitstream please refer to the [ITU H.264 spec.]( http://www.itu.int/rec/T-REC-H.264-201610-I) Here's a macro diagram which shows where the picture data (compressed YUV) resides.

![h264 bitstream macro diagram](/i/h264_bitstream_macro_diagram.png "h264 bitstream macro diagram")

We can explore others bitstreams like the [VP9 bitstream](https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf), [H.265 (HEVC)](http://handle.itu.int/11.1002/1000/11885-en?locatt=format:pdf) or even our **new best friend** [**AV1** bitstream](https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8
), [do they all look similar? No](http://www.gpac-licensing.com/2016/07/12/vp9-av1-bitstream-format/), but once you learned one you can easily get the others.

> ### Hands-on: Inspect the H.264 bitstream
> We can [generate a single frame video](https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#generate-a-single-frame-video) and use  [mediainfo](https://en.wikipedia.org/wiki/MediaInfo) to inspect its H.264 bitstream. In fact, you can even see the [source code that parses h264 (AVC)](https://github.com/MediaArea/MediaInfoLib/blob/master/Source/MediaInfo/Video/File_Avc.cpp) bitstream.
>
> ![mediainfo details h264 bitstream](/i/mediainfo_details_1.png "mediainfo details h264 bitstream")
>
> We can also use the [Intel Video Pro Analyzer](https://software.intel.com/en-us/intel-video-pro-analyzer) which is paid but there is a free trial version which limits you to only work with the first 10 frames but that's okay for learning purposes.
>
> ![intel video pro analyzer details h264 bitstream](/i/intel-video-pro-analyzer.png "intel video pro analyzer details h264 bitstream")

## Review

We'll notice that many of the **modern codecs uses this same model we learned**. In fact, let's look at the Thor video codec block diagram, it contains all the steps we studied. The idea is that you now should be able to at least understand better the innovations and papers for the area.

![thor_codec_block_diagram](/i/thor_codec_block_diagram.png "thor_codec_block_diagram")

Previously we had calculated that we needed [139GB of storage to keep a video file with one hour at 720p resolution and 30fps](#chroma-subsampling) if we use the techniques we learned here, like **inter and intra prediction, transform, quantization, entropy coding and other** we can achieve, assuming we are spending **0.031 bit per pixel**, the same perceivable quality video **requiring only 367.82MB vs 139GB** of store.

> We choose to use **0.031 bit per pixel** based on the example video provided here.

## How does H.265 achieve a better compression ratio than H.264?

Now that we know more about how codecs work, then it is easy to understand how new codecs are able to deliver higher resolutions with fewer bits.

We will compare AVC and HEVC, let's keep in mind that it is almost always a trade-off between more CPU cycles (complexity) and compression rate.

HEVC has bigger and more **partitions** (and **sub-partitions**) options than AVC, more **intra predictions directions/angles**, **improved entropy coding** and more, all these improvements made H.265 capable to compress 50% more than H.264.

![h264 vs h265](/i/avc_vs_hevc.png "H.264 vs H.265")

# Online streaming
## General architecture

![general architecture](/i/general_architecture.png "general architecture")

[TODO]

## Progressive download and adaptive streaming

![progressive download](/i/progressive_download.png "progressive download")

![adaptive streaming](/i/adaptive_streaming.png "adaptive streaming")

[TODO]

## Content protection

We can use **a simple token system** to protect the content. The user without a token tries to request a video and the CDN forbids her or him while a user with a valid token can play the content, it works pretty similarly to most of the web authentication systems.

![token protection](/i/token_protection.png "token_protection")

The sole use of this token system still allows a user to download a video and distribute it. Then the **DRM (digital rights management)** systems can be used to try to avoid this.

![drm](/i/drm.png "drm")

In real life production systems, people often use both techniques to provide authorization and authentication.

### DRM
#### Main systems

* FPS - [**FairPlay Streaming**](https://developer.apple.com/streaming/fps/)
* PR - [**PlayReady**](https://www.microsoft.com/playready/)
* WV - [**Widevine**](http://www.widevine.com/)


#### What?

DRM means [Digital rights management](https://sander.saares.eu/categories/drm-is-not-a-black-box/), it's a way **to provide copyright protection for digital media**, for instance, digital video and audio. Although it's used in many places [it's not universally accepted](https://en.wikipedia.org/wiki/Digital_rights_management#DRM-free_works).

#### Why?

Content creator (mostly studios) want to protect its intelectual property against copy to prevent unauthorized redistribution of digital media.

#### How?

We're going to describe an abstract and generic form of DRM in a very simplified way.

Given a **content C1** (i.e. an hls or dash video streaming), with a **player P1** (i.e. shaka-clappr, exo-player or ios) in a **device D1** (i.e. a smartphone, TV, tablet or desktop/notebook) using a **DRM system DRM1** (widevine, playready or FairPlay).

The content C1 is encrypted with a **symmetric-key K1** from the system DRM1, generating the **encrypted content C'1**.

![drm general flow](/i/drm_general_flow.jpeg "drm general flow")

The player P1, of a device D1, has two keys (asymmetric), a **private key PRK1** (this key is protected<sup>1</sup> and only known by **D1**) and a **public key PUK1**.

> **<sup>1</sup>protected**: this protection can be **via hardware**, for instance, this key can be stored inside a special (read-only) chip that works like [a black-box](https://en.wikipedia.org/wiki/Black_box) to provide decryption, or **by software** (less safe), the DRM system provides means to know which type of protection a given device has.


When the **player P1 wants to play** the **content C'1**, it needs to deal with the **DRM system DRM1**, giving its public key **PUK1**. The DRM system DRM1 returns the **key K1 encrypted** with the client''s public key **PUK1**. In theory, this response is something that **only D1 is capable of decrypting**.

`K1P1D1 = enc(K1, PUK1)`

**P1** uses its DRM local system (it could be a [SOC](https://en.wikipedia.org/wiki/System_on_a_chip), a specialized hardware or software), this system is **able to decrypt** the content using its private key PRK1, it can decrypt **the symmetric-key K1 from the K1P1D1** and **play C'1**. At best case, the keys are not exposed through RAM.

 ```
 K1 = dec(K1P1D1, PRK1)

 P1.play(dec(C'1, K1))
 ```

![drm decoder flow](/i/drm_decoder_flow.jpeg "drm decoder flow")

# How to use jupyter

Make sure you have **docker installed** and just run `./s/start_jupyter.sh` and follow the instructions on the terminal.

# Conferences

* [DEMUXED](https://demuxed.com/) - you can [check the last 2 events presentations.](https://www.youtube.com/channel/UCIc_DkRxo9UgUSTvWVNCmpA).

# References

The richest content is here, it's where all the info we saw in this text was extracted, based or inspired by. You can deepen your knowledge with these amazing links, books, videos and etc.

Online Courses and Tutorials:

* https://www.coursera.org/learn/digital/
* https://people.xiph.org/~tterribe/pubs/lca2012/auckland/intro_to_video1.pdf
* https://xiph.org/video/vid1.shtml
* https://xiph.org/video/vid2.shtml
* https://wiki.multimedia.cx
* https://mahanstreamer.net
* http://slhck.info/ffmpeg-encoding-course
* http://www.cambridgeincolour.com/tutorials/camera-sensors.htm
* http://www.slideshare.net/vcodex/a-short-history-of-video-coding
* http://www.slideshare.net/vcodex/introduction-to-video-compression-13394338
* https://developer.android.com/guide/topics/media/media-formats.html
* http://www.slideshare.net/MadhawaKasun/audio-compression-23398426
* http://inst.eecs.berkeley.edu/~ee290t/sp04/lectures/02-Motion_Compensation_girod.pdf

Books:

* https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/ref=sr_1_1?s=books&ie=UTF8&qid=1486395327&sr=1-1
* https://www.amazon.com/H-264-Advanced-Video-Compression-Standard/dp/0470516925
* https://www.amazon.com/High-Efficiency-Video-Coding-HEVC/dp/3319068946
* https://www.amazon.com/Practical-Guide-Video-Audio-Compression/dp/0240806301/ref=sr_1_3?s=books&ie=UTF8&qid=1486396914&sr=1-3&keywords=A+PRACTICAL+GUIDE+TO+VIDEO+AUDIO
* https://www.amazon.com/Video-Encoding-Numbers-Eliminate-Guesswork/dp/0998453005/ref=sr_1_1?s=books&ie=UTF8&qid=1486396940&sr=1-1&keywords=jan+ozer

Onboarding material:

* https://github.com/Eyevinn/streaming-onboarding
* https://howvideo.works/
* https://www.aws.training/Details/eLearning?id=17775
* https://www.aws.training/Details/eLearning?id=17887
* https://www.aws.training/Details/Video?id=24750

Bitstream Specifications:

* http://www.itu.int/rec/T-REC-H.264-201610-I
* http://www.itu.int/ITU-T/recommendations/rec.aspx?rec=12904&lang=en
* https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf
* http://iphome.hhi.de/wiegand/assets/pdfs/2012_12_IEEE-HEVC-Overview.pdf
* http://phenix.int-evry.fr/jct/doc_end_user/current_document.php?id=7243
* http://gentlelogic.blogspot.com.br/2011/11/exploring-h264-part-2-h264-bitstream.html
* https://forum.doom9.org/showthread.php?t=167081
* https://forum.doom9.org/showthread.php?t=168947

Software:

* https://ffmpeg.org/
* https://ffmpeg.org/ffmpeg-all.html
* https://ffmpeg.org/ffprobe.html
* https://mediaarea.net/en/MediaInfo
* https://www.jongbel.com/
* https://trac.ffmpeg.org/wiki/
* https://software.intel.com/en-us/intel-video-pro-analyzer
* https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8

Non-ITU Codecs:

* https://aomedia.googlesource.com/
* https://github.com/webmproject/libvpx/tree/master/vp9
* https://ghostarchive.org/archive/0W0d8 (was: https://people.xiph.org/~xiphmont/demo/daala/demo1.shtml)
* https://people.xiph.org/~jm/daala/revisiting/
* https://www.youtube.com/watch?v=lzPaldsmJbk
* https://fosdem.org/2017/schedule/event/om_av1/
* https://jmvalin.ca/papers/AV1_tools.pdf

Encoding Concepts:

* http://x265.org/hevc-h265/
* http://slhck.info/video/2017/03/01/rate-control.html
* http://slhck.info/video/2017/02/24/vbr-settings.html
* http://slhck.info/video/2017/02/24/crf-guide.html
* https://arxiv.org/pdf/1702.00817v1.pdf
* https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors
* http://web.ece.ucdavis.edu/cerl/ReliableJPEG/Cung/jpeg.html
* http://www.adobe.com/devnet/adobe-media-server/articles/h264_encoding.html
* https://prezi.com/8m7thtvl4ywr/mp3-and-aac-explained/
* https://blogs.gnome.org/rbultje/2016/12/13/overview-of-the-vp9-video-codec/
* https://videoblerg.wordpress.com/2017/11/10/ffmpeg-and-how-to-use-it-wrong/

Video Sequences for Testing:

* http://bbb3d.renderfarming.net/download.html
* https://www.its.bldrdoc.gov/vqeg/video-datasets-and-organizations.aspx

Miscellaneous:

* https://github.com/Eyevinn/streaming-onboarding
* http://stackoverflow.com/a/24890903
* http://stackoverflow.com/questions/38094302/how-to-understand-header-of-h264
* http://techblog.netflix.com/2016/08/a-large-scale-comparison-of-x264-x265.html
* http://vanseodesign.com/web-design/color-luminance/
* http://www.biologymad.com/nervoussystem/eyenotes.htm
* http://www.compression.ru/video/codec_comparison/h264_2012/mpeg4_avc_h264_video_codecs_comparison.pdf
* https://web.archive.org/web/20100728070421/http://www.csc.villanova.edu/~rschumey/csc4800/dct.html (was: http://www.csc.villanova.edu/~rschumey/csc4800/dct.html)
* http://www.explainthatstuff.com/digitalcameras.html
* http://www.hkvstar.com
* http://www.hometheatersound.com/
* http://www.lighterra.com/papers/videoencodingh264/
* http://www.red.com/learn/red-101/video-chroma-subsampling
* http://www.slideshare.net/ManoharKuse/hevc-intra-coding
* http://www.slideshare.net/mwalendo/h264vs-hevc
* http://www.slideshare.net/rvarun7777/final-seminar-46117193
* http://www.springer.com/cda/content/document/cda_downloaddocument/9783642147029-c1.pdf
* http://www.streamingmedia.com/Articles/Editorial/Featured-Articles/A-Progress-Report-The-Alliance-for-Open-Media-and-the-AV1-Codec-110383.aspx
* http://www.streamingmediaglobal.com/Articles/ReadArticle.aspx?ArticleID=116505&PageNum=1
* http://yumichan.net/video-processing/video-compression/introduction-to-h264-nal-unit/
* https://cardinalpeak.com/blog/the-h-264-sequence-parameter-set/
* https://cardinalpeak.com/blog/worlds-smallest-h-264-encoder/
* https://codesequoia.wordpress.com/category/video/
* https://developer.apple.com/library/content/technotes/tn2224/_index.html
* https://en.wikibooks.org/wiki/MeGUI/x264_Settings
* https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming
* https://en.wikipedia.org/wiki/AOMedia_Video_1
* https://en.wikipedia.org/wiki/Chroma_subsampling#/media/File:Colorcomp.jpg
* https://en.wikipedia.org/wiki/Cone_cell
* https://en.wikipedia.org/wiki/File:H.264_block_diagram_with_quality_score.jpg
* https://en.wikipedia.org/wiki/Inter_frame
* https://en.wikipedia.org/wiki/Intra-frame_coding
* https://en.wikipedia.org/wiki/Photoreceptor_cell
* https://en.wikipedia.org/wiki/Pixel_aspect_ratio
* https://en.wikipedia.org/wiki/Presentation_timestamp
* https://en.wikipedia.org/wiki/Rod_cell
* https://it.wikipedia.org/wiki/File:Pixel_geometry_01_Pengo.jpg
* https://leandromoreira.com.br/2016/10/09/how-to-measure-video-quality-perception/
* https://sites.google.com/site/linuxencoding/x264-ffmpeg-mapping
* https://softwaredevelopmentperestroika.wordpress.com/2014/02/11/image-processing-with-python-numpy-scipy-image-convolution/
* https://tools.ietf.org/html/draft-fuldseth-netvc-thor-03
* https://www.encoding.com/android/
* https://www.encoding.com/http-live-streaming-hls/
* https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm
* https://www.lifewire.com/cmos-image-sensor-493271
* https://www.linkedin.com/pulse/brief-history-video-codecs-yoav-nativ
* https://www.linkedin.com/pulse/video-streaming-methodology-reema-majumdar
* https://www.vcodex.com/h264avc-intra-precition/
* https://www.youtube.com/watch?v=9vgtJJ2wwMA
* https://www.youtube.com/watch?v=LFXN9PiOGtY
* https://www.youtube.com/watch?v=Lto-ajuqW3w&list=PLzH6n4zXuckpKAj1_88VS-8Z6yn9zX_P6
* https://www.youtube.com/watch?v=LWxu4rkZBLw
* https://web.stanford.edu/class/ee398a/handouts/lectures/EE398a_MotionEstimation_2012.pdf
* https://sander.saares.eu/categories/drm-is-not-a-black-box/
