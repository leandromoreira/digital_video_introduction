[游쥟릖](/README.md "English")
[游뻟릖쓯(/README-cn.md "Simplified Chinese")
[游游엫(/README-ja.md "Japanese")
[游쉻릖젷(/README-it.md "Italian")
[游썷릖윻(/README-ko.md "Korean")
[游游죟(/README-ru.md "Russian")
[游游윻(/README-pt.md "Portuguese")
[游쀯릖](/README-es.md "Spanish")

[![license](https://img.shields.io/badge/license-BSD--3--Clause-blue.svg)](https://img.shields.io/badge/license-BSD--3--Clause-blue.svg)

# Introducci칩n

Esto es una breve introducci칩n a la tecnolog칤a de v칤deo, principalmente dirigido a desarrolladores o ingenieros de software. La intenci칩n es que sea **f치cil de aprender para cualquier persona**. La idea naci칩 durante un [breve taller para personas nuevas en v칤deo](https://docs.google.com/presentation/d/17Z31kEkl_NGJ0M66reqr9_uTG6tI5EDDVXpdPKVuIrs/edit#slide=id.p).

El objetivo es introducir algunos conceptos de v칤deo digital con un **vocabulario simple, muchas figuras y ejemplos pr치cticos** en cuanto sea posible, y dejar disponible este conocimiento. Por favor, si칠ntete libre de enviar correcciones, sugerencias y mejoras.

Habr치n **pr치cticas** que requiere tener instalado **docker** y este repositorio clonado. 

```bash
git clone https://github.com/leandromoreira/digital_video_introduction.git
cd digital_video_introduction
./setup.sh
```
> **ADVERTENCIA**: cuando observes el comando `./s/ffmpeg` o `./s/mediainfo`, significa que estamos ejecutando la **versi칩n contenerizada** del mismo, que a su vez incluye todos los requerimientos necesarios.

Todas las **pr치cticas deber치n ser ejecutadas desde el directorio donde has clonado** este repositorio. Para ejecutar los **ejemplos de jupyter** deber치s primero iniciar el servicio `./s/start_jupyter.sh`, copiar la URL y pegarla en tu navegador.

# Control de cambios

* Detalles sobre sistemas DRM
* Versi칩n 1.0.0 liberada
* Traducci칩n a Chino Simplificado
* Ejemplo de FFmpeg oscilloscope filter
* Traducci칩n a Portugu칠s (Brasil)
* Traducci칩n a Espa침ol

# Tabla de contenidos

- [Introducci칩n](#introducci칩n)
- [Tabla de contenidos](#tabla-de-contenidos)
- [Terminolog칤a B치sica](#terminolog칤a-b치sica)
  * [Otras formas de codificar una imagen a color](#otras-formas-de-codificar-una-imagen-a-color)
  * [Pr치ctica: juguemos con una imagen y colores](#pr치ctica-juguemos-con-una-imagen-y-colores)
  * [DVD es DAR 4:3](#dvd-es-dar-43)
  * [Pr치ctica: Verifiquemos las propiedades del v칤deo](#pr치ctica-verifiquemos-las-propiedades-del-v칤deo)
- [Eliminaci칩n de Redundancias](#eliminaci칩n-de-redundancias)
  * [Colores, Luminancia y nuestros ojos](#colores-luinancia-y-nuestros-ojos)
    + [Modelo de color](#modelo-de-color)
    + [Conversiones entre YCbCr y RGB](#conversiones-entre-ycbcr-y-rgb)
    + [Chroma subsampling](#chroma-subsampling)
    + [Pr치ctica: Observemos el histograma YCbCr](#pr치ctica-observemos-el-histograma-ycbcr)
    + [Color, luma, luminancia, gamma](#color-luma-luminancia-gamma)
    + [Pr치ctica: Observa la intensidad YCbCr](#pr치ctica-observa-la-intensidad-ycbcr)
  * [Tipos de fotogramas](#tipos-de-fotogramas)
    + [I Frame (intra, keyframe)](#i-frame-intra-keyframe)
    + [P Frame (predicted)](#p-frame-predicted)
      - [Pr치ctica: Un v칤deo con un solo I-frame](#pr치ctica-un-v칤deo-con-un-solo-i-frame)
    + [B Frame (bi-predictive)](#b-frame-bi-predictive)
      - [Pr치ctica: Compara v칤deos con B-frame](#pr치ctica-compara-videos-con-b-frame)
    + [Resumen](#resumen)
  * [Redundancia Temporal (inter prediction)](#redundancia-temporal-inter-prediction)
      - [Pr치ctica: Observa los vectores de movimiento](#pr치ctica-observa-los-vectores-de-movimiento)
  * [Redundancia Espacial (intra prediction)](#redundancia-espacial-intra-prediction)
      - [Pr치ctica: Observa las intra predictions](#pr치ctica-observa-las-intra-predictions)
- [쮺칩mo funciona un c칩dec de v칤deo?](#c칩mo-funciona-un-c칩dec-de-v칤deo)
  * [쯈u칠? 쯇or qu칠? 쮺칩mo?](#qu칠-por-qu칠-c칩mo)
  * [Historia](#historia)
    + [El nacimiento de AV1](#el-nacimiento-de-av1)
  * [Un c칩dec gen칠rico](#un-c칩dec-gen칠rico)
  * [Paso 1 - Particionado de im치genes](#paso-1---particionado-de-im치genes)
    + [Pr치ctica: Verificar particiones](#pr치ctica-verificar-particiones)
  * [Paso 2 - Predicciones](#paso-2---predicciones)
  * [Paso 3 - Transformaci칩n](#paso-3---transformaci칩n)
    + [Pr치ctica: Descartar diferentes coeficientes](#pr치ctica-descartar-diferentes-coeficientes)
  * [Paso 4 - Cuantizaci칩n](#paso-4---cuantizaci칩n)
    + [Pr치ctica: Cuantizaci칩n](#pr치ctica-cuantizaci칩n)
  * [Paso 5 - Codificaci칩n de la entrop칤a](#paso-5---codificaci칩n-de-la-entrop칤a)
    + [Codificaci칩n VLC](#codificaci칩n-vlc)
    + [Codificaci칩n aritm칠tica](#codificaci칩n-aritm칠tica)
    + [Pr치ctica: CABAC vs CAVLC](#pr치ctica-cabac-vs-cavlc)
  * [Paso 6 - formato *bitstream*](#paso-6---formato-bitstream)
    + [H.264 bitstream](#h264-bitstream)
    + [Pr치ctica: Inspeccionar el *bitstream* H.264](#pr치ctica-inspeccionar-el-bitstream-h264)
  * [Repaso](#repaso)
  * [쮺칩mo logra H.265 una mejor relaci칩n de compresi칩n que H.264?](#c칩mo-logra-h265-una-mejor-relaci칩n-de-compresi칩n-que-h264)
- [Online streaming](#online-streaming)
  * [Arquitectura general](#arquitectura-general)
  * [Descarga progresiva y streaming adaptativo](#descarga-progresiva-y-streaming-adaptativo)
  * [Protecci칩n de contenido](#protecci칩n-de-contenido)
- [쮺칩mo usar jupyter?](#c칩mo-usar-jupyter)
- [Conferencias](#conferencias)
- [Referencias](#referencias)

# Terminolog칤a B치sica

Una **imagen** puede ser pensada como una **matriz 2D**. Si pensamos en **colores**, podemos extrapolar este concepto a que una imagen es una **matriz 3D** donde la **dimensi칩n adicional** es usada para indicar la **informaci칩n de color**.

Si elegimos representar esos colores usando los [colores primarios (rojo, verde y azul)](https://es.wikipedia.org/wiki/Color_primario), definimos tres planos: el primero para el **rojo**, el segundo para el **verde**, y el 칰ltimo para el **azul**.

![una imagen es una matriz 3D RGB](/i/image_3d_matrix_rgb.png "Una imagen es una matriz 3D")

Llamaremos a cada punto en esta matriz **un p칤xel** (del ingl칠s, *picture element*). Un p칤xel representa la **intensidad** (usualmente un valor num칠rico) de un color dado. Por ejemplo, un **p칤xel rojo** significa 0 de verde, 0 de azul y el m치ximo de rojo. El **p칤xel de color rosa** puede formarse de la combinaci칩n de estos tres colores. Usando la representaci칩n num칠rica con un rango desde 0 a 255, el p칤xel rosa es definido como **Rojo=255, Verde=192 y Azul=203**.

> #### Otras formas de codificar una imagen a color
> Otros modelos pueden ser utilizados para representar los colores que forman una imagen. Por ejemplo, podemos usar una paleta indexada donde necesitaremos solamente un byte para representar cada p칤xel en vez de los 3 necesarios cuando usamos el modelo RGB. En un modelo como este, podemos utilizar una matriz 2D para representar nuestros colores, esto nos ayudar칤a a ahorrar memoria aunque tendr칤amos menos colores para representar.
>
> ![NES palette](/i/nes-color-palette.png "NES palette")

Observemos la figura que se encuentra a continuaci칩n. La primer cara muestra todos los colores utilizados. Mientras las dem치s muestran a intencidad de cada plano rojo, verde y azul (representado en escala de grises).

![Intensidad de los canales RGB](/i/rgb_channels_intensity.png "Intensidad de los canales RGB")

Podemos ver que el **color rojo** es el que **contribuye m치s** (las partes m치s brillantes en la segunda cara) mientras que en la cuarta cara observamos que el **color azul** su contribuci칩n se **limita a los ojos de Mario** y parte de su ropa. Tambi칠n podemos observar como **todos los planos no contribuyen demasiado** (partes oscuras) al **bigote de Mario**.

Cada intensidad de color requiere de una cantidad de bits para ser representada, esa cantidad es conocida como **bit depth**. Digamos que utilizamos **8 bits** (aceptando valores entre 0 y 255) por color (plano), entonces tendremos una **color depth** (profundidad de color) de **24 bits** (8 bits * 3 planos R/G/B), por lo que podemos inferir que tenemos disponibles 2^24 colores diferentes.

> **Es asombroso** aprender [como una imagen es capturada desde el Mundo a los bits](http://www.cambridgeincolour.com/tutorials/camera-sensors.htm).

Otra propiedad de una images es la **resoluci칩n**, que es el n칰mero de p칤xeles en una dimensi칩n. Frecuentemente es presentado como ancho 칑 alto (*width 칑 height*), por ejemplo, la siguiente imagen es **4칑4**.

![image resolution](/i/resolution.png "image resolution")

> #### Pr치ctica: juguemos con una imagen y colores
> Puedes [jugar con una imagen y colores](/image_as_3d_array.ipynb) usando [jupyter](#how-to-use-jupyter) (python, numpy, matplotlib, etc).
>
> Tambi칠n puedes aprender [c칩mo funcionan los filtros de im치genes (edge detection, sharpen, blur...)](/filters_are_easy.ipynb).

Trabajando con im치genes o v칤deos, te encontrar치s con otra propiedad llamada **aspect ratio** (relaci칩n de aspecto) que simplemente describe la relaci칩n proporcional que existe entre el ancho y alto de una imagen o p칤xel.

Cuando las personas dicen que una pel칤cula o imagen es **16x9** usualmente se est치n refiriendo a la relaci칩n de aspecto de la pantalla (**Display Aspect Ratio (DAR)**), sin embargo podemos tener diferentes formas para cada p칤xel, a ello le llamamos relaci칩n de aspecto del p칤xel (**Pixel Aspect Ratio (PAR)**).

![display aspect ratio](/i/DAR.png "display aspect ratio")

![pixel aspect ratio](/i/PAR.png "pixel aspect ratio")

> #### DVD es DAR 4:3
> La resoluci칩n del formato de DVD es 704x480, igualmente mantiente una relaci칩n de aspecto (DAR) de 4:3 porque tiene PAR de 10:11 (704x10/480x11)

Finalmente, podemos definir a un **v칤deo** como una **sucesi칩n de *n* fotogramas** en el **tiempo**, la cual la podemos definir como otra dimensi칩n, *n* es el *frame rate* o fotogramas por segundo (**FPS**, del ingl칠s *frames per second*).

![video](/i/video.png "video")

El n칰mero necesario de bits por segundo para mostrar un v칤deo es llamado **bit rate**.

> bit rate = ancho * alto * bit depth * FPS

Por ejemplo, un v칤deo de 30 fotogramas por segundo, 24 bits por p칤xel, 480x240 de resoluci칩n necesitar칤a de **82,944,000 bits por segundo** o 82.944 Mbps (30x480x240x24) si no queremos aplicar ning칰n tipo de compresi칩n.

Cuando el **bit rate** es casi constante es llamado bit rate constante (**CBR**, del ingl칠s *constant bit rate*), pero tambi칠n puede ser variable por lo que lo llamaremos bit rate variable (**VBR**, del ingl칠s *variable bit rate*).

> La siguiente gr치fica muestra como utilizando VBR no se necesita gastar demasiados bits para un fotograma es negro.
>
> ![constrained vbr](/i/vbr.png "constrained vbr")

En otros tiempos, unos ingenieros idearon una t칠cnica para duplicar la cantidad de fotogramas por segundo percividos en una pantalla **sin consumir ancho de banda adicional**. Esta t칠cnica es conocida como **v칤deo entrelazado** (*interlaced video*); b치sicamente env칤a la mitad de la pantalla en un "fotograma" y la otra mitad en el siguiente "fotograma".  

Hoy en d칤a, las pantallas representan im치genes principalmente utilizando la **t칠cnica de escaneo progresivo** (*progressive v칤deo*). El v칤deo progresivo es una forma de mostrar, almacenar o transmitir im치genes en movimiento en la que todas las l칤neas de cada fotograma se dibujan en secuencia.

![entrelazado vs. progresivo](/i/interlaced_vs_progressive.png "entrelazado vs. progresivo")

Ahora ya tenemos una idea acerca de c칩mo una **imagen** es representada digitalmente, c칩mo sus **colores** son codificados, cu치ntos **bits por segundo** necesitamos para mostrar un v칤deo, si es constante (CBR) o variable (VBR), con una **resoluci칩n** dada a determinado **frame rate** y otros t칠rminos como entrelazado, PAR, etc.

> #### Pr치ctica: Verifiquemos las propiedades del v칤deo
> Puedes [verificar la mayor칤a de las propiedades mencionadas con ffmpeg o mediainfo.](https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#inspect-stream)

# Eliminaci칩n de Redundancias

Aprendimos que no es factible utilizar v칤deo sin ninguna compresi칩n; **un solo v칤deo de una hora** a una resoluci칩n de 720p y 30fps **requerir칤a 278GB<sup>*</sup>**. Dado que **utilizar 칰nicamente algoritmos de compresi칩n de datos sin p칠rdida** como DEFLATE (utilizado en PKZIP, Gzip y PNG), **no disminuir칤a** suficientemente el ancho de banda necesario, debemos encontrar otras formas de comprimir el v칤deo.

> <sup>*</sup> Encontramos este n칰mero multiplicando 1280 x 720 x 24 x 30 x 3600 (ancho, alto, bits por p칤xel, fps y tiempo en segundos).

Para hacerlo, podemos **aprovechar c칩mo funciona nuestra visi칩n**. Somos mejores para distinguir el brillo que los colores, las **repeticiones en el tiempo**, un v칤deo contiene muchas im치genes con pocos cambios, y las **repeticiones dentro de la imagen**, cada fotograma tambi칠n contiene muchas 치reas que utilizan colores iguales o similares.

## Colores, Luminancia y nuestros ojos
Nuestros ojos son [m치s sensibles al brillo que a los colores](http://vanseodesign.com/web-design/color-luminance/), puedes comprobarlo por ti mismo, mira esta imagen.

![luminance vs color](/i/luminance_vs_color.png "luminance vs color")

Si no puedes ver que los colores de los **cuadrados A y B son id칠nticos** en el lado izquierdo, no te preocupes, es nuestro cerebro jug치ndonos una pasada para que **prestemos m치s atenci칩n a la luz y la oscuridad que al color**. Hay un conector, del mismo color, en el lado derecho para que nosotros (nuestro cerebro) podamos identificar f치cilmente que, de hecho, son del mismo color.

> **Explicaci칩n simplista de c칩mo funcionan nuestros ojos**
> El [ojo es un 칩rgano complejo](http://www.biologymad.com/nervoussystem/eyenotes.htm), compuesto por muchas partes, pero principalmente nos interesan las c칠lulas de conos y bastones. El ojo [contiene alrededor de 120 millones de c칠lulas de bastones y 6 millones de c칠lulas de conos](https://en.wikipedia.org/wiki/Photoreceptor_cell).
>
> Para **simplificar absurdamente**, intentemos relacionar los colores y el brillo con las funciones de las partes del ojo. Los **[bastones](https://es.wikipedia.org/wiki/Bast%C3%B3n_(c%C3%A9lula)) son principalmente responsables del brillo**, mientras que los **[conos](https://es.wikipedia.org/wiki/Cono_(c%C3%A9lula)) son responsables del color**. Hay tres tipos de conos, cada uno con un pigmento diferente, a saber: [Tipo S (azul), Tipo M (verde) y Tipo L (rojos)](https://upload.wikimedia.org/wikipedia/commons/1/1e/Cones_SMJ2_E.svg).
>
> Dado que tenemos muchas m치s c칠lulas de bastones (brillo) que c칠lulas de conos (color), se puede inferir que somos m치s capaces de distinguir el contraste entre la luz y la oscuridad que los colores.
>
> ![eyes composition](/i/eyes.jpg "eyes composition")
>
> **Funciones de sensibilidad al contraste**
>
> Los investigadores de psicolog칤a experimental y muchas otras disciplinas han desarrollado varias teor칤as sobre la visi칩n humana. Una de ellas se llama funciones de sensibilidad al contraste. Est치n relacionadas con el espacio y el tiempo de la luz y su valor se presenta en una luz inicial dada, cu치nto cambio se requiere antes de que un observador informe que ha habido un cambio. Observa el plural de la palabra "funci칩n", esto se debe a que podemos medir las funciones de sensibilidad al contraste no solo con blanco y negro, sino tambi칠n con colores. El resultado de estos experimentos muestra que en la mayor칤a de los casos nuestros ojos son m치s sensibles al brillo que al color.

Una vez que sabemos que somos m치s sensibles a la **luminancia** (*luma*, el brillo en una imagen), podemos aprovecharlo.
### Modelo de color

Primero aprendimos [c칩mo funcionan las im치genes en color](#terminolog칤a-b치sica) utilizando el **modelo RGB**, pero existen otros modelos tambi칠n. De hecho, hay un modelo que separa la luminancia (brillo) de la crominancia (colores) y se conoce como **YCbCr**<sup>*</sup>.

> <sup>*</sup> hay m치s modelos que hacen la misma separaci칩n.

Este modelo de color utiliza **Y** para representar el brillo y dos canales de color, **Cb** (croma azul) y **Cr** (croma rojo). El [YCbCr](https://es.wikipedia.org/wiki/YCbCr) se puede derivar a partir de RGB y tambi칠n se puede convertir de nuevo a RGB. Utilizando este modelo, podemos crear im치genes a todo color, como podemos ver a continuaci칩n.

![ycbcr example](/i/ycbcr.png "ycbcr example")

### Conversiones entre YCbCr y RGB

Algunos pueden preguntarse, 쯖칩mo podemos producir **todos los colores sin usar el verde**?

Para responder a esta pregunta, vamos a realizar una conversi칩n de RGB a YCbCr. Utilizaremos los coeficientes del **[est치ndar BT.601](https://en.wikipedia.org/wiki/Rec._601)** que fue recomendado por el **[grupo ITU-R<sup>*</sup>](https://en.wikipedia.org/wiki/ITU-R)**. El primer paso es **calcular la luminancia**, utilizaremos las constantes sugeridas por la ITU y sustituiremos los valores RGB.

```
Y = 0.299R + 0.587G + 0.114B
```

Una vez obtenida la luminancia, podemos **separar los colores** (croma azul y croma rojo):

```
Cb = 0.564(B - Y)
Cr = 0.713(R - Y)
```

Y tambi칠n podemos **covertirlo de vuelta** e incluse obtener el **verde utilizando YCbCr**.

```
R = Y + 1.402Cr
B = Y + 1.772Cb
G = Y - 0.344Cb - 0.714Cr
```

> <sup>*</sup> Los grupos y est치ndares son comunes en el v칤deo digital y suelen definir cu치les son los est치ndares, por ejemplo, [쯤u칠 es 4K? 쯤u칠 FPS debemos usar? resoluci칩n? 쯠odelo de color?](https://en.wikipedia.org/wiki/Rec._2020)

Generalmente, las **pantallas** (monitores, televisores, etc.) utilizan **solo el modelo RGB**, organizado de diferentes maneras, como se muestra a continuaci칩n:

![pixel geometry](/i/new_pixel_geometry.jpg "pixel geometry")

### Chroma subsampling

Con la imagen representada en componentes de luminancia y crominancia, podemos aprovechar la mayor sensibilidad del sistema visual humano a la resoluci칩n de luminancia en lugar de la crominancia para eliminar selectivamente informaci칩n. El **chroma subsampling** es la t칠cnica de codificar im치genes utilizando **menor resoluci칩n para la crominancia que para la luminancia**.

![ycbcr subsampling resolutions](/i/ycbcr_subsampling_resolution.png "ycbcr subsampling resolutions")


쮺u치nto debemos reducir la resoluci칩n de la crominancia? Resulta que ya existen algunos esquemas que describen c칩mo manejar la resoluci칩n y la combinaci칩n (`color final = Y + Cb + Cr`).

Estos esquemas se conocen como sistemas de *subsampling* y se expresan como una relaci칩n de 3 partes: `a:x:y`, que define la resoluci칩n de la crominancia en relaci칩n con un bloque `a x 2` de p칤xeles de luminancia.

 * `a` es la referencia de muestreo horizontal (generalmente 4).
 * `x` es el n칰mero de muestras de crominancia en la primera fila de `a` p칤xeles (resoluci칩n horizontal en relaci칩n con `a`).
 * `y` es el n칰mero de cambios de muestras de crominancia entre la primera y segunda filas de a p칤xeles.

> Una excepci칩n a esto es el 4:1:0, que proporciona una sola muestra de crominancia dentro de cada bloque de `4 x 4` p칤xeles de resoluci칩n de luminancia.

Los esquemas comunes utilizados en c칩decs modernos son: **4:4:4** *(sin subsampling)*, **4:2:2, 4:1:1, 4:2:0, 4:1:0 y 3:1:1**.

> Puedes seguir algunas discusiones para [aprender m치s sobre el Chroma Subsampling](https://github.com/leandromoreira/digital_video_introduction/issues?q=YCbCr).

> **Fusi칩n YCbCr 4:2:0**
>
> Aqu칤 tienes una parte fusionada de una imagen utilizando YCbCr 4:2:0, observa que solo utilizamos 12 bits por p칤xel.
>
> ![YCbCr 4:2:0 merge](/i/ycbcr_420_merge.png "YCbCr 4:2:0 merge")

Puedes ver la misma imagen codificada por los principales tipos de *chroma subsampling*, las im치genes en la primera fila son las YCbCr finales, mientras que la 칰ltima fila de im치genes muestra la resoluci칩n de crominancia. Es realmente una gran ganancia con una p칠rdida tan peque침a.

![chroma subsampling examples](/i/chroma_subsampling_examples.jpg "chroma subsampling examples")

Anteriormente hab칤amos calculado que necesit치bamos [278 GB de almacenamiento para mantener un archivo de video con una hora de resoluci칩n de 720p y 30 fps](#eliminaci칩n-de-redundancias). Si usamos **YCbCr 4:2:0**, podemos reducir **este tama침o a la mitad (139 GB)**<sup>*</sup>, pero a칰n est치 lejos del ideal.

> <sup>*</sup> encontramos este valor multiplicando el ancho, alto, bits por p칤xel y fps. Anteriormente necesit치bamos 24 bits, ahora solo necesitamos 12.

<br/>

> ### Pr치ctica: Observemos el histograma YCbCr
> Puedes [observar el histograma YCbCr con ffmpeg](/encoding_pratical_examples.md#generates-yuv-histogram). Esta excena tiene una mayor contribuci칩n de azul la cual se muestra en el [histograma](https://es.wikipedia.org/wiki/Histograma).
>
> ![ycbcr color histogram](/i/yuv_histogram.png "ycbcr color histogram")

### Color, luma, luminancia, gamma

Mira este incre칤ble video que explica qu칠 es la luma y aprende sobre luminancia, gamma y color.
**V칤deo en Ingl칠s**
[![Analog Luma - A history and explanation of video](http://img.youtube.com/vi/Ymt47wXUDEU/0.jpg)](http://www.youtube.com/watch?v=Ymt47wXUDEU)

> ### Pr치ctica: Observa la intensidad YCbCr
> Puedes visualizar la intensidad Y (luma) para una l칤nea espec칤fica de un v칤deo utilizando el [filtro de osciloscopio de FFmpeg](https://ffmpeg.org/ffmpeg-filters.html#oscilloscope).
> ```bash
> ffplay -f lavfi -i 'testsrc2=size=1280x720:rate=30000/1001,format=yuv420p' -vf oscilloscope=x=0.5:y=200/720:s=1:c=1
> ```
> ![y color oscilloscope](/i/ffmpeg_oscilloscope.png "y color oscilloscope")

## Tipos de fotogramas

Ahora podemos continuar y tratar de eliminar la **redundancia en el tiempo**, pero antes de hacerlo, establezcamos algunas terminolog칤as b치sicas. Supongamos que tenemos una pel칤cula con 30 fps. Aqu칤 est치n sus primeros 4 fotogramas.

![ball 1](/i/smw_background_ball_1.png "ball 1") ![ball 2](/i/smw_background_ball_2.png "ball 2") ![ball 3](/i/smw_background_ball_3.png "ball 3")
![ball 4](/i/smw_background_ball_4.png "ball 4")

Podemos ver **muchas repeticiones** dentro de los fotogramas, como **el fondo azul**, que no cambia del fotograma 0 al fotograma 3. Para abordar este problema, podemos **categorizarlos abstractamente** en tres tipos de fotogramas.

### I Frame (intra, keyframe)

Un *I-frame* (*reference*, *keyframe*, *intra*, en espa침ol fotograma o cuadro de referencia) es un **fotograma aut칩nomo**. No depende de nada para ser renderizado, un *I-frame* se parece a una fotograf칤a est치tica. Por lo general, el primer fotograma es un *I-frame*, pero veremos *I-frames* insertados regularmente entre otros tipos de fotogramas.

![ball 1](/i/smw_background_ball_1.png "ball 1")

### P Frame (predicted)

Un *P-frame* (en espa침ol, fotograma o cuadro predictivo) aprovecha el hecho de que casi siempre l**a imagen actual se puede renderizar utilizando el fotograma anterior**. Por ejemplo, en el segundo fotograma, el 칰nico cambio fue que la pelota se movi칩 hacia adelante. Podemos **reconstruir el fotograma 1, utilizando solo la diferencia y haciendo referencia al fotograma anterior**.

![ball 1](/i/smw_background_ball_1.png "ball 1") <-  ![ball 2](/i/smw_background_ball_2_diff.png "ball 2")

> #### Pr치ctica: Un v칤deo con un solo I-frame
> Dado que un fotograma P utiliza menos datos, 쯣or qu칠 no podemos codificar un [video entero con un solo *I-frame* y todos los dem치s siendo *P-frames*?](/encoding_pratical_examples.md#1-i-frame-and-the-rest-p-frames)
>
> Despu칠s de codificar este v칤deo, comienza a verlo y **busca una parte avanzada** del v칤deo; notar치s que **toma algo de tiempo** llegar realmente a esa parte. Esto se debe a que un ***P-frame* necesita un fotograma de referencia** (como un *I-frame*, por ejemplo) para renderizarse.
>
> Otra prueba r치pida que puedes hacer es codificar un v칤deo utilizando un solo *I-frame* y luego [codificarlo insertando un *I-frame* cada 2 segundos](/encoding_pratical_examples.md#1-i-frames-per-second-vs-05-i-frames-per-second) y **comprobar el tama침o de cada versi칩n**.

### B Frame (bi-predictive)

쯈u칠 pasa si hacemos referencia a los fotogramas pasados y futuros para proporcionar una compresi칩n a칰n mejor? Eso es b치sicamente lo que hace un *B-frame*.

![ball 1](/i/smw_background_ball_1.png "ball 1") <-  ![ball 2](/i/smw_background_ball_2_diff.png "ball 2") -> ![ball 3](/i/smw_background_ball_3.png "ball 3")

> #### Pr치ctica: Compara v칤deos con B-frame
> Puedes generar dos versiones, una con *B-frames* y otra sin ning칰n [*B-frame* en absoluto](/encoding_pratical_examples.md#no-b-frames-at-all) y verificar el tama침o del archivo y la calidad.

### Resumen

Estos tipos de fotogramas se utilizan para **proporcionar una mejor compresi칩n**. Veremos c칩mo sucede esto en la pr칩xima secci칩n, pero por ahora podemos pensar en un ***I-frame* como costoso, un *P-frame* como m치s econ칩mico y el m치s econ칩mico es el *B-frame*.**

![frame types example](/i/frame_types.png "frame types example")

## Redundancia temporal (inter prediction)

Vamos a explorar las opciones que tenemos para reducir las **repeticiones en el tiempo**, este tipo de redundancia se puede resolver con t칠cnicas de **interpredicci칩n**.

Intentaremos **utilizar menos bits** para codificar la secuencia de los fotogramas 0 y 1.

![original frames](/i/original_frames.png "original frames")

Una cosa que podemos hacer es una resta, simplemente **restamos el fotograma 1 del fotograma 0** y obtenemos lo que necesitamos para **codificar el residual**.

![delta frames](/i/difference_frames.png "delta frames")

Pero, 쯤u칠 pasa si te digo que hay un **m칠todo mejor** que utiliza incluso menos bits? Primero, tratemos el `fotograma 0` como una colecci칩n de particiones bien definidas y luego intentaremos emparejar los bloques del `fotograma 0` en el `fotograma 1`. Podemos pensar en ello como una **estimaci칩n de movimiento**.

> ### Wikipedia - compensaci칩n de movimiento por bloques
> "La **compensaci칩n de movimiento por bloques** divide el fotograma actual en bloques no superpuestos, y el vector de compensaci칩n de movimiento **indica de d칩nde provienen esos bloques** (una idea err칩nea com칰n es que el fotograma anterior se divide en bloques no superpuestos y los vectores de compensaci칩n de movimiento indican hacia d칩nde se mueven esos bloques). Los bloques de origen suelen superponerse en el fotograma fuente. Algunos algoritmos de compresi칩n de v칤deo ensamblan el fotograma actual a partir de piezas de varios fotogramas previamente transmitidos."

![delta frames](/i/original_frames_motion_estimation.png "delta frames")

Podr칤amos estimar que la pelota se movi칩 de `x=0, y=25` a `x=6, y=26`, los valores de **x** e **y** son los **vectores de movimiento**. Un **paso adicional** que podemos dar para ahorrar bits es **codificar solo la diferencia del vector de movimiento** entre la 칰ltima posici칩n del bloque y la predicci칩n, por lo que el vector de movimiento final ser칤a `x=6 (6-0), y=1 (26-25)`.

> En una situaci칩n del mundo real, esta **pelota se dividir칤a en n particiones**, pero el proceso es el mismo.

Los objetos en el fotograma se **mueven de manera tridimensional**, la pelota puede volverse m치s peque침a cuando se mueve al fondo. Es normal que no encontremos una coincidencia perfecta con el bloque que intentamos encontrar. Aqu칤 hay una vista superpuesta de nuestra estimaci칩n frente a la imagen real.

![motion estimation](/i/motion_estimation.png "motion estimation")

Pero podemos ver que cuando aplicamos la **estimaci칩n de movimiento**, los **datos a codificar son m치s peque침os** que cuando simplemente usamos t칠cnicas de fotograma delta.

![motion estimation vs delta ](/i/comparison_delta_vs_motion_estimation.png "motion estimation delta")

> ### C칩mo se ver칤a la compensaci칩n de movimiento real
> Esta t칠cnica se aplica a todos los bloques, muy a menudo una pelota se dividir칤a en m치s de un bloque.
>  ![real world motion compensation](/i/real_world_motion_compensation.png "real world motion compensation")
> Fuente: https://web.stanford.edu/class/ee398a/handouts/lectures/EE398a_MotionEstimation_2012.pdf

Puedes [experimentar con estos conceptos utilizando jupyter](/frame_difference_vs_motion_estimation_plus_residual.ipynb).

> #### Pr치ctica: Observa los vectores de movimiento
> Podemos [generar un v칤deo con la interpredicci칩n (vectores de movimiento) con ffmpeg.](/encoding_pratical_examples.md#generate-debug-video)
>
> ![inter prediction (motion vectors) with ffmpeg](/i/motion_vectors_ffmpeg.png "inter prediction (motion vectors) with ffmpeg")
>
> O podemos usar el [Intel Video Pro Analyzer](https://software.intel.com/en-us/intel-video-pro-analyzer) (que es de pago, pero hay una versi칩n de prueba gratuita que te limita a trabajar solo con los primeros 10 fotogramas).
>
> ![inter prediction intel video pro analyzer](/i/inter_prediction_intel_video_pro_analyzer.png "inter prediction intel video pro analyzer")

## Redundancia Espacial (intra prediction)

Si analizamos **cada fotograma** en un v칤deo, veremos que tambi칠n hay **muchas 치reas que est치n correlacionadas**.

![](/i/repetitions_in_space.png)

Vamos a trav칠s de un ejemplo. Esta escena est치 compuesta principalmente por colores azules y blancos.

![](/i/smw_bg.png)

Este es un `I-frame` y **no podemos usar fotogramas anteriores** para hacer una predicci칩n, pero a칰n podemos comprimirlo. Codificaremos la selecci칩n de bloques en rojo. Si **observamos a sus vecinos**, podemos **estimar** que hay una **tendencia de colores a su alrededor**.

![](/i/smw_bg_block.png)

Vamos a **predecir** que el fotograma continuar치 **extendiendo los colores verticalmente**, lo que significa que los colores de los **p칤xeles desconocidos mantendr치n los valores de sus vecinos**.

![](/i/smw_bg_prediction.png)

Nuestra **predicci칩n puede ser incorrecta**, por esa raz칩n necesitamos aplicar esta t칠cnica (**intra prediction**) y luego **restar los valores reales**, lo que nos da el bloque residual, lo que resulta en una matriz mucho m치s compresible en comparaci칩n con la original.

![](/i/smw_residual.png)

Existen muchos tipos diferentes de este tipo de predicci칩n. La que se muestra aqu칤 es una forma de predicci칩n plana recta, donde los p칤xeles de la fila superior del bloque se copian fila por fila dentro del bloque. La predicci칩n plana tambi칠n puede tener una componente angular, donde se utilizan p칤xeles tanto de la izquierda como de la parte superior para ayudar a predecir el bloque actual. Y tambi칠n existe la predicci칩n DC, que implica tomar el promedio de las muestras justo arriba y a la izquierda del bloque.

> #### Pr치ctica: Observa las intra predictions
> Puedes [generar con ffmpeg un v칤deo con macrobloques y sus predicciones.](/encoding_pratical_examples.md#generate-debug-video) Por favor verifica la documentaci칩n de ffmpeg para comprender el [significado de cada bloque de color](https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors?version=7#AnalyzingMacroblockTypes).
>
> ![intra prediction (macro blocks) with ffmpeg](/i/macro_blocks_ffmpeg.png "inter prediction (motion vectors) with ffmpeg")
>
> O puedes usar [Intel Video Pro Analyzer](https://software.intel.com/en-us/intel-video-pro-analyzer) (que es de pago, pero hay una versi칩n de prueba gratuita que te limita a trabajar solo con los primeros 10 fotogramas).
>
> ![intra prediction intel video pro analyzer](/i/intra_prediction_intel_video_pro_analyzer.png "intra prediction intel video pro analyzer")

# 쮺칩mo funciona un c칩dec de v칤deo?

## 쯈u칠? 쯇or qu칠? 쮺칩mo?

**쯈u칠?** Es un software o hardware que comprime o descomprime v칤deo digital. **쯇or qu칠?** El mercado y la sociedad demandan v칤deos de mayor calidad con ancho de banda o almacenamiento limitados. Recuerdas cuando [calculamos el ancho de banda necesario](#terminolog칤a-b치sica) para 30 fps, 24 bits por p칤xel, resoluci칩n de un v칤deo de 480x240? Fueron **82.944 Mbps** sin aplicar compresi칩n. Es la 칰nica forma de entregar HD/FullHD/4K en televisores e Internet. **쮺칩mo?** Echaremos un vistazo breve a las t칠cnicas principales aqu칤.

> **CODEC vs Contenedor**
>
> Un error com칰n que cometen los principiantes es confundir el CODEC de v칤deo digital y el [contenedor de v칤deo digital](https://en.wikipedia.org/wiki/Container_format). Podemos pensar en los **contenedores** como un formato que contiene metadatos del v칤deo (y posiblemente tambi칠n audio), y el **v칤deo comprimido** se puede ver como su contenido.
>
> Por lo general, la extensi칩n de un archivo de v칤deo define su contenedor de v칤deo. Por ejemplo, el archivo `video.mp4` probablemente es un contenedor **[MPEG-4 Part 14](https://en.wikipedia.org/wiki/MP4_file_format)** y un archivo llamado `video.mkv` probablemente es un **[matroska](https://en.wikipedia.org/wiki/Matroska)**. Para estar completamente seguro sobre el c칩dec y el formato del contenedor, podemos usar [ffmpeg o mediainfo](/encoding_pratical_examples.md#inspect-stream).

## Historia

Antes de adentrarnos en el funcionamiento interno de un c칩dec gen칠rico, echemos un vistazo atr치s para comprender un poco mejor algunos c칩decs de v칤deo antiguos.

El c칩dec de v칤deo [H.261](https://en.wikipedia.org/wiki/H.261) naci칩 en 1990 (t칠cnicamente en 1988) y fue dise침ado para trabajar con **tasas de datos de 64 kbit/s**. Ya utilizaba ideas como el *chroma subsampling*, el macrobloque, etc. En el a침o 1995, se public칩 el est치ndar de c칩dec de v칤deo **H.263** y continu칩 siendo extendido hasta 2001.

En 2003 se complet칩 la primera versi칩n de **H.264/AVC**. En el mismo a침o, **On2 Technologies** (anteriormente conocida como Duck Corporation) lanz칩 su c칩dec de v칤deo como una compresi칩n de v칤deo con p칠rdida **libre de regal칤as** llamada **VP3**. En 2008, **Google compr칩** esta empresa y lanz칩 **VP8** en el mismo a침o. En diciembre de 2012, Google lanz칩 **VP9**, el cual es **compatible con aproximadamente el  del mercado de navegadores** (incluyendo m칩viles).

  **[AV1](https://en.wikipedia.org/wiki/AOMedia_Video_1)** es un nuevo c칩dec de v칤deo **libre de regal칤as** y de c칩digo abierto que est치 siendo dise침ado por la [Alliance for Open Media (AOMedia)](http://aomedia.org/), la cual est치 compuesta por **empresas como Google, Mozilla, Microsoft, Amazon, Netflix, AMD, ARM, NVidia, Intel y Cisco**, entre otras. La **primera versi칩n**, 0.1.0 del c칩dec de referencia, **se public칩 el 7 de abril de 2016**.

![codec history timeline](/i/codec_history_timeline.png "codec history timeline")

> #### El nacimiento de AV1
>
> A principios de 2015, Google estaba trabajando en [VP10](https://en.wikipedia.org/wiki/VP9#Successor:_from_VP10_to_AV1), Xiph (Mozilla) estaba trabajando en [Daala](https://xiph.org/daala/) y Cisco lanz칩 su c칩dec de video libre de regal칤as llamado [Thor](https://tools.ietf.org/html/draft-fuldseth-netvc-thor-03).
>
> Entonces, MPEG LA anunci칩 por primera vez l칤mites anuales para HEVC (H.265) y tarifas 8 veces m치s altas que H.264, pero pronto cambiaron las reglas nuevamente:
> * **sin l칤mite anual**,
> * **tarifa por contenido** (0.5% de los ingresos) y
> * **tarifas por unidad aproximadamente 10 veces m치s altas que H.264**.
>
> La [Alliance for Open Media](http://aomedia.org/about/) fue creada por empresas de fabricantes de hardware (Intel, AMD, ARM, Nvidia, Cisco), distribuci칩n de contenido (Google, Netflix, Amazon), mantenimiento de navegadores (Google, Mozilla) y otros.
>
> Las empresas ten칤an un objetivo com칰n, un c칩dec de v칤deo libre de regal칤as, y as칤 naci칩 AV1 con una [licencia de patente mucho m치s simple](http://aomedia.org/license/patent/). **Timothy B. Terriberry** hizo una excelente presentaci칩n, que es la fuente de esta secci칩n, sobre la [concepci칩n de AV1, el modelo de licencia y su estado actual](https://www.youtube.com/watch?v=lzPaldsmJbk).
>
> Te sorprender치 saber que puedes **analizar el c칩dec AV1 a trav칠s de tu navegador**, visita https://arewecompressedyet.com/analyzer/
>
> ![av1 browser analyzer](/i/av1_browser_analyzer.png "av1 browser analyzer")
>
> PD: Si deseas aprender m치s sobre la historia de los c칩decs, debes comprender los conceptos b치sicos detr치s de las [patentes de compresi칩n de v칤deo](https://www.vcodex.com/video-compression-patents/).

## Un c칩dec gen칠rico

Vamos a presentar los **mecanismos principales detr치s de un c칩dec de v칤deo gen칠rico**, pero la mayor칤a de estos conceptos son 칰tiles y se utilizan en c칩decs modernos como VP9, AV1 y HEVC. Aseg칰rate de entender que vamos a simplificar las cosas MUCHO. A veces usaremos un ejemplo real (principalmente H.264) para demostrar una t칠cnica.

## Paso 1 - Particionado de im치genes

El primer paso es **dividir el fotograma** en varias **particiones, sub-particiones** y m치s all치.

![picture partitioning](/i/picture_partitioning.png "picture partitioning")

Pero, **쯣or qu칠?**. Hay muchas razones, por ejemplo, cuando dividimos la imagen, podemos trabajar las predicciones de manera m치s precisa, utilizando peque침as particiones para las partes m칩viles y particiones m치s grandes para un fondo est치tico.

Por lo general, los c칩decs **organizan estas particiones** en *slices* (o *tiles*), macrobloques (o unidades de 치rbol de codificaci칩n) y muchas sub-particiones. El tama침o m치ximo de estas particiones var칤a, HEVC establece 64x64 mientras que AVC usa 16x16, pero las sub-particiones pueden alcanzar tama침os de 4x4.

Recuerdas que aprendimos c칩mo se **clasifican los fotogramas**? Bueno, tambi칠n puedes **aplicar esas ideas a los bloques**, por lo tanto, podemos tener *I-Slice*, *B-Slice*, *I-Macroblock*, etc.

> ### Pr치ctica: Verificar particiones
> Podemos usar [Intel Video Pro Analyzer](https://software.intel.com/en-us/intel-video-pro-analyzer)  (que es de pago, pero hay una versi칩n de prueba gratuita que limita el trabajo a los primeros 10 fotogramas). Aqu칤 se analizan las [particiones de VP90](/encoding_pratical_examples.md#transcoding).
>
> ![VP9 partitions view intel video pro analyzer ](/i/paritions_view_intel_video_pro_analyzer.png "VP9 partitions view intel video pro analyzer")

## Paso 2 - Predicciones

Una vez que tenemos las particiones, podemos hacer predicciones sobre ellas. Para la [inter prediction](#redundancia-temporal-inter-prediction), necesitamos **enviar los vectores de movimiento y el residual**, y para la [intra prediction](#redundancia-espacial-intra-prediction), **enviaremos la direcci칩n de predicci칩n y el residual** tambi칠n.

## Paso 3 - Transformaci칩n

Despu칠s de obtener el bloque residual (`partici칩n predicha - partici칩n real`), podemos **transformarlo** de tal manera que nos permita saber cu치les **p칤xeles podemos descartar** mientras mantenemos la **calidad general**. Hay algunas transformaciones para este comportamiento espec칤fico.

Aunque existen [otras transformaciones](https://en.wikipedia.org/wiki/List_of_Fourier-related_transforms#Discrete_transforms), examinaremos m치s de cerca la transformada coseno discreta (DCT). Las principales caracter칤sticas de la [**DCT**](https://en.wikipedia.org/wiki/Discrete_cosine_transform) son las siguientes:

* **convierte** bloques de **p칤xeles** en bloques del mismo tama침o de los **coeficientes de frecuencia**.
* **compacta** la energ칤a, lo que facilita eliminar la redundancia espacial.
* es **reversible**, es decir, se puede revertir a p칤xeles.

> El 2 de febrero de 2017, Cintra, R. J. y Bayer, F. M publicaron su art칤culo [DCT-like Transform for Image Compression Requires 14 Additions Only](https://arxiv.org/abs/1702.00817).

No te preocupes si no comprendiste los beneficios de cada punto, intentaremos realizar algunos experimentos para ver el valor real de esto.

Tomemos el siguiente **bloque de p칤xeles** (8x8):

![pixel values matrix](/i/pixel_matrice.png "pixel values matrix")

Lo cual se representa en la siguiente imagen de bloque (8x8):

![pixel values matrix](/i/gray_image.png "pixel values matrix")

Cuando **aplicamos la DCT** a este bloque de p칤xeles, obtenemos el **bloque de coeficientes** (8x8):

![coefficients values](/i/dct_coefficient_values.png "coefficients values")

Y si representamos este bloque de coeficientes, obtendremos esta imagen:

![dct coefficients image](/i/dct_coefficient_image.png "dct coefficients image")

Como puedes ver, no se parece en nada a la imagen original, podr칤amos notar que el **primer coeficiente** es muy diferente de todos los dem치s. Este primer coeficiente se conoce como el coeficiente DC, que representa **todas las muestras** en el array de entrada, algo **similar a un promedio**.

Este bloque de coeficientes tiene una propiedad interesante, que es que separa los componentes de alta frecuencia de los de baja frecuencia.

![dct frequency coefficients property](/i/dctfrequ.jpg "dct frequency coefficients property")

En una imagen, la **mayor parte de la energ칤a** se concentrar치 en las [**frecuencias m치s bajas**](https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm), por lo que si transformamos una imagen en sus componentes de frecuencia y **eliminamos los coeficientes de frecuencia m치s altos**, podemos r**educir la cantidad de datos necesarios** para describir la imagen sin sacrificar demasiada calidad de imagen.

> La frecuencia significa cu치n r치pido cambia una se침al.

Intentemos aplicar el conocimiento que adquirimos en la prueba, convirtiendo la imagen original a su dominio de frecuencia (bloque de coeficientes) usando DCT y luego descartando parte (67%) de los coeficientes menos importantes, principalmente la parte inferior derecha.

Primero, lo convertimos a su **dominio de frecuencia**.

![coefficients values](/i/dct_coefficient_values.png "coefficients values")

A continuaci칩n, descartamos parte (67%) de los coeficientes, principalmente la parte inferior derecha.

![zeroed coefficients](/i/dct_coefficient_zeroed.png "zeroed coefficients")

Finalmente, reconstruimos la imagen a partir de este bloque de coeficientes descartados (recuerda, debe ser reversible) y la comparamos con la original.

![original vs quantized](/i/original_vs_quantized.png "original vs quantized")

Como podemos ver, se asemeja a la imagen original pero introduce muchas diferencias con respecto a la original. **Descartamos el 67.1875%** y a칰n as칤 pudimos obtener algo similar a la original. Podr칤amos descartar de manera m치s inteligente los coeficientes para tener una mejor calidad de imagen, pero eso es el pr칩ximo tema.

> **Cada coeficiente se forma utilizando todos los p칤xeles**
>
> Es importante destacar que cada coeficiente no se asigna directamente a un solo p칤xel, sino que es una suma ponderada de todos los p칤xeles. Este gr치fico asombroso muestra c칩mo se calcula el primer y segundo coeficiente, utilizando pesos 칰nicos para cada 칤ndice.
>
> ![dct calculation](/i/applicat.jpg "dct calculation")
>
> Fuente: https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm
>
> Tambi칠n puedes intentar [visualizar la DCT mirando una imagen simple](/dct_better_explained.ipynb) formada sobre la base de la DCT. Por ejemplo, aqu칤 tienes c칩mo [se forma el car치cter A](https://en.wikipedia.org/wiki/Discrete_cosine_transform#Example_of_IDCT) utilizando el peso de cada coeficiente.
>
> ![](https://upload.wikimedia.org/wikipedia/commons/5/5e/Idct-animation.gif )




<br/>

> ### Pr치ctica: Descartar diferentes coeficientes
> Puedes jugar con la [transformaci칩n DCT](/uniform_quantization_experience.ipynb).

## Paso 4 - Cuantizaci칩n

Cuando descartamos algunos de los coeficientes en el 칰ltimo paso (transformaci칩n), de alguna manera hicimos una forma de cuantizaci칩n. En este paso es donde elegimos perder informaci칩n (la **parte perdida**) o, en t칠rminos simples, **cuantificamos coeficientes para lograr la compresi칩n**.

쮺칩mo podemos cuantificar un bloque de coeficientes? Un m칠todo simple ser칤a una cuantificaci칩n uniforme, donde tomamos un bloque, lo **dividimos por un valor 칰nico** (10) y redondeamos este valor.

![quantize](/i/quantize.png "quantize")

쮺칩mo podemos **revertir** (re-cuantificar) este bloque de coeficientes? Podemos hacerlo **multiplicando el mismo valor** (10) por el que lo dividimos inicialmente.

![re-quantize](/i/re-quantize.png "re-quantize")

Este **enfoque no es el mejor** porque no tiene en cuenta la importancia de cada coeficiente. Podr칤amos usar una **matriz de cuantizadores** en lugar de un solo valor. Esta matriz puede explotar la propiedad de la DCT, cuantificando m치s los coeficientes de la parte inferior derecha y menos los de la parte superior izquierda. El [JPEG utiliza un enfoque similar](https://www.hdm-stuttgart.de/~maucher/Python/MMCodecs/html/jpegUpToQuant.html); puedes consultar el [c칩digo fuente para ver esta matriz](https://github.com/google/guetzli/blob/master/guetzli/jpeg_data.h#L40).

> ### Pr치ctica: Cuantizaci칩n
> Puedes experimentar con la [cuantizaci칩n aqu칤](/dct_experiences.ipynb).

## Paso 5 - Codificaci칩n de la entrop칤a

Despu칠s de cuantificar los datos (bloques/slices/fotogramas de im치genes), a칰n podemos comprimirlos de manera sin p칠rdida. Hay muchas formas (algoritmos) de comprimir datos. Vamos a experimentar brevemente con algunos de ellos. Para una comprensi칩n m치s profunda, puedes leer el incre칤ble libro [Understanding Compression: Data Compression for Modern Developers](https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/).

### Codificaci칩n VLC

Supongamos que tenemos una secuencia de s칤mbolos: **a**, **e**, **r** y **t**; y su probabilidad (de 0 a 1) se representa en esta tabla.

|             | a   | e   | r    | t   |
|-------------|-----|-----|------|-----|
| probabilidad| 0.3 | 0.3 | 0.2 |  0.2 |

Podemos asignar c칩digos binarios 칰nicos (preferiblemente peque침os) a los m치s probables y c칩digos m치s grandes a los menos probables.

|             | a   | e   | r    | t   |
|-------------|-----|-----|------|-----|
| probabilidad | 0.3 | 0.3 | 0.2 | 0.2 |
| c칩digo binario | 0 | 10 | 110 | 1110 |

Comprimamos la secuencia **eat**, asumiendo que gastar칤amos 8 bits para cada s칤mbolo, gastar칤amos **24 bits** sin ninguna compresi칩n. Pero en caso de que reemplacemos cada s칤mbolo por su c칩digo, podemos ahorrar espacio.

El primer paso es codificar el s칤mbolo **e**, que es `10`, y el segundo s칤mbolo es **a**, que se agrega (no en un sentido matem치tico) como `[10][0]`, y finalmente el tercer s칤mbolo **t**, lo que hace que nuestra secuencia de bits comprimida final sea `[10][0][1110]` o `1001110`, que solo requiere **7 bits** (3.4 veces menos espacio que el original).

Observa que cada c칩digo debe ser un c칩digo 칰nico y prefijado [Huffman puede ayudarte a encontrar estos n칰meros](https://en.wikipedia.org/wiki/Huffman_coding). Aunque tiene algunos problemas, todav칤a hay [c칩decs de v칤deo que ofrecen este m칠todo](https://en.wikipedia.org/wiki/Context-adaptive_variable-length_coding) y es el algoritmo para muchas aplicaciones que requieren compresi칩n.

Tanto el codificador como el decodificador **deben conocer** la tabla de s칤mbolos con sus c칩digos, por lo tanto, tambi칠n debes enviar la tabla.

### Codificaci칩n aritm칠tica

Supongamos que tenemos una secuencia de s칤mbolos: **a**, **e**, **r**, **s** y **t**; y su probabilidad se representa en esta tabla.

|              | a   | e   | r    | s    | t   |
|--------------|-----|-----|------|------|-----|
| probabilidad | 0.3 | 0.3 | 0.15 | 0.05 | 0.2 |

Con esta tabla en mente, podemos construir rangos que contengan todos los posibles s칤mbolos ordenados por los m치s frecuentes.

![initial arithmetic range](/i/range.png "initial arithmetic range")

Ahora codifiquemos la secuencia **eat**, tomamos el primer s칤mbolo **e**, que se encuentra dentro del subrango **0.3 a 0.6** (pero no se incluye), y tomamos este subrango y lo dividimos nuevamente utilizando las mismas proporciones utilizadas anteriormente, pero dentro de este nuevo rango.

![second sub range](/i/second_subrange.png "second sub range")

Continuemos codificando nuestra secuencia **eat**, ahora tomamos el segundo s칤mbolo **a**, que est치 dentro del nuevo subrango **0.3 a 0.39**, y luego tomamos nuestro 칰ltimo s칤mbolo **t** y hacemos el mismo proceso nuevamente y obtenemos el 칰ltimo subrango **0.354 a 0.372**.

![final arithmetic range](/i/arithimetic_range.png "final arithmetic range")

Solo necesitamos elegir un n칰mero dentro del 칰ltimo subrango **0.354 a 0.372**, elijamos **0.36**, pero podr칤amos elegir cualquier n칰mero dentro de este subrango. Con **solo** este n칰mero podremos recuperar nuestra secuencia original **eat**. Si lo piensas, es como si estuvi칠ramos dibujando una l칤nea dentro de rangos de rangos para codificar nuestra secuencia.

![final range traverse](/i/range_show.png "final range traverse")

El **proceso inverso** (tambi칠n conocido como decodificaci칩n) es igualmente sencillo, con nuestro n칰mero **0.36** y nuestro rango original, podemos realizar el mismo proceso pero ahora usando este n칰mero para revelar la secuencia codificada original detr치s de este n칰mero.

Con el primer rango, notamos que nuestro n칰mero encaja en la porci칩n, por lo tanto, es nuestro primer s칤mbolo, ahora dividimos este subrango nuevamente, haciendo el mismo proceso que antes, y notamos que **0.36** encaja en el s칤mbolo **a**, y despu칠s de repetir el proceso llegamos al 칰ltimo s칤mbolo **t** (formando nuestra secuencia original codificada *eat*).

Tanto el codificador como el decodificador **tienen que conocer** la tabla de probabilidades de los s칤mbolos, por lo tanto, tambi칠n debes enviar la tabla.

쮹astante ingenioso, verdad? Las personas son realmente inteligentes para idear una soluci칩n as칤. Algunos [c칩decs de v칤deo utilizan esta t칠cnica](https://en.wikipedia.org/wiki/Context-adaptive_binary_arithmetic_coding) (o al menos la ofrecen como opci칩n).

La idea es comprimir sin p칠rdidas el flujo de bits cuantificados. Sin duda, este art칤culo carece de muchos detalles, razones, compensaciones, etc. Pero [puedes aprender m치s](https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/). Los c칩decs m치s nuevos est치n tratando de utilizar diferentes[ algoritmos de codificaci칩n de entrop칤a como ANS.](https://en.wikipedia.org/wiki/Asymmetric_Numeral_Systems)

> ### Pr치ctica: CABAC vs CAVLC
> Puedes [generar 2 streams, uno con CABAC y otro con CAVLC](https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#cabac-vs-cavlc) y **compara cu치nto tiempo** toma generar cada uno de ellos, como as칤 tambi칠n el **tama침o final**.

## Paso 6 - formato *bitstream*

Despu칠s de completar todos estos pasos, necesitamos **empaquetar los fotogramas comprimidos y el contexto de estos pasos**. Debemos informar expl칤citamente al decodificador sobre **las decisiones tomadas por el codificador**, como el bit depth, el espacio de color, la resoluci칩n, la informaci칩n de predicciones (vectores de movimiento, direcci칩n de intra prediction), perfil, nivel, fps, tipo de fotograma, n칰mero de fotograma y mucho m치s.

Vamos a estudiar superficialmente el *bitstream* de H.264. Nuestro primer paso es [generar un *bitstream* H.264<sup>*</sup> m칤nimo](/encoding_pratical_examples.md#generate-a-single-frame-h264-bitstream), lo podemos hacer utilizando nuestro propio repositorio y [ffmpeg](http://ffmpeg.org/).

```
./s/ffmpeg -i /files/i/minimal.png -pix_fmt yuv420p /files/v/minimal_yuv420.h264
```

> <sup>*</sup> fmpeg agrega, por defecto, todos los par치metros de codificaci칩n como un **SEI NAL**, pronto definiremos qu칠 es un NAL.

Este comando generar치 un *bitstream* H.264 en bruto con un **칰nico fotograma**, de 64x64 p칤xeles, con espacio de color yuv420, utilizando la siguiente imagen como fotograma.

> ![used frame to generate minimal h264 bitstream](/i/minimal.png "used frame to generate minimal h264 bitstream")

### H.264 bitstream

El est치ndar AVC (H.264) define que la informaci칩n se enviar치 en **macro frames** (en el sentido de red), llamadas **[NAL](https://en.wikipedia.org/wiki/Network_Abstraction_Layer)** (Network Abstraction Layer). El objetivo principal de la NAL es proporcionar una representaci칩n de v칤deo "amigable para la red". Este est치ndar debe funcionar en televisores (basados en transmisi칩n) y en Internet (basado en paquetes), entre otros.

![NAL units H.264](/i/nal_units.png "NAL units H.264")

Hay un **[marcador de sincronizaci칩n](https://en.wikipedia.org/wiki/Frame_synchronization)** para definir los l칤mites de las unidades NAL. Cada marcador de sincronizaci칩n tiene un valor de `0x00 0x00 0x01`, excepto el primero, que es `0x00 0x00 0x00 0x01`. Si ejecutamos el comando **hexdump** en el flujo de bits H.264 generado, podemos identificar al menos tres NAL al principio del archivo.

![synchronization marker on NAL units](/i/minimal_yuv420_hex.png "synchronization marker on NAL units")

Como mencionamos antes, el decodificador necesita conocer no solo los datos de la imagen, sino tambi칠n los detalles del v칤deo, el fotograma, los colores, los par치metros utilizados y otros. El primer byte de cada NAL define su categor칤a y **tipo**.

| NAL type id  | Description  |
|---  |---|
| 0  |  Undefined |
| 1  |  Coded slice of a non-IDR picture |
| 2  |  Coded slice data partition A |
| 3  |  Coded slice data partition B |
| 4  |  Coded slice data partition C |
| 5  |  **IDR** Coded slice of an IDR picture |
| 6  |  **SEI** Supplemental enhancement information |
| 7  |  **SPS** Sequence parameter set |
| 8  |  **PPS** Picture parameter set |
| 9  |  Access unit delimiter |
| 10 |  End of sequence |
| 11 |  End of stream |
| ... |  ... |

Normalmente, el primer NAL de un flujo de bits es un **SPS** (Sequence Parameter Set). Este tipo de NAL es responsable de proporcionar informaci칩n sobre las variables de codificaci칩n generales como **perfil**, **nivel**, **resoluci칩n** y otros.

Si omitimos el primer marcador de sincronizaci칩n, podemos decodificar el **primer byte** para saber qu칠 **tipo de NAL** es el primero.

Por ejemplo, el primer byte despu칠s del marcador de sincronizaci칩n es `01100111`, donde el primer bit (`0`) es el campo **forbidden_zero_bit**, los siguientes 2 bits (`11`) nos indican el campo **nal_ref_idc**, que indica si este NAL es un campo de referencia o no, y los siguientes 5 bits (`00111`) nos informan sobre el campo **nal_unit_type**, en este caso, es un NAL de tipo **SPS** (7).

El segundo byte (`binary=01100100, hex=0x64, dec=100`) de un NAL SPS es el campo **profile_idc**, que muestra el perfil que el codificador ha utilizado. En este caso, hemos utilizado el **[high profile](https://en.wikipedia.org/wiki/H.264/MPEG-4_AVC#Profiles)**. Adem치s, el tercer byte contiene varias banderas que determinan el perfil exacto (como restringido o progresivo). Pero en nuestro caso, el tercer byte es 0x00 y, por lo tanto, el codificador ha utilizado solo el *high profile*.

![SPS binary view](/i/minimal_yuv420_bin.png "SPS binary view")

Cuando leemos la especificaci칩n de flujo de bits H.264 para un NAL SPS, encontraremos muchos valores para el **parameter name**, **category** y **description**. Por ejemplo, veamos los campos `pic_width_in_mbs_minus_1` y `pic_height_in_map_units_minus_1`.

| Parameter name  | Category  |  Description  |
|---  |---|---|
| pic_width_in_mbs_minus_1 |  0 | ue(v) |
| pic_height_in_map_units_minus_1 |  0 | ue(v) |

> **ue(v)**: unsigned integer [Exp-Golomb-coded](https://ghostarchive.org/archive/JBwdI)

Si realizamos algunos c치lculos con el valor de estos campos, obtendremos la **resoluci칩n**. Podemos representar `1920 x 1080` usando un valor de `pic_width_in_mbs_minus_1` de `119 ((119 + 1) * macroblock_size = 120 * 16 = 1920)`, nuevamente ahorrando espacio, en lugar de codificar `1920`, lo hicimos con `119`.

Si continuamos examinando nuestro v칤deo creado con una vista binaria (por ejemplo, `xxd -b -c 11 v/minimal_yuv420.h264`), podemos saltar al 칰ltimo NAL que es el propio cuadro.

![h264 idr slice header](/i/slice_nal_idr_bin.png "h264 idr slice header")

Podemos ver los valores de sus primeros 6 bytes: `01100101 10001000 10000100 00000000 00100001 11111111`. Como ya sabemos, el primer byte nos dice qu칠 tipo de NAL es, en este caso, (`00101`) es un **IDR Slice (5)** y podemos inspeccionarlo m치s a fondo:

![h264 slice header spec](/i/slice_header.png "h264 slice header spec")

Utilizando la informaci칩n de la especificaci칩n, podemos decodificar qu칠 tipo de slice (**slice_type**), el n칰mero de fotograma (**frame_num**) y otros campos importantes.

Para obtener los valores de algunos campos (`ue(v), me(v), se(v) o te(v)`), debemos decodificarlos utilizando un decodificador especial llamado [Exponential-Golomb](https://ghostarchive.org/archive/JBwdI). Este m칠todo es **muy eficiente para codificar valores variables**, sobre todo cuando hay muchos valores predeterminados.

> Los valores de **slice_type** y **frame_num** de este v칤deo son 7 (I slice) y 0 (el primer fotograma).

Podemos ver el ***bitstream* como un protocolo**, y si deseas aprender m치s sobre este *bitstream*, consulta la especificaci칩n [ITU H.264]( http://www.itu.int/rec/T-REC-H.264-201610-I). Aqu칤 tienes un diagrama macro que muestra d칩nde reside los datos de la imagen (YUV comprimido).

![h264 bitstream macro diagram](/i/h264_bitstream_macro_diagram.png "h264 bitstream macro diagram")

Podemos explorar otros *bitstreams* como el [*bitstreams* VP9](https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf), [H.265 (HEVC)](http://handle.itu.int/11.1002/1000/11885-en?locatt=format:pdf) o incluso nuestro **nuevo mejor amigo** el [*bitstream* AV1](https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8), [쯥e ven todos similares, no?](http://www.gpac-licensing.com/2016/07/12/vp9-av1-bitstream-format/) Pero una vez que aprendes uno, puedes entender f치cilmente los dem치s.

> ### Pr치ctica: Inspeccionar el *bitstream* H.264
> Podemos [generar un video de un solo fotograma](https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#generate-a-single-frame-video) y usar [mediainfo](https://en.wikipedia.org/wiki/MediaInfo) para inspeccionar su *bitstream* H.264. De hecho, incluso puedes ver el [c칩digo fuente que analiza el *bitstream* h264 (AVC)](https://github.com/MediaArea/MediaInfoLib/blob/master/Source/MediaInfo/Video/File_Avc.cpp).
>
> ![mediainfo details h264 bitstream](/i/mediainfo_details_1.png "mediainfo details h264 bitstream")
>
> Tambi칠n podemos utilizar el [Intel Video Pro Analyzer](https://software.intel.com/en-us/intel-video-pro-analyzer) que es de pago, pero hay una versi칩n de prueba gratuita que limita el trabajo a solo los primeros 10 fotogramas, lo cual est치 bien para fines de aprendizaje.
>
> ![intel video pro analyzer details h264 bitstream](/i/intel-video-pro-analyzer.png "intel video pro analyzer details h264 bitstream")

## Repaso

Es evidente que muchos de los **c칩decs modernos utilizan el mismo modelo que aprendimos**. De hecho, echemos un vistazo al diagrama de bloques del c칩dec de v칤deo Thor, que contiene todos los pasos que estudiamos. La idea es que ahora deber칤as ser capaz de al menos comprender mejor las innovaciones y los documentos de esta 치rea.

![thor_codec_block_diagram](/i/thor_codec_block_diagram.png "thor_codec_block_diagram")

Anteriormente calculamos que necesitar칤amos [139GB de almacenamiento para mantener un archivo de v칤deo de una hora a una resoluci칩n de 720p y 30 fps](#chroma-subsampling) si utilizamos las t칠cnicas que aprendimos aqu칤, como **inter e intra predictions, transformaci칩n, cuantizaci칩n, codificaci칩n de entrop칤a y otras**. Podemos lograrlo, asumiendo que estamos utilizando **0.031 bit por p칤xel**, el mismo v칤deo de calidad perceptible **requiriendo solo 367.82MB en lugar de 139GB** de almacenamiento.

> Elegimos usar **0.031 bit por p칤xel** bas치ndonos en el ejemplo de v칤deo proporcionado aqu칤.

## 쮺칩mo logra H.265 una mejor relaci칩n de compresi칩n que H.264?

Ahora que sabemos m치s sobre c칩mo funcionan los c칩decs, es f치cil entender c칩mo los nuevos c칩decs pueden ofrecer mayores resoluciones con menos bits.

Compararemos AVC y HEVC, ten en cuenta que casi siempre hay un equilibrio entre m치s ciclos de CPU (complejidad) y la velocidad de compresi칩n.

HEVC tiene m치s opciones de **particiones** (y **sub-particiones**) que AVC, m치s **direcciones/치ngulos de intra predictions**, **codificaci칩n de entrop칤a mejorada** y m치s, todas estas mejoras hicieron que H.265 fuera capaz de comprimir un 50% m치s que H.264.

![h264 vs h265](/i/avc_vs_hevc.png "H.264 vs H.265")

# Online streaming
## Arquitectura general

![general architecture](/i/general_architecture.png "general architecture")

[TODO]

## Descarga progresiva y streaming adaptativo

![progressive download](/i/progressive_download.png "progressive download")

![adaptive streaming](/i/adaptive_streaming.png "adaptive streaming")

[TODO]

## Protecci칩n de contenido

Podemos utilizar **un sistema de tokens simple** para proteger el contenido. El usuario sin un token intenta solicitar un video y la CDN (Red de Entrega de Contenido, del ingl칠s *Content Delivery Network*) le proh칤be el acceso, mientras que un usuario con un token v치lido puede reproducir el contenido, funciona de manera bastante similar a la mayor칤a de los sistemas de autenticaci칩n web.

![token protection](/i/token_protection.png "token_protection")

El uso exclusivo de este sistema de tokens todav칤a permite que un usuario descargue un video y lo distribuya. Es aqu칤 d칩nde, los sistemas de **DRM (gesti칩n de derechos digitales, del ingl칠s *digital rights management*)** se pueden utilizar para tratar de evitar esto.

![drm](/i/drm.png "drm")

En los sistemas de producci칩n del mundo real, a menudo se utilizan ambas t칠cnicas para proporcionar autorizaci칩n y autenticaci칩n.

### DRM
#### Soluciones principales

* FPS - [**FairPlay Streaming**](https://developer.apple.com/streaming/fps/)
* PR - [**PlayReady**](https://www.microsoft.com/playready/)
* WV - [**Widevine**](http://www.widevine.com/)


#### 쯈u칠 es?

DRM significa [*Digital Rights Management* o gesti칩n de derechos digitales](https://sander.saares.eu/categories/drm-is-not-a-black-box/), es una forma de proporcionar protecci칩n de derechos de autor para medios digitales, como v칤deo y audio digitales. Aunque se utiliza en muchos lugares, no es universalmente aceptado.

#### 쯇or qu칠?

Los creadores de contenido (principalmente estudios) desean proteger su propiedad intelectual contra la copia para prevenir la redistribuci칩n no autorizada de medios digitales.

#### 쮺칩mo?

Vamos a describir una forma abstracta y gen칠rica de DRM de manera muy simplificada.

Dado un **contenido C1** (por ejemplo, un v칤deo en streaming HLS o DASH), con un **reproductor P1** (por ejemplo, shaka-clappr, exo-player o iOS) en un **dispositivo D1** (por ejemplo, un tel칠fono inteligente, una televisi칩n, una tableta o una computadora de escritorio/port치til) utilizando un **sistema DRM llamado DRM1** (widevine, playready o FairPlay).

El contenido C1 est치 encriptado con una **clave sim칠trica K1** del sistema DRM1, generando el **contenido encriptado C'1**.

![drm general flow](/i/drm_general_flow.jpeg "drm general flow")

El reproductor P1, de un dispositivo D1, tiene dos claves (asim칠tricas), una **clave privada PRK1** (esta clave est치 protegida<sup>1</sup> y solo la conoce **D1**) y una **clave p칰blica PUK1**.

> <sup>1</sup>Protegida: esta protecci칩n puede ser **mediante hardware**, por ejemplo, esta clave puede almacenarse dentro de un chip especial (de solo lectura) que funciona como [una caja negra](https://en.wikipedia.org/wiki/Black_box) para proporcionar la descifrado, o **por software** (menos seguro), el sistema DRM proporciona medios para saber qu칠 tipo de protecci칩n tiene un dispositivo dado.

Cuando el **reproductor P1 quiere reproducir** el **contenido C'1**, debe interactuar con el **sistema DRM DRM1**, proporcionando su clave p칰blica **PUK1**. El sistema DRM DRM1 devuelve la **clave K1 cifrada** con la clave p칰blica del cliente **PUK1**. En teor칤a, esta respuesta es algo que **solo D1 es capaz de descifrar**.

`K1P1D1 = enc(K1, PUK1)`

**P1** utiliza su sistema local de DRM (puede ser un [SOC](https://en.wikipedia.org/wiki/System_on_a_chip), hardware especializado o software), este sistema es **capaz de descifrar** el contenido utilizando su clave privada PRK1, puede descifrar **la clave sim칠trica K1 de la K1P1D1** y **reproducir C'1**. En el mejor caso, las claves no se exponen a trav칠s de la RAM.

 ```
 K1 = dec(K1P1D1, PRK1)

 P1.play(dec(C'1, K1))
 ```

![drm decoder flow](/i/drm_decoder_flow.jpeg "drm decoder flow")

# 쮺칩mo usar jupyter?

Aseg칰rate de tener **docker instalado**, ejecuta `./s/start_jupyter.sh` y sigue las instrucciones en la consola.

# Conferencias

* [DEMUXED](https://demuxed.com/) - puedes [mirar las ediciones pasadas aqu칤.](https://www.youtube.com/channel/UCIc_DkRxo9UgUSTvWVNCmpA).

# Referencias

El contenido m치s rico se encuentra aqu칤, es de donde se extrajo, bas칩 o inspir칩 toda la informaci칩n que vimos en este texto. Puedes profundizar tu conocimiento con estos incre칤bles enlaces, libros, v칤deos, etc.

Cursos online y tutoriales:

* https://www.coursera.org/learn/digital/
* https://people.xiph.org/~tterribe/pubs/lca2012/auckland/intro_to_video1.pdf
* https://xiph.org/video/vid1.shtml
* https://xiph.org/video/vid2.shtml
* https://wiki.multimedia.cx
* https://mahanstreamer.net
* http://slhck.info/ffmpeg-encoding-course
* http://www.cambridgeincolour.com/tutorials/camera-sensors.htm
* http://www.slideshare.net/vcodex/a-short-history-of-video-coding
* http://www.slideshare.net/vcodex/introduction-to-video-compression-13394338
* https://developer.android.com/guide/topics/media/media-formats.html
* http://www.slideshare.net/MadhawaKasun/audio-compression-23398426
* http://inst.eecs.berkeley.edu/~ee290t/sp04/lectures/02-Motion_Compensation_girod.pdf

Libros:

* https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/ref=sr_1_1?s=books&ie=UTF8&qid=1486395327&sr=1-1
* https://www.amazon.com/H-264-Advanced-Video-Compression-Standard/dp/0470516925
* https://www.amazon.com/High-Efficiency-Video-Coding-HEVC/dp/3319068946
* https://www.amazon.com/Practical-Guide-Video-Audio-Compression/dp/0240806301/ref=sr_1_3?s=books&ie=UTF8&qid=1486396914&sr=1-3&keywords=A+PRACTICAL+GUIDE+TO+VIDEO+AUDIO
* https://www.amazon.com/Video-Encoding-Numbers-Eliminate-Guesswork/dp/0998453005/ref=sr_1_1?s=books&ie=UTF8&qid=1486396940&sr=1-1&keywords=jan+ozer

Material de *onboarding*:

* https://github.com/Eyevinn/streaming-onboarding
* https://howvideo.works/
* https://www.aws.training/Details/eLearning?id=17775
* https://www.aws.training/Details/eLearning?id=17887
* https://www.aws.training/Details/Video?id=24750

Especificaciones de *Bitstream*:

* http://www.itu.int/rec/T-REC-H.264-201610-I
* http://www.itu.int/ITU-T/recommendations/rec.aspx?rec=12904&lang=en
* https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf
* http://iphome.hhi.de/wiegand/assets/pdfs/2012_12_IEEE-HEVC-Overview.pdf
* http://phenix.int-evry.fr/jct/doc_end_user/current_document.php?id=7243
* http://gentlelogic.blogspot.com.br/2011/11/exploring-h264-part-2-h264-bitstream.html
* https://forum.doom9.org/showthread.php?t=167081
* https://forum.doom9.org/showthread.php?t=168947

Software:

* https://ffmpeg.org/
* https://ffmpeg.org/ffmpeg-all.html
* https://ffmpeg.org/ffprobe.html
* https://mediaarea.net/en/MediaInfo
* https://www.jongbel.com/
* https://trac.ffmpeg.org/wiki/
* https://software.intel.com/en-us/intel-video-pro-analyzer
* https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8

C칩decs *Non-ITU*:

* https://aomedia.googlesource.com/
* https://github.com/webmproject/libvpx/tree/master/vp9
* https://ghostarchive.org/archive/0W0d8 (was: https://people.xiph.org/~xiphmont/demo/daala/demo1.shtml)
* https://people.xiph.org/~jm/daala/revisiting/
* https://www.youtube.com/watch?v=lzPaldsmJbk
* https://fosdem.org/2017/schedule/event/om_av1/
* https://jmvalin.ca/papers/AV1_tools.pdf

Conceptos de codificaci칩n:

* http://x265.org/hevc-h265/
* http://slhck.info/video/2017/03/01/rate-control.html
* http://slhck.info/video/2017/02/24/vbr-settings.html
* http://slhck.info/video/2017/02/24/crf-guide.html
* https://arxiv.org/pdf/1702.00817v1.pdf
* https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors
* http://web.ece.ucdavis.edu/cerl/ReliableJPEG/Cung/jpeg.html
* http://www.adobe.com/devnet/adobe-media-server/articles/h264_encoding.html
* https://prezi.com/8m7thtvl4ywr/mp3-and-aac-explained/
* https://blogs.gnome.org/rbultje/2016/12/13/overview-of-the-vp9-video-codec/
* https://videoblerg.wordpress.com/2017/11/10/ffmpeg-and-how-to-use-it-wrong/

Ejemplos de v칤deos para pruebas:

* http://bbb3d.renderfarming.net/download.html
* https://www.its.bldrdoc.gov/vqeg/video-datasets-and-organizations.aspx

Miscel치nea:

* https://github.com/Eyevinn/streaming-onboarding
* http://stackoverflow.com/a/24890903
* http://stackoverflow.com/questions/38094302/how-to-understand-header-of-h264
* http://techblog.netflix.com/2016/08/a-large-scale-comparison-of-x264-x265.html
* http://vanseodesign.com/web-design/color-luminance/
* http://www.biologymad.com/nervoussystem/eyenotes.htm
* http://www.compression.ru/video/codec_comparison/h264_2012/mpeg4_avc_h264_video_codecs_comparison.pdf
* https://web.archive.org/web/20100728070421/http://www.csc.villanova.edu/~rschumey/csc4800/dct.html (was: http://www.csc.villanova.edu/~rschumey/csc4800/dct.html)
* http://www.explainthatstuff.com/digitalcameras.html
* http://www.hkvstar.com
* http://www.hometheatersound.com/
* http://www.lighterra.com/papers/videoencodingh264/
* http://www.red.com/learn/red-101/video-chroma-subsampling
* http://www.slideshare.net/ManoharKuse/hevc-intra-coding
* http://www.slideshare.net/mwalendo/h264vs-hevc
* http://www.slideshare.net/rvarun7777/final-seminar-46117193
* http://www.springer.com/cda/content/document/cda_downloaddocument/9783642147029-c1.pdf
* http://www.streamingmedia.com/Articles/Editorial/Featured-Articles/A-Progress-Report-The-Alliance-for-Open-Media-and-the-AV1-Codec-110383.aspx
* http://www.streamingmediaglobal.com/Articles/ReadArticle.aspx?ArticleID=116505&PageNum=1
* http://yumichan.net/video-processing/video-compression/introduction-to-h264-nal-unit/
* https://cardinalpeak.com/blog/the-h-264-sequence-parameter-set/
* https://cardinalpeak.com/blog/worlds-smallest-h-264-encoder/
* https://codesequoia.wordpress.com/category/video/
* https://developer.apple.com/library/content/technotes/tn2224/_index.html
* https://en.wikibooks.org/wiki/MeGUI/x264_Settings
* https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming
* https://en.wikipedia.org/wiki/AOMedia_Video_1
* https://en.wikipedia.org/wiki/Chroma_subsampling#/media/File:Colorcomp.jpg
* https://en.wikipedia.org/wiki/Cone_cell
* https://en.wikipedia.org/wiki/File:H.264_block_diagram_with_quality_score.jpg
* https://en.wikipedia.org/wiki/Inter_frame
* https://en.wikipedia.org/wiki/Intra-frame_coding
* https://en.wikipedia.org/wiki/Photoreceptor_cell
* https://en.wikipedia.org/wiki/Pixel_aspect_ratio
* https://en.wikipedia.org/wiki/Presentation_timestamp
* https://en.wikipedia.org/wiki/Rod_cell
* https://it.wikipedia.org/wiki/File:Pixel_geometry_01_Pengo.jpg
* https://leandromoreira.com.br/2016/10/09/how-to-measure-video-quality-perception/
* https://sites.google.com/site/linuxencoding/x264-ffmpeg-mapping
* https://softwaredevelopmentperestroika.wordpress.com/2014/02/11/image-processing-with-python-numpy-scipy-image-convolution/
* https://tools.ietf.org/html/draft-fuldseth-netvc-thor-03
* https://www.encoding.com/android/
* https://www.encoding.com/http-live-streaming-hls/
* https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm
* https://www.lifewire.com/cmos-image-sensor-493271
* https://www.linkedin.com/pulse/brief-history-video-codecs-yoav-nativ
* https://www.linkedin.com/pulse/video-streaming-methodology-reema-majumdar
* https://www.vcodex.com/h264avc-intra-precition/
* https://www.youtube.com/watch?v=9vgtJJ2wwMA
* https://www.youtube.com/watch?v=LFXN9PiOGtY
* https://www.youtube.com/watch?v=Lto-ajuqW3w&list=PLzH6n4zXuckpKAj1_88VS-8Z6yn9zX_P6
* https://www.youtube.com/watch?v=LWxu4rkZBLw
* https://web.stanford.edu/class/ee398a/handouts/lectures/EE398a_MotionEstimation_2012.pdf
* https://sander.saares.eu/categories/drm-is-not-a-black-box/
