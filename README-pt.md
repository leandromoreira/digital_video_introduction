[üá∫üá∏](/README.md "English")
[üá®üá≥](/README-cn.md "Simplified Chinese")
[üáØüáµ](/README-ja.md "Japanese")
[üáÆüáπ](/README-it.md "Italian")
[üá∞üá∑](/README-ko.md "Korean")
[üá∑üá∫](/README-ru.md "Russian")
[üáßüá∑](/README-pt.md "Portuguese")
[üá™üá∏](/README-es.md "Spanish")

[![license](https://img.shields.io/badge/license-BSD--3--Clause-blue.svg)](https://img.shields.io/badge/license-BSD--3--Clause-blue.svg)

# Introdu√ß√£o

Essa √© uma r√°pida introdu√ß√£o a tecnologia de v√≠deo para pessoas desenvolvedoras de software. Entretanto, queremos que esse documento seja f√°cil o suficiente **para qualquer pessoa aprender**. A id√©ia nasceu de uma [pequena oficina para pessoas interessadas em tecnologia de v√≠deo](https://docs.google.com/presentation/d/17Z31kEkl_NGJ0M66reqr9_uTG6tI5EDDVXpdPKVuIrs/edit#slide=id.p).

O objetivo principal do documento √© introduzir alguns conceitos de v√≠deo digital utilizando um **vocabul√°rio simples, elementos visuais e exemplos pr√°ticos**. Al√©m disso, gostar√≠amos que esse conhecimento estivesse dispon√≠vel em qualquer lugar. Por favor, fique √† vontade para enviar corre√ß√µes, sugest√µes e melhorias. 

Haver√£o exerc√≠cios pr√°ticos que ir√£o necessitar do **docker instalado** e esse reposit√≥rio clonado.  

```bash
git clone https://github.com/leandromoreira/digital_video_introduction.git
cd digital_video_introduction
./setup.sh
```
> **ATENC√ÉO**: quando voc√™ ver um comando `./s/ffmpeg` ou `./s/mediainfo`, isso quer dizer que estamos executando uma **vers√£o do programa dentro de um container**, sendo assim, esse programa j√° possui todas as depend√™ncias instaladas e configuradas para sua execu√ß√£o.

Todos os **exerc√≠cios pr√°ticos devem ser executados da pasta que voc√™ clonou originalmente** esse reposit√≥rio. Para os **exemplos que utilizam Jupyter**, voc√™ deve inicializar o servidor utilizando `./s/start_jupyter.sh` e acessar a URL no seu browser.

# Registro de altera√ß√µes

* Adicionado detalhes sobre sistemas de DRM (Digital Rights Management)
* Vers√£o 1.0.0 lan√ßada 
* Adicionado tradu√ß√£o para chin√™s simplificado
* Adicionado exemplo de filtro oscilosc√≥pio de FFmpeg

# √çndice

- [Introdu√ß√£o](#introdu√ß√£o)
- [√çndice](#√≠ndice)
- [Terminologia B√°sica](#terminologia-b√°sica)
  * [Outras maneiras de codificar uma imagem em cores](#outras-maneiras-de-codificar-uma-imagem-colorida)
  * [Pr√°tica: exerc√≠cio com imagem e cor](#pr√°tica-brincando-com-imagens-e-cores)
  * [DVD √© DAR 4:3](#dvd-√©-dar-43)
  * [Pr√°tica: Verificar propriedades de v√≠deo](#pr√°tica-brincando-com-imagens-e-cores)
- [Remo√ß√£o de redund√¢ncia](#remo√ß√£o-de-redund√¢ncia)
  * [Cores, Lumin√¢ncia e os nossos olhos](#cores-lumin√¢ncia-e-nossos-olhos)
    + [Modelo de cores](#modelo-de-cor)
    + [Convers√£o entre YCbCr e RGB](#convers√£o-entre-ycbcr-e-rgb)
    + [Subamostragem Chroma](#subamostragem-chroma)
    + [Pr√°tica: Verifique um histograma YCbCr](#pr√°tica-verifique-um-histograma-ycbcr)
  * [Tipos de Frames](#tipos-de-frames)
    + [I frame (intra, keyframe)](#i-frame-intra-keyframe)
    + [P frame (predicted)](#p-frame-predicted)
      - [Pr√°tica: Um v√≠deo com somente um I-frame](#hands-on-a-video-with-a-single-i-frame)
    + [B Frame (bi-predictive)](#b-frame-bi-predictive)
      - [Pr√°tica: Compare v√≠deos com B-frame](#hands-on-compare-videos-with-b-frame)
    + [Sum√°rio](#sum√°rio)
  * [Redund√¢ncia Temporal (inter prediction)](#redund√¢ncia-temporal-inter-prediction)
      - [Pr√°tica: Veja os vetores de movimento](#pr√°tica-veja-os-vetores-de-movimento)
  * [Redund√¢ncia Espacial (intra prediction)](#redund√¢ncia-espacial-intra-prediction)
      - [Pr√°tica: Verifique intra prediction](#pr√°tica-verifique-intra-prediction)
- [Como um codec de v√≠deo funciona?](#como-um-codec-de-v√≠deo-funciona)
  * [O qu√™? Por qu√™? Como?](#o-qu√™-por-qu√™-como)
  * [Hist√≥ria](#hist√≥ria)
    + [O nascimento de AV1](#o-nascimento-de-av1)
  * [Um codec gen√©rico](#um-codec-gen√©rico)
  * [Primeiro passo - particionando uma figura](#primeiro-passo-particionando-uma-figura)
    + [Pr√°tica: Verifique as parti√ß√µes](#pr√°tica-verifique-as-parti√ß√µes)
  * [Segundo passo - previs√µes](#segundo-passo-previs√µes)
  * [Terceiro passo - transforma√ß√£o](#terceiro-passo-transforma√ß√£o)
    + [Pr√°tica: jogando com diferentes coeficientes](#pr√°tica-jogando-com-diferentes-coeficientes)
  * [Quarto passo - quantiza√ß√£o](#quartopasso-quantiza√ß√£o)
    + [Pr√°tica: quantiza√ß√£o](#pr√°tica-quantiza√ß√£o)
  * [Quinto passo - Codificando entropia](#quinto-passo-codificando-entropia)
    + [C√≥digo VLC](#c√≥digo-vlc)
    + [C√≥digo Aritm√©tico](#c√≥digo-aritm√©tico)
    + [Pr√°tica: CABAC vs CAVLC](#pr√°tica-jogando-com-diferentes-coeficientes)
  * [Sexto passo - formato bitstream](#sexto-passo-formato-bitstream)
    + [H.264 bitstream](#h264-bitstream)
    + [Pr√°tica: Inspecionar o H.264 bitstream](#pr√°tica-inspecionar-o-h264-bitstream)
  * [Revis√£o](#revis√£o)
  * [Como H.265 consegue uma melhor raz√£o de compress√£o comparado com H.264?](#como-h265-consegue-uma-melhor-raz√£o-de-compress√£o-comparado-com-h264)
- [Transmiss√£o online](#transmiss√£o-online)
  * [Arquitetura geral](#arquitetura-geral)
  * [Download progressivo e transmiss√£o adaptativa](#download-progressivo-e-transmiss√£o-adaptativa)
  * [Prote√ß√£o de conte√∫do](#prote√ß√£o-de-conte√∫do)
- [Como usar o Jupyter](#como-usar-o-jupyter)
- [Confer√™ncias](#confer√™ncias)
- [Refer√™ncias](#refer√™ncias)

# Terminologia B√°sica

Uma **imagem** podem ser pensada como uma **matriz 2d**. Se n√≥s come√ßamos a pensar sobre **cores**, podemos extrapolar essa id√©ia inicial e ver essa imagem como uma **matriz 3d** onde as **dimens√µes adicionais** s√£o utilizadas para armazenar **informa√ß√µes de cor**.

Se escolhemos representar essas cores com [cores prim√°rias (vermelho, verde and azul)](https://pt.wikipedia.org/wiki/Cor_prim%C3%A1ria), podemos definir tr√™s planos: o primeiro plano para o **vermelho**, o segundo para o **verde** e o √∫ltimo para a cor **azul**.

![uma imagem √© uma matriz 3D RGB](/i/image_3d_matrix_rgb.png "Uma imagem √© uma matriz 3D")

Iremos chamar cada ponto nessa matriz de **um pixel** (picture element). Um pixel representa a **intensidade** (geralmente um valor num√©rico) de uma dada cor. Por exemplo, um **pixel vermelho** significa 0 de verde, 0 de azul e o m√°ximo de vermelho. O **pixel cor rosa** pode ser formado a partir de uma combina√ß√£o de tr√™s cores. Usando uma representa√ß√£o num√©rica de 0 at√© 255, onde o pixel rosa √© definido por **Vermelho=255, Verde=192 e Azul=203**.

> #### Outras maneiras de codificar uma imagem colorida
> Muitos outros poss√≠veis modelos podem ser utilizados para representar uma imagem colorida. N√≥s podemos, por exemplo, utilizar uma paleta indexada de cores, onde um √∫nico byte representa cada pixel, ao inv√©s de tr√™s bytes quando utilizando o modelo RGB. Em um modelo como esse, podemos utilizar uma matriz 2D ao inv√©s de uma matriz 3D para representar cor. Dessa maneira, n√≥s iremos salvar mem√≥ria mas teremos potencialmente menos op√ß√µes de cores.
>
> ![paleta do NES](/i/nes-color-palette.png "Paleta de cores do NES")

Por exemplo, olhe para a figura abaixo. O primeiro rosto ali √© totalmente colorido. Os outros rostos s√£o planos vermelho, verde e azul (mostrados em tons de cinza).

![Intensidade de canais de cores RGB](/i/rgb_channels_intensity.png "Intensidade de canais de cores RGB")

N√≥s podemos ver que a **cor vermelha** ser√° a cor que **contribu√≠ra mais** (as partes mais brilhantes no segundo rosto) para a cor final, enquanto que a contribui√ß√£o da **cor azul** pode ser vista **mais concentrada nos olhos do M√°rio** (√∫ltimo rosto) e em algumas partes de suas roupas. Perceba como **todos os planos de cores** (partes escuras) contribuem menos para o **bigode do M√°rio**.

Cada intensidade de cor requer um determinada quantidade de bits, essa quantidade √© chamada de **profundidade de bit**. Digamos que vamos gastar **8 bits** (tendo como valores v√°lidos 0 at√© 255) por cor (plano), dessa forma, n√≥s temos uma **profundidade de cor** de **24 bits** (8 bits vezes 3 planos RGB). Tamb√©m podemos inferir que podemos utilizar 2 na 24 (2^24) diferentes cores.

> **√â √≥timo** para aprender [como uma imagem se transforma em bits](http://www.cambridgeincolour.com/tutorials/camera-sensors.htm).

Uma outra propriedade de uma imagem √© a **resolu√ß√£o**, que √© basicamente o n√∫mero de pixels em uma dimens√£o. √â geralmente representada por altura x largura, por exemplo: a imagem **4x4** abaixo:

![resolu√ß√£o da imagem](/i/resolution.png "resolu√ß√£o da imagem")

> #### Pr√°tica: brincando com imagens e cores
> Voc√™ pode [brincar com imagens e cores](/image_as_3d_array.ipynb) utilizando [jupyter](#how-to-use-jupyter) (python, numpy, matplotlib, etc).
>
> Voc√™ tamb√©m pode aprender [como filtros de imagem (detec√ß√£o de bordas, nitidez, desfoque...) funcionam](/filters_are_easy.ipynb).

Uma outra propriedade que podemos perceber enquanto trabalhamos com imagens ou v√≠deo √© a **propor√ß√£o da tela** que descreve um relacionamento proporcional entre altura e largura de uma imagem ou pixel.

Quando as pessoas falam que um filme ou foto √© **16x9** geralmente elas est√£o se referindo ao  **Display Aspect Ratio (DAR)**, entretanto n√≥s tamb√©m podemos ter diferentes formas de pixels e podemos chamar isso de **Pixel Aspect Ratio (PAR)**.

![display aspect ratio](/i/DAR.png "display aspect ratio")

![pixel aspect ratio](/i/PAR.png "pixel aspect ratio")

> #### DVD √© DAR 4:3
> Mesmo que a resolu√ß√£o real de um DVD seja 704x480, a propor√ß√£o √© mantida em 4:3 considerando que esse filme possui PAR de 10:11 (704x10/480x11)

Finalmente, podemos definir um **v√≠deo** como uma **sucess√£o de *n* frames** no **tempo** que podemos perceber como uma outra dimens√£o, onde *n* √© a quantidade de quadros por segundo ou frames per second (FPS).

![video](/i/video.png "video")

O n√∫mero de bits por segundo necess√°rios para exibir um v√≠deo √© sua **taxa de bits**.

> taxa de bits = largura * altura * profundidade de bits * quadros por segundo

Por exemplo, um v√≠deo com 30 frames por segundo, 24 bits por pixel, resolu√ß√£o de 480x240 necessitar√° de **82.944.000 bits por segundo** ou 82.944 Mbps (30x480x240x24) se n√£o empregarmos nenhum tipo de compress√£o.

Quando a **taxa de bits** √© quase constante, ela √© chamada de taxa de bits constante (**CBR**), mas tamb√©m pode variar, chamada de taxa de bits vari√°vel (**VBR**).

> Este gr√°fico mostra um VBR restrito que n√£o gasta muitos bits enquanto o quadro √© preto.
>
> ![constrained vbr](/i/vbr.png "constrained vbr")

No in√≠cio, os engenheiros criaram uma t√©cnica para dobrar a taxa de quadros percebida de uma exibi√ß√£o de v√≠deo **sem consumir largura de banda extra**. Essa t√©cnica √© conhecida como **v√≠deo entrela√ßado**; basicamente envia metade da tela em 1 "quadro" e a outra metade no pr√≥ximo "quadro".

Hoje, as telas s√£o renderizadas principalmente usando a **t√©cnica de varredura progressiva**. Progressivo √© uma forma de exibir, armazenar ou transmitir imagens em movimento em que todas as linhas de cada quadro s√£o desenhadas em sequ√™ncia.

![interlaced vs progressive](/i/interlaced_vs_progressive.png "interlaced vs progressive")

Agora temos uma ideia de como uma **imagem** √© representada digitalmente, como suas **cores** s√£o arranjadas, quantos **bits por segundo** gastamos para mostrar um v√≠deo, se √© constante (CBR) ou vari√°vel (VBR), com uma determinada **resolu√ß√£o** usando uma determinada **taxa de quadros** e muitos outros termos como entrela√ßado, PAR e outros.

> #### Exerc√≠cio pr√°tico: Verifique as propriedades do v√≠deo
> Voc√™ pode [verificar a maioria das propriedades explicadas com ffmpeg ou mediainfo.](https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#inspect-stream)

# Remo√ß√£o de redund√¢ncia

Aprendemos que n√£o √© vi√°vel usar v√≠deo sem compress√£o; **um √∫nico v√≠deo de uma hora** em resolu√ß√£o de 720p com 30fps **requer 278 GB<sup>*</sup>**. Como **usar apenas algoritmos de compacta√ß√£o de dados sem perdas** como DEFLATE (usado em PKZIP, Gzip e PNG), **n√£o** diminuir√° suficientemente a largura de banda necess√°ria, precisamos encontrar outras maneiras de compactar o v√≠deo.

> <sup>*</sup> Encontramos esse n√∫mero multiplicando 1280 x 720 x 24 x 30 x 3600 (largura, altura, bits por pixel, fps e tempo em segundos)

Para fazer isso, podemos **explorar como nossa vis√£o funciona**. Distinguimos melhor o brilho do que as cores, as **repeti√ß√µes no tempo**, um v√≠deo cont√©m muitas imagens com poucas altera√ß√µes, e as **repeti√ß√µes dentro da imagem**, cada quadro tamb√©m cont√©m muitas √°reas usando o mesmo ou cor semelhante.

## Cores, Lumin√¢ncia e nossos olhos

Nossos olhos s√£o [mais sens√≠veis √† luminosidade do que √†s cores](http://vanseodesign.com/web-design/color-luminance/), voc√™ pode testar isso por si mesmo, olhe para essa imagem.

![luminance vs color](/i/luminance_vs_color.png "luminance vs color")

Se voc√™ n√£o consegue ver que as cores dos **quadrados A e B s√£o id√™nticas** no lado esquerdo, tudo bem, √© nosso c√©rebro nos enganando para **prestarmos mais aten√ß√£o √† luz e √† escurid√£o do que √† cor**. H√° um conector, da mesma cor, no lado direito para que possamos (nosso c√©rebro) facilmente perceber que, na verdade, s√£o a mesma cor.

> **Explica√ß√£o simplista de como nossos olhos funcionam**
> O [olho √© um √≥rg√£o complexo](http://www.biologymad.com/nervoussystem/eyenotes.htm), √© composto por muitas partes, mas estamos principalmente interessados nas c√©lulas cones e bastonetes. O olho [cont√©m cerca de 120 milh√µes de bastonetes e 6 milh√µes de c√©lulas cones](https://en.wikipedia.org/wiki/Photoreceptor_cell).
>
> Para **simplificar bastante**, vamos tentar colocar as cores e a luminosidade na fun√ß√£o das partes do olho. As **[c√©lulas bastonetes](https://en.wikipedia.org/wiki/Rod_cell) s√£o principalmente respons√°veis pela luminosidade** enquanto as **[c√©lulas cones](https://en.wikipedia.org/wiki/Cone_cell) s√£o respons√°veis pela cor**, existem tr√™s tipos de cones, cada um com um pigmento diferente, a saber: [S-cones (azul), M-cones (verde) e L-cones (vermelho)](https://upload.wikimedia.org/wikipedia/commons/1/1e/Cones_SMJ2_E.svg).
>
> Uma vez que temos muitas mais c√©lulas bastonetes (luminosidade) do que c√©lulas cones (cor), pode-se inferir que somos mais capazes de distinguir entre claro e escuro do que cores.
>
> ![eyes composition](/i/eyes.jpg "eyes composition")
>
> **Fun√ß√µes de sensibilidade ao contraste**
>
> Pesquisadores da psicologia experimental e muitos outros campos desenvolveram muitas teorias sobre a vis√£o humana. E uma delas √© chamada de fun√ß√µes de sensibilidade ao contraste. Elas est√£o relacionadas ao espa√ßo e ao tempo da luz e seu valor indica, para uma determinada intensidade de luz inicial, quanto mudan√ßa √© necess√°ria antes que um observador informe que houve uma mudan√ßa. Observe o plural da palavra "fun√ß√£o", isso ocorre porque podemos medir as fun√ß√µes de sensibilidade ao contraste n√£o apenas em preto e branco, mas tamb√©m em cores. O resultado desses experimentos mostra que, na maioria dos casos, nossos olhos s√£o mais sens√≠veis ao brilho do que √† cor.

Uma vez que sabemos que somos mais sens√≠veis √† **luma** (a luminosidade em uma imagem), podemos tentar explor√°-la.

### Modelo de cor

Aprendemos primeiro [como funcionam as imagens coloridas](#terminologia-b√°sica) usando o **modelo RGB** (consulte terminologia b√°sica), mas existem outros modelos tamb√©m. Na verdade, existe um modelo que separa a lumin√¢ncia (brilho) da cromin√¢ncia (cores) e √© conhecido como YCbCr<sup>*</sup>.

> <sup>*</sup> existem mais modelos que fazem a mesma separa√ß√£o.

Este modelo de cor usa **Y** para representar o brilho e dois canais de cor **Cb** (azul crom√°tico) e **Cr** (vermelho crom√°tico). O [YCbCr](https://en.wikipedia.org/wiki/YCbCr) pode ser derivado do RGB e tamb√©m pode ser convertido de volta para o RGB. Usando este modelo, podemos criar imagens totalmente coloridas, como podemos ver abaixo.

![ycbcr example](/i/ycbcr.png "ycbcr example")

### Convers√£o entre YCbCr e RGB

Alguns podem argumentar, como podemos produzir todas as **cores sem usar o verde**?

Para responder a essa pergunta, vamos percorrer uma convers√£o de RGB para YCbCr. Usaremos os coeficientes do  **[standard BT.601](https://en.wikipedia.org/wiki/Rec._601)** recomendado pelo **[grupo ITU-R<sup>*</sup>](https://en.wikipedia.org/wiki/ITU-R)**. O primeiro passo √© **calcular a lumin√¢ncia**, usaremos as constantes sugeridas pelo ITU e substituiremos os valores RGB.

```
Y = 0.299R + 0.587G + 0.114B
```

Uma vez que temos a lumin√¢ncia, podemos **separar as cores** (cromin√¢ncia azul e vermelha):

```
Cb = 0.564(B - Y)
Cr = 0.713(R - Y)
```

E tamb√©m podemos **convert√™-la de volta** e at√© mesmo obter o **verde usando YCbCr**.

```
R = Y + 1.402Cr
B = Y + 1.772Cb
G = Y - 0.344Cb - 0.714Cr
```

> <sup>*</sup> grupos e padr√µes s√£o comuns em v√≠deo digital, eles geralmente definem quais s√£o os padr√µes, por exemplo, [o que √© 4K? qual taxa de quadros devemos usar? resolu√ß√£o? modelo de cores?](https://en.wikipedia.org/wiki/Rec._2020)

Geralmente, **monitores** (monitores, televisores, telas, etc.) utilizam **apenas o modelo RGB**, organizado de maneiras diferentes, veja alguns deles ampliados abaixo:

![pixel geometry](/i/new_pixel_geometry.jpg "pixel geometry")

### Subamostragem Chroma

Com a imagem representada como componentes de lumin√¢ncia e cromin√¢ncia, podemos aproveitar a maior sensibilidade do sistema visual humano para a resolu√ß√£o de lumin√¢ncia em vez de cromin√¢ncia para remover seletivamente informa√ß√µes. A **subamostragem de cromin√¢ncia** √© a t√©cnica de codifica√ß√£o de imagens usando **menor resolu√ß√£o para cromin√¢ncia do que para lumin√¢ncia**.


![ycbcr subsampling resolutions](/i/ycbcr_subsampling_resolution.png "ycbcr subsampling resolutions")


Quanto devemos reduzir a resolu√ß√£o de cromin√¢ncia? Descobriu-se que j√° existem alguns esquemas que descrevem como lidar com a resolu√ß√£o e a combina√ß√£o (`cor final = Y + Cb + Cr`).

Esses esquemas s√£o conhecidos como sistemas de subamostragem e s√£o expressos como uma raz√£o de 3 partes - `a:x:y` - que define a resolu√ß√£o de cromin√¢ncia em rela√ß√£o a um bloco `a x 2` de pixels de lumin√¢ncia.

 * `a` √© a refer√™ncia de amostragem horizontal (geralmente 4)
 * `x`  √© o n√∫mero de amostras de cromin√¢ncia na primeira linha de pixels `a` (resolu√ß√£o horizontal em rela√ß√£o a `a`)
 * `y` √© o n√∫mero de mudan√ßas de amostras de cromin√¢ncia entre a primeira e a segunda linhas de pixels `a`.

> Uma exce√ß√£o a isso existe com 4:1:0, que fornece uma √∫nica amostra de cromin√¢ncia dentro de cada bloco `4 x 4` de resolu√ß√£o de luma.

Os esquemas comuns usados em codecs modernos s√£o: **4:4:4** (sem subsampling), **4:2:2, 4:1:1, 4:2:0, 4:1:0 e 3:1:1**.

> Voc√™ pode acompanhar algumas discuss√µes [para aprender mais sobre a subsampling de cromin√¢ncia](https://github.com/leandromoreira/digital_video_introduction/issues?q=YCbCr).

> **YCbCr 4:2:0 merge**
>
> Aqui est√° um trecho de uma imagem mesclada usando YCbCr 4:2:0, observe que gastamos apenas 12 bits por pixel.
>
> ![YCbCr 4:2:0 merge](/i/ycbcr_420_merge.png "YCbCr 4:2:0 merge")

Aqui est√° um trecho de uma imagem mesclada usando YCbCr 4:2:0, observe que gastamos apenas 12 bits por pixel.

Voc√™ pode ver a mesma imagem codificada pelos principais tipos de subsampling de croma, as imagens na primeira linha s√£o o YCbCr final, enquanto a √∫ltima linha de imagens mostra a resolu√ß√£o croma. √â realmente uma grande vit√≥ria para uma perda t√£o pequena.

![chroma subsampling examples](/i/chroma_subsampling_examples.jpg "chroma subsampling examples")

Anteriormente, t√≠nhamos calculado que precis√°vamos de [278GB de armazenamento para manter um arquivo de v√≠deo com uma hora de resolu√ß√£o 720p e 30fps](#remo√ß√£o-de-redund√¢ncia). Se usarmos **YCbCr 4:2:0** podemos **cortar pela metade esse tamanho (139 GB)**<sup>*</sup>, mas ainda est√° longe do ideal.

> <sup>*</sup> encontramos esse valor multiplicando largura, altura, bits por pixel e fps. Anteriormente, precis√°vamos de 24 bits, agora precisamos apenas de 12.

<br/>

> ### Pr√°tica: Verifique um histograma YCbCr
> Voc√™ pode [verificar o histograma YCbCr com o ffmpeg.](/encoding_pratical_examples.md#generates-yuv-histogram) Essa cena tem uma contribui√ß√£o maior de azul, como mostrado pelo [histograma](https://en.wikipedia.org/wiki/Histogram).
>
> ![ycbcr color histogram](/i/yuv_histogram.png "ycbcr color histogram")

### Cor, luma, lumin√¢ncia, revis√£o de v√≠deo gamma

Assista a este incr√≠vel v√≠deo explicando o que √© luma e aprenda sobre lumin√¢ncia, gamma e cor.
[![Analog Luma - Uma hist√≥ria e explica√ß√£o do v√≠deo](http://img.youtube.com/vi/Ymt47wXUDEU/0.jpg)](http://www.youtube.com/watch?v=Ymt47wXUDEU)

> ### Pratica: Verificar a intensidade do YCbCr
> Voc√™ pode visualizar a intensidade do Y para uma determinada linha de um v√≠deo usando o [filtro oscilosc√≥pio do FFmpeg]((https://ffmpeg.org/ffmpeg-filters.html#oscilloscope).).
> ```bash
> ffplay -f lavfi -i 'testsrc2=size=1280x720:rate=30000/1001,format=yuv420p' -vf oscilloscope=x=0.5:y=200/720:s=1:c=1
> ```
> ![y color oscilloscope](/i/ffmpeg_oscilloscope.png "y color oscilloscope")

## Tipos de Frames

Agora podemos prosseguir e tentar eliminar a **redund√¢ncia temporal**, mas antes disso vamos estabelecer algumas terminologias b√°sicas. Suponha que temos um filme com 30fps, aqui est√£o seus primeiros 4 quadros.

![ball 1](/i/smw_background_ball_1.png "ball 1") ![ball 2](/i/smw_background_ball_2.png "ball 2") ![ball 3](/i/smw_background_ball_3.png "ball 3")
![ball 4](/i/smw_background_ball_4.png "ball 4")

Podemos ver **muitas repeti√ß√µes** dentro dos quadros, como **o fundo azul,** que n√£o muda do quadro 0 ao quadro 3. Para lidar com esse problema, podemos **categoriz√°-los abstratamente** em tr√™s tipos de quadros.

### I Frame (intra, keyframe)

Um quadro I (refer√™ncia, chave, intra) √© um **quadro autocontido**. Ele n√£o depende de nada para ser renderizado, um quadro I se parece com uma foto est√°tica. O primeiro quadro geralmente √© um quadro I, mas veremos quadros I inseridos regularmente entre outros tipos de quadros.

![ball 1](/i/smw_background_ball_1.png "ball 1")

### P Frame (predicted)

Um quadro P (previs√£o) aproveita o fato de que quase sempre a imagem atual pode ser **renderizada usando o quadro anterior.** Por exemplo, no segundo quadro, a √∫nica mudan√ßa foi a bola que se moveu para frente. Podemos **reconstruir o quadro 1, usando apenas a diferen√ßa e referenciando o quadro anterior**.

![ball 1](/i/smw_background_ball_1.png "ball 1") <-  ![ball 2](/i/smw_background_ball_2_diff.png "ball 2")

> #### Hands-on: A video with a single I-frame
> J√° que um quadro P usa menos dados, por que n√£o codificar um [v√≠deo com apenas um quadro I e todos os outros quadros sendo P?](/encoding_pratical_examples.md#1-i-frame-and-the-rest-p-frames)
>
> Depois de codificar esse v√≠deo, comece a assisti-lo e fa√ßa uma **busca por uma parte avan√ßada** do v√≠deo, voc√™ notar√° que **leva algum tempo** para realmente ir para essa parte. Isso acontece porque um **quadro P precisa de um quadro de refer√™ncia** (como um quadro I, por exemplo) para ser renderizado.
>
> Outro teste r√°pido que voc√™ pode fazer √© codificar um v√≠deo usando apenas um quadro I e, em seguida, [codific√°-lo inserindo um quadro I a cada 2s](/encoding_pratical_examples.md#1-i-frames-per-second-vs-05-i-frames-per-second) e **verificar o tamanho de cada vers√£o**.

### B Frame (bi-predictive)

E quanto a referenciar quadros passados e futuros para proporcionar uma compress√£o ainda melhor?! Isso √© basicamente o que um quadro B faz.

![ball 1](/i/smw_background_ball_1.png "ball 1") <-  ![ball 2](/i/smw_background_ball_2_diff.png "ball 2") -> ![ball 3](/i/smw_background_ball_3.png "ball 3")

> #### Hands-on: Compare videos with B-frame
> Voc√™ pode gerar duas vers√µes, uma com quadros B e outra com [nenhum quadro B](/encoding_pratical_examples.md#no-b-frames-at-all) e verificar o tamanho do arquivo, bem como a qualidade.

### Sum√°rio

Esses tipos de quadros s√£o usados para **proporcionar uma melhor compress√£o**. Veremos como isso acontece na pr√≥xima se√ß√£o, mas por enquanto podemos pensar que o **quadro I (I-frame) √© mais caro enquanto o P √© mais barato, mas o mais barato √© o quadro B (B-frame).**


![frame types example](/i/frame_types.png "frame types example")

## Redund√¢ncia Temporal (inter prediction)

Vamos explorar as op√ß√µes que temos para reduzir as **repeti√ß√µes no tempo**, esse tipo de redund√¢ncia pode ser resolvido com t√©cnicas de **inter-prediction**.


Vamos tentar **usar menos bits** para codificar a sequ√™ncia de frames 0 e 1.

![original frames](/i/original_frames.png "original frames")

Uma coisa que podemos fazer √© subtra√ß√£o, simplesmente **subtraindo o frame 1 do frame 0**, obtemos apenas o que precisamos para **codificar o residual**.

![delta frames](/i/difference_frames.png "delta frames")

Mas e se eu te disser que h√° um **m√©todo melhor** que usa ainda menos bits?! Primeiro, vamos tratar o `frame 0` como uma cole√ß√£o de parti√ß√µes bem definidas e, em seguida, tentaremos combinar os blocos do frame 0 no `frame 1`. Podemos pensar nisso como **estimativa de movimento**.

> ### Wikipedia - compensa√ß√£o de movimento por blocos

> "A **compensa√ß√£o de movimento por blocos** divide o quadro atual em blocos n√£o sobrepostos, e o vetor de compensa√ß√£o de movimento **indica de onde esses blocos v√™m** (uma cren√ßa comum √© que o quadro anterior √© dividido em blocos n√£o sobrepostos, e os vetores de compensa√ß√£o de movimento indicam para onde esses blocos se movem). Os blocos de origem geralmente se sobrep√µem no quadro de origem. Alguns algoritmos de compress√£o de v√≠deo montam o quadro atual a partir de pe√ßas de v√°rios quadros transmitidos anteriormente."

![delta frames](/i/original_frames_motion_estimation.png "delta frames")

N√≥s poder√≠amos estimar que a bola se moveu de `x=0, y=25` para `x=6, y=26`, os valores **x** e **y** s√£o os **vetores de movimento**. Um **passo adicional** que podemos fazer para economizar bits √© **codificar apenas a diferen√ßa** do vetor de movimento entre a √∫ltima posi√ß√£o do bloco e a prevista, ent√£o o vetor de movimento final seria `x=6 (6-0), y=1 (26-25)`.

> Em uma situa√ß√£o do mundo real, essa **bola seria dividida em n parti√ß√µes** mas o processo √© o mesmo.

Os objetos na cena se **movem de forma 3D**, a bola pode ficar menor quando se move para o fundo. √â normal que **n√£o encontremos a correspond√™ncia perfeita** para o bloco que tentamos encontrar. Aqui est√° uma vis√£o superposta de nossa estimativa em compara√ß√£o com a imagem real.

![motion estimation](/i/motion_estimation.png "motion estimation")

Mas podemos ver que quando aplicamos a **estimativa de movimento**, os **dados para codificar s√£o menores** do que quando usamos apenas as t√©cnicas de quadro delta.

![motion estimation vs delta ](/i/comparison_delta_vs_motion_estimation.png "motion estimation delta")

> ### Como seria a compensa√ß√£o de movimento real

> Essa t√©cnica √© aplicada a todos os blocos, muitas vezes uma bola seria dividida em mais de um bloco.
>  ![real world motion compensation](/i/real_world_motion_compensation.png "real world motion compensation")
> Fonte: https://web.stanford.edu/class/ee398a/handouts/lectures/EE398a_MotionEstimation_2012.pdf

Voc√™ pode [praticar esses conceitos usando o jupyter](/frame_difference_vs_motion_estimation_plus_residual.ipynb).

> #### Pr√°tica: Veja os vetores de movimento

> Podemos [gerar um v√≠deo com a interpola√ß√£o (vetores de movimento) usando o ffmpeg.](/encoding_pratical_examples.md#generate-debug-video)
>
> ![inter prediction (motion vectors) with ffmpeg](/i/motion_vectors_ffmpeg.png "inter prediction (motion vectors) with ffmpeg")
>
> Ou podemos usar o [Intel Video Pro Analyzer](https://software.intel.com/en-us/intel-video-pro-analyzer) (que √© pago, mas h√° uma vers√£o de teste gratuita que limita voc√™ a trabalhar apenas com os primeiros 10 quadros).
>
> ![inter prediction intel video pro analyzer](/i/inter_prediction_intel_video_pro_analyzer.png "inter prediction intel video pro analyzer")

## Redund√¢ncia Espacial (intra prediction)

Se analisarmos **cada quadro** em um v√≠deo, veremos que tamb√©m h√° **muitas √°reas que est√£o correlacionadas**.

![](/i/repetitions_in_space.png)

Vamos caminhar por um exemplo. Esta cena √© composta principalmente por cores azuis e brancas.

![](/i/smw_bg.png)

Este √© um `I-frame` e **n√£o podemos usar quadros anteriores** para fazer previs√µes, mas ainda assim podemos comprimi-lo. Vamos codificar a sele√ß√£o do bloco vermelho. Se **olharmos para seus vizinhos**, podemos **estimar** que h√° uma **tend√™ncia de cores ao redor dele**.

![](/i/smw_bg_block.png)

Vamos **prever** que o quadro continuar√° a **espalhar as cores verticalmente**, o que significa que as cores dos pixels **desconhecidos ter√£o os valores de seus vizinhos**.

![](/i/smw_bg_prediction.png)

Nossa **previs√£o pode estar errada**, por essa raz√£o, precisamos aplicar essa t√©cnica (**intra-prediction**) e depois **subtrair os valores reais**, o que nos d√° o bloco residual, resultando em uma matriz muito mais compress√≠vel em compara√ß√£o com a original.

![](/i/smw_residual.png)

Existem muitos tipos diferentes desse tipo de previs√£o. O que voc√™ v√™ aqui na imagem √© uma forma de previs√£o planar direta, onde os pixels da linha acima do bloco s√£o copiados linha por linha dentro do bloco. A previs√£o planar tamb√©m pode envolver um componente angular, onde pixels tanto da esquerda quanto da parte superior s√£o usados para ajudar a prever o bloco atual. E h√° tamb√©m a previs√£o DC, que envolve a m√©dia das amostras imediatamente acima e √† esquerda do bloco.

> #### Pr√°tica: Verifique intra prediction
> Voc√™ pode [gerar um v√≠deo com macroblocos e suas previs√µes usando o ffmpeg.](/encoding_pratical_examples.md#generate-debug-video) Por favor, verifique a documenta√ß√£o do ffmpeg para entender o [significado de cada cor de bloco](https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors?version=7#AnalyzingMacroblockTypes).
>
> ![intra prediction (macro blocks) with ffmpeg](/i/macro_blocks_ffmpeg.png "inter prediction (motion vectors) with ffmpeg")
>
> Ou podemos usar o [Intel Video Pro Analyzer](https://software.intel.com/en-us/intel-video-pro-analyzer) (que √© pago, mas h√° uma vers√£o de avalia√ß√£o gratuita que limita voc√™ a trabalhar apenas com os primeiros 10 quadros).
>
> ![intra prediction intel video pro analyzer](/i/intra_prediction_intel_video_pro_analyzer.png "intra prediction intel video pro analyzer")

# Como um codec de v√≠deo funciona?

## O qu√™? Por qu√™? Como?

**O qu√™?** √â um software / hardware que comprime ou descomprime v√≠deo digital. **Por qu√™?** O mercado e a sociedade exigem v√≠deos de alta qualidade com largura de banda ou armazenamento limitado. Lembra quando [calculamos a largura de banda necess√°ria](#terminologia-b√°sica) para 30 quadros por segundo, 24 bits por pixel, resolu√ß√£o de um v√≠deo de 480x240? Era **82,944 Mbps** sem aplicar compress√£o. √â a √∫nica maneira de fornecer HD / FullHD / 4K em TVs e na Internet. **Como?** Daremos uma breve olhada nas principais t√©cnicas aqui.

> **CODEC vs Container**
>
> Um erro comum que iniciantes frequentemente cometem √© confundir o codec de v√≠deo digital e o [formato de cont√™iner de v√≠deo digital](https://en.wikipedia.org/wiki/Digital_container_format). Podemos pensar em **cont√™ineres** como um formato de inv√≥lucro que cont√©m metadados do v√≠deo (e possivelmente tamb√©m de √°udio), e o **v√≠deo comprimido** pode ser visto como sua carga √∫til.
>
> Geralmente, a extens√£o de um arquivo de v√≠deo define seu cont√™iner de v√≠deo. Por exemplo, o arquivo `video.mp4` provavelmente √© um cont√™iner **[MPEG-4 Part 14](https://en.wikipedia.org/wiki/MPEG-4_Part_14)** e um arquivo chamado `video.mkv` provavelmente √© um **[matroska](https://en.wikipedia.org/wiki/Matroska)**. Para ter certeza absoluta sobre o codec e o formato do cont√™iner, podemos usar o [ffmpeg ou mediainfo](/encoding_pratical_examples.md#inspect-stream).

## Hist√≥ria

Antes de entrarmos nos detalhes de funcionamento interno de um codec gen√©rico, vamos dar uma olhada para entender um pouco melhor sobre alguns codecs de v√≠deo antigos.

O codec de v√≠deo [H.261](https://en.wikipedia.org/wiki/H.261) nasceu em 1990 (tecnicamente em 1988), e foi projetado para trabalhar com **taxas de dados de 64 kbit/s**. Ele j√° usava ideias como subsampling de croma, bloco macro, entre outros. No ano de 1995, o padr√£o de codec de v√≠deo **H.263** foi publicado e continuou a ser estendido at√© 2001.

Em 2003, a primeira vers√£o do **H.264/AVC** foi conclu√≠da. No mesmo ano, a **On2 Technologies** (anteriormente conhecida como Duck Corporation) lan√ßou seu codec de v√≠deo como uma compress√£o de v√≠deo **lossy** e **livre de royalties** chamada **VP3**. Em 2008, o **Google comprou** esta empresa, lan√ßando o **VP8** no mesmo ano. Em dezembro de 2012, o Google lan√ßou o **VP9** e √© **suportado por aproximadamente ¬æ do mercado de navegadores** (incluindo mobile).

**[AV1](https://en.wikipedia.org/wiki/AOMedia_Video_1)** √© um novo codec de v√≠deo **livre de royalties** e de c√≥digo aberto que est√° sendo projetado pela [Alliance for Open Media (AOMedia)](http://aomedia.org/), que √© composta por **empresas como Google, Mozilla, Microsoft, Amazon, Netflix, AMD, ARM, NVidia, Intel e Cisco**, entre outras. A **primeira vers√£o** 0.1.0 do codec de refer√™ncia foi **publicada em 7 de abril de 2016**.

![codec history timeline](/i/codec_history_timeline.png "codec history timeline")

> #### O nascimento de AV1
>
> No momento do in√≠cio de 2015, a Google estava trabalhando no [VP10](https://en.wikipedia.org/wiki/VP9#Successor:_from_VP10_to_AV1), a Xiph (Mozilla) estava trabalhando no [Daala](https://xiph.org/daala/) e a Cisco havia disponibilizado como software livre seu codec de v√≠deo livre de royalties chamado [Thor](https://tools.ietf.org/html/draft-fuldseth-netvc-thor-03).
>
> Ent√£o a MPEG LA anunciou pela primeira vez limites anuais para o HEVC (H.265) e taxas oito vezes mais altas que a do H.264, mas logo em seguida eles mudaram as regras novamente:
> * **sem limite anual**,
> * **taxa de conte√∫do** (0,5% da receita) e
> * **taxas por unidade cerca de 10 vezes mais altas que o h264**.
>
> O [alliance for open media](http://aomedia.org/about/) foi criada por empresas fabricantes de hardware (Intel, AMD, ARM, Nvidia, Cisco), fornecedoras de conte√∫do (Google, Netflix, Amazon), mantenedoras de navegadores (Google, Mozilla) e outras.
> As empresas tinham um objetivo comum, um codec de v√≠deo sem royalties e ent√£o nasceu o AV1 com uma [licen√ßa de patente muito mais simples](http://aomedia.org/license/patent/). **Timothy B. Terriberry** fez uma apresenta√ß√£o incr√≠vel, que √© a fonte desta se√ß√£o, sobre a [concep√ß√£o, modelo de licen√ßa e estado atual do AV1](https://www.youtube.com/watch?v=lzPaldsmJbk).
>
> Voc√™ ficar√° surpreso ao saber que pode **analisar o codec AV1 pelo seu navegador**, acesse https://arewecompressedyet.com/analyzer/
>
> ![av1 browser analyzer](/i/av1_browser_analyzer.png "av1 browser analyzer")
>
> PS: Se voc√™ quiser saber mais sobre a hist√≥ria dos codecs, √© preciso aprender o b√°sico por tr√°s das [patentes de compress√£o de v√≠deo](https://www.vcodex.com/video-compression-patents/).

## Um codec gen√©rico

Vamos apresentar os **principais mecanismos por tr√°s de um codec de v√≠deo gen√©rico**, mas a maioria desses conceitos s√£o √∫teis e usados em codecs modernos, como VP9, AV1 e HEVC. Certifique-se de entender que vamos simplificar MUITO as coisas. √Äs vezes, usaremos um exemplo real (principalmente H.264) para demonstrar uma t√©cnica.

## Primeiro passo - particionando uma figura

O primeiro passo √© **dividir o quadro** em v√°rias **parti√ß√µes, subparti√ß√µes** e al√©m.

![picture partitioning](/i/picture_partitioning.png "picture partitioning")

**Mas por qu√™?** Existem muitas raz√µes, por exemplo, quando dividimos a imagem, podemos trabalhar com previs√µes mais precisas, usando parti√ß√µes menores para as partes m√≥veis menores, enquanto usamos parti√ß√µes maiores para um fundo est√°tico.

Geralmente, os CODECs **organizam essas parti√ß√µes** em fatias (ou azulejos), macroblocos (ou unidades de √°rvore de codifica√ß√£o) e muitas subparti√ß√µes. O tamanho m√°ximo dessas parti√ß√µes varia, o HEVC define 64x64 enquanto o AVC usa 16x16, mas as subparti√ß√µes podem atingir tamanhos de 4x4.

Lembra que aprendemos como s√£o **tipos de quadros**?! Bem, voc√™ pode **aplicar essas ideias a blocos** tamb√©m, portanto podemos ter I-Slice, B-Slice, I-Macroblock e etc.

> ### Pr√°tica: Verifique as parti√ß√µes
> Tamb√©m podemos usar o [Intel Video Pro Analyzer](https://software.intel.com/en-us/intel-video-pro-analyzer) (que √© pago, mas h√° uma vers√£o de teste gratuita que limita voc√™ a trabalhar apenas com os primeiros 10 quadros). Aqui est√£o as parti√ß√µes [VP9 analisadas](/encoding_pratical_examples.md#transcoding).
>
> ![VP9 partitions view intel video pro analyzer ](/i/paritions_view_intel_video_pro_analyzer.png "VP9 partitions view intel video pro analyzer")

## Segundo passo - previs√µes

Uma vez que temos as parti√ß√µes, podemos fazer previs√µes sobre elas. Para a [predi√ß√£o inter](#redund√¢ncia-temporal-inter-prediction), precisamos **enviar os vetores de movimento e o residual** e para a [predi√ß√£o intra](#redund√¢ncia-espacial-intra-prediction), n√≥s **enviaremos a dire√ß√£o da predi√ß√£o e o residual** tamb√©m.

## Terceiro passo - transforma√ß√£o

Depois de obtermos o bloco residual (`parti√ß√£o prevista - parti√ß√£o real`), podemos **transform√°-lo** de uma maneira que nos permite saber quais **pixels podemos descartar** enquanto mantemos a **qualidade geral**. Existem algumas transforma√ß√µes para esse comportamento exato.

Embora existam [outras transforma√ß√µes](https://en.wikipedia.org/wiki/List_of_Fourier-related_transforms#Discrete_transforms), vamos olhar mais de perto a transformada discreta de cosseno (DCT). As principais caracter√≠sticas da [**DCT**](https://en.wikipedia.org/wiki/Discrete_cosine_transform) s√£o:

* **converte** blocos de **pixels** em blocos do mesmo tamanho de **coeficientes de frequ√™ncia**.
* **compacta** energia, tornando f√°cil eliminar a redund√¢ncia espacial.
* √© **revers√≠vel**, ou seja, voc√™ pode voltar para os pixels.

> Em 2 de fevereiro de 2017, Cintra, R. J. e Bayer, F. M publicaram seu artigo [DCT-like Transform for Image Compression Requires 14 Additions Only](https://arxiv.org/abs/1702.00817).

N√£o se preocupe se voc√™ n√£o entendeu os benef√≠cios de cada ponto, tentaremos fazer alguns experimentos para ver o valor real disso.

Vamos pegar o seguinte **bloco de pixels** (8x8):

![pixel values matrix](/i/pixel_matrice.png "pixel values matrix")

Que renderiza a seguinte imagem de bloco (8x8):

![pixel values matrix](/i/gray_image.png "pixel values matrix")

Quando **aplicamos a DCT** a este bloco de pixels, obtemos o **bloco de coeficientes** (8x8):

![coefficients values](/i/dct_coefficient_values.png "coefficients values")

E se renderizarmos este bloco de coeficientes, obteremos esta imagem:

![dct coefficients image](/i/dct_coefficient_image.png "dct coefficients image")

Como voc√™ pode ver, n√£o se parece em nada com a imagem original, podemos perceber que o **primeiro coeficiente** √© muito diferente de todos os outros. Este primeiro coeficiente √© conhecido como coeficiente DC que representa **todas as amostras** na matriz de entrada, algo **semelhante a uma m√©dia**.

Este bloco de coeficientes tem uma propriedade interessante que √© separar os componentes de alta frequ√™ncia dos de baixa frequ√™ncia.

![dct frequency coefficients property](/i/dctfrequ.jpg "dct frequency coefficients property")

Em uma imagem, a **maior parte da energia** estar√° concentrada nas [**frequ√™ncias mais baixas**](https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm), ent√£o se transformarmos uma imagem em seus componentes de frequ√™ncia e **descartarmos os coeficientes de frequ√™ncia mais alta**, podemos **reduzir a quantidade de dados** necess√°rios para descrever a imagem sem sacrificar muito a qualidade da imagem.

> frequ√™ncia significa qu√£o r√°pido um sinal est√° mudando

Vamos tentar aplicar o conhecimento que adquirimos no teste convertendo a imagem original para sua frequ√™ncia (bloco de coeficientes) usando a DCT e depois descartando parte dos coeficientes menos importantes.

Primeiro, convertemos para o seu **dom√≠nio de frequ√™ncia**.

![coefficients values](/i/dct_coefficient_values.png "coefficients values")

Em seguida, descartamos parte (67%) dos coeficientes, principalmente a parte inferior direita.

![zeroed coefficients](/i/dct_coefficient_zeroed.png "zeroed coefficients")

Finalmente, reconstru√≠mos a imagem a partir deste bloco de coeficientes descartados (lembre-se, precisa ser revers√≠vel) e comparamos com o original.

![original vs quantized](/i/original_vs_quantized.png "original vs quantized")

Como podemos ver, a imagem resultante se assemelha √† imagem original, mas apresenta muitas diferen√ßas em rela√ß√£o √† mesma. Jogamos **fora 67,1875%** e ainda conseguimos algo semelhante ao original. Poder√≠amos descartar os coeficientes de forma mais inteligente para obter uma melhor qualidade de imagem, mas esse √© o pr√≥ximo t√≥pico.

> **Cada coeficiente √© formado usando todos os pixels**
>
> √â importante observar que cada coeficiente n√£o mapeia diretamente para um √∫nico pixel, mas √© uma soma ponderada de todos os pixels. Este gr√°fico incr√≠vel mostra como o primeiro e o segundo coeficiente s√£o calculados, usando pesos √∫nicos para cada √≠ndice.
>
> ![dct calculation](/i/applicat.jpg "dct calculation")
>
> Fonte: https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm
>
> Voc√™ tamb√©m pode tentar [visualizar a DCT observando a forma√ß√£o de uma imagem simples](/dct_better_explained.ipynb) na base da DCT. Por exemplo, aqui est√° o [caractere A sendo formado](https://en.wikipedia.org/wiki/Discrete_cosine_transform#Example_of_IDCT) usando o peso de cada coeficiente.
>
> ![](https://upload.wikimedia.org/wikipedia/commons/5/5e/Idct-animation.gif )




<br/>

> ### Pr√°tica: jogando com diferentes coeficientes
> Voc√™ pode experimentar com a [transforma√ß√£o DCT](/uniform_quantization_experience.ipynb).

## Quarto passo - quantiza√ß√£o

Quando descartamos alguns dos coeficientes, na √∫ltima etapa (transforma√ß√£o), de alguma forma fizemos uma forma de quantiza√ß√£o. Nessa etapa, escolhemos perder informa√ß√µes (a parte **perdida**) ou, em termos simples, **quantizamos os coeficientes para alcan√ßar a compress√£o**.

Como podemos quantizar um bloco de coeficientes? Um m√©todo simples seria uma quantiza√ß√£o uniforme, em que pegamos um bloco, **dividimos por um √∫nico valor** (10) e arredondamos esse valor.

![quantize](/i/quantize.png "quantize")

Como podemos **reverter** (re-quantizar) esse bloco de coeficientes? Podemos fazer isso **multiplicando o mesmo valor** (10) que dividimos primeiro.

![re-quantize](/i/re-quantize.png "re-quantize")

Essa **abordagem n√£o √© a melhor** porque n√£o leva em conta a import√¢ncia de cada coeficiente, poder√≠amos usar uma **matriz de quantizadores** em vez de um √∫nico valor. Essa matriz pode explorar a propriedade da DCT, quantizando a maior parte do canto inferior direito e menos o canto superior esquerdo, a [JPEG usa uma abordagem semelhante](https://www.hdm-stuttgart.de/~maucher/Python/MMCodecs/html/jpegUpToQuant.html), voc√™ pode verificar o [c√≥digo-fonte para ver essa matriz](https://github.com/google/guetzli/blob/master/guetzli/jpeg_data.h#L40).

> ### Pr√°tica: quantiza√ß√£o
> Voc√™ pode experimentar [quantiza√ß√£o](/dct_experiences.ipynb).

## Quinto passo - Codificando entropia

Depois de quantizarmos os dados (blocos/fatias/quadros de imagem), ainda podemos comprimi-los de forma lossless. Existem muitas maneiras (algoritmos) de comprimir dados. Vamos experimentar brevemente alguns deles, para uma compreens√£o mais profunda, voc√™ pode ler o incr√≠vel livro [Understanding Compression: Data Compression for Modern Developers](https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/).

### C√≥digo VLC:

Vamos supor que temos uma sequ√™ncia dos s√≠mbolos: **a**, **e**, **r** e **t** e sua probabilidade (de 0 a 1) √© representada por esta tabela.


|             | a   | e   | r    | t   |
|-------------|-----|-----|------|-----|
| probabilidade | 0.3 | 0.3 | 0.2 |  0.2 |

Podemos atribuir c√≥digos bin√°rios exclusivos (preferencialmente pequenos) para os s√≠mbolos mais prov√°veis e c√≥digos maiores para os menos prov√°veis.

|             | a   | e   | r    | t   |
|-------------|-----|-----|------|-----|
| probabilidade | 0.3 | 0.3 | 0.2 | 0.2 |
| c√≥digo bin√°rio | 0 | 10 | 110 | 1110 |

Vamos comprimir a sequ√™ncia **eat**, assumindo que gastar√≠amos 8 bits para cada s√≠mbolo, gastar√≠amos **24 bits*8 sem qualquer compress√£o. Mas, se substituirmos cada s√≠mbolo por seu c√≥digo, podemos economizar espa√ßo.

O primeiro passo √© codificar o s√≠mbolo **e** que √© `10` e o segundo s√≠mbolo √© **a** que √© adicionado (n√£o de uma maneira matem√°tica) `[10][0]` e, finalmente, o terceiro s√≠mbolo **t**, que faz com que nosso bitstream comprimido final seja `[10][0][1110]` ou `1001110`, que requer apenas **7 bits** (3.4 vezes menos espa√ßo que o original).

Observe que cada c√≥digo deve ser um c√≥digo prefixado √∫nico [Huffman pode ajud√°-lo a encontrar esses n√∫meros](https://en.wikipedia.org/wiki/Huffman_coding). Embora tenha alguns problemas, existem [codecs de v√≠deo que ainda oferecem](https://en.wikipedia.org/wiki/Context-adaptive_variable-length_coding) esse m√©todo e √© o algoritmo para muitas aplica√ß√µes que requerem compress√£o.

Tanto o codificador quanto o decodificador **precisam saber** a tabela de s√≠mbolos com seus c√≥digos, portanto, voc√™ tamb√©m precisa enviar a tabela.

### C√≥digo Aritm√©tico:

Vamos supor que temos um fluxo de s√≠mbolos: **a**, **e**, **r**, **s** e **t** e suas probabilidades s√£o representadas por esta tabela.

|             | a   | e   | r    | s    | t   |
|-------------|-----|-----|------|------|-----|
| probabilidade | 0.3 | 0.3 | 0.15 | 0.05 | 0.2 |

Com essa tabela em mente, podemos construir intervalos contendo todos os s√≠mbolos poss√≠veis classificados pelos mais frequentes.

![initial arithmetic range](/i/range.png "initial arithmetic range")

Agora vamos codificar a sequ√™ncia **eat**, escolhemos o primeiro s√≠mbolo **e**, que est√° dentro da subfaixa **0,3 a 0,6** (mas n√£o incluso) e pegamos essa subfaixa e a dividimos novamente usando as mesmas propor√ß√µes usadas antes, mas dentro dessa nova faixa.

![second sub range](/i/second_subrange.png "second sub range")

Continuando a codificar nossa sequ√™ncia **eat**, agora pegamos o segundo s√≠mbolo **a**, que est√° dentro da nova subfaixa **0,3 a 0,39**, e depois pegamos nosso √∫ltimo s√≠mbolo **t** e fazemos o mesmo processo novamente e obtemos a √∫ltima subfaixa **0,354 a 0,372**.

![final arithmetic range](/i/arithimetic_range.png "final arithmetic range")

Agora precisamos escolher um n√∫mero dentro da √∫ltima subfaixa **0,354 a 0,372**, vamos escolher **0,36**, mas poder√≠amos escolher qualquer n√∫mero dentro dessa subfaixa. Com **apenas** esse n√∫mero, seremos capazes de recuperar nossa sequ√™ncia original **eat**. Se voc√™ pensar sobre isso, √© como se estiv√©ssemos desenhando uma linha dentro de faixas de faixas para codificar nossa sequ√™ncia.

![final range traverse](/i/range_show.png "final range traverse")

O **processo reverso** (tamb√©m conhecido como decodifica√ß√£o) √© igualmente f√°cil, com nosso n√∫mero **0,36** e nossa faixa original, podemos executar o mesmo processo, mas agora usando esse n√∫mero para revelar o fluxo codificado por tr√°s desse n√∫mero.

Com a primeira faixa, percebemos que nosso n√∫mero se encaixa na fatia, portanto, √© o nosso primeiro s√≠mbolo, agora dividimos essa subfaixa novamente, fazendo o mesmo processo anterior e perceberemos que **0,36** se encaixa no s√≠mbolo **a** e depois de repetirmos o processo, chegamos ao √∫ltimo s√≠mbolo **t** (formando nosso fluxo codificado original *eat*).

Tanto o codificador quanto o decodificador **devem conhecer** a tabela de probabilidade de s√≠mbolos, portanto, voc√™ precisa enviar a tabela.

Bastante legal, n√£o √©? As pessoas s√£o muito inteligentes para criar uma solu√ß√£o dessas, alguns [codecs de v√≠deo usam](https://en.wikipedia.org/wiki/Context-adaptive_binary_arithmetic_coding) essa t√©cnica (ou pelo menos a oferecem como uma op√ß√£o).

A ideia √© comprimir sem perda o fluxo de bits quantizado, com certeza este artigo est√° perdendo toneladas de detalhes, raz√µes, compensa√ß√µes e etc. Mas [voc√™ deve aprender mais](https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/) como desenvolvedor. Novos codecs est√£o tentando usar diferentes [algoritmos de codifica√ß√£o de entropia como ANS.](https://en.wikipedia.org/wiki/Asymmetric_Numeral_Systems)

> ### Pr√°tica: CABAC vs CAVLC
> Voc√™ pode [gerar dois fluxos, um com CABAC e outro com CAVLC](https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#cabac-vs-cavlc) e **comparar o tempo** que cada um leva para ser gerado, bem como **o tamanho final**.

## Sexto passo - formato bitstream

Depois de termos conclu√≠do todas essas etapas, precisamos **empacotar os quadros comprimidos e o contexto dessas etapas**. Precisamos informar explicitamente ao decodificador sobre **as decis√µes tomadas pelo codificador**, como profundidade de bits, espa√ßo de cores, resolu√ß√£o, informa√ß√µes de previs√µes (vetores de movimento, dire√ß√£o de previs√£o intra), perfil, n√≠vel, taxa de quadros, tipo de quadro, n√∫mero de quadros e muito mais.

Vamos estudar superficialmente o bitstream H.264. Nosso primeiro passo √© [gerar um bitstream H.264 m√≠nimo <sup>*</sup>](/encoding_pratical_examples.md#generate-a-single-frame-h264-bitstream), podemos fazer isso usando nosso pr√≥prio reposit√≥rio e [ffmpeg](http://ffmpeg.org/).

```
./s/ffmpeg -i /files/i/minimal.png -pix_fmt yuv420p /files/v/minimal_yuv420.h264
```

> <sup>*</sup> Por padr√£o, o ffmpeg adiciona todos os par√¢metros de codifica√ß√£o como um **NAL SEI**, em breve definiremos o que √© um NAL.

Este comando ir√° gerar um bitstream H.264 bruto com um **√∫nico quadro**, 64x64, com espa√ßo de cor yuv420 e usando a seguinte imagem como quadro.

> ![used frame to generate minimal h264 bitstream](/i/minimal.png "used frame to generate minimal h264 bitstream")

### H.264 bitstream

O padr√£o AVC (H.264) define que as informa√ß√µes ser√£o enviadas em **macro quadros** (no sentido de rede), chamados de **[NAL](https://en.wikipedia.org/wiki/Network_Abstraction_Layer)** (Camada de Abstra√ß√£o de Rede). O principal objetivo do NAL √© fornecer uma representa√ß√£o de v√≠deo "amig√°vel √† rede", este padr√£o deve funcionar em TVs (baseadas em fluxo), na Internet (baseadas em pacotes) e em outros.

![NAL units H.264](/i/nal_units.png "NAL units H.264")

Existe um **[marcador de sincroniza√ß√£o](https://en.wikipedia.org/wiki/Frame_synchronization)** para definir os limites das unidades NAL. Cada marcador de sincroniza√ß√£o cont√©m o valor `0x00 0x00 0x01`, exceto o primeiro, que √© `0x00 0x00 0x00 0x01`. Se executarmos o **hexdump** no bitstream h264 gerado, podemos identificar pelo menos tr√™s NALs no in√≠cio do arquivo.

![synchronization marker on NAL units](/i/minimal_yuv420_hex.png "synchronization marker on NAL units")

Como j√° dissemos antes, o decodificador precisa saber n√£o apenas os dados da imagem, mas tamb√©m os detalhes do v√≠deo, quadro, cores, par√¢metros usados e outros. O **primeiro byte** de cada NAL define sua categoria e **tipo**.

| id tipo NAL  | Descri√ß√£o  |
|---  |---|
| 0  |  Indefinido |
| 1  |  Fatia codificada de uma imagem n√£o-IDR |
| 2  |  Dados de fatia codificada, parti√ß√£o A |
| 3  |  Dados de fatia codificada, parti√ß√£o B |
| 4  |  Dados de fatia codificada, parti√ß√£o C |
| 5  |  **IDR** Fatia codificada de uma imagem IDR |
| 6  |  **SEI** Informa√ß√£o adicional de melhoria |
| 7  |  **SPS** Conjunto de par√¢metros de sequ√™ncia |
| 8  |  **PPS** Conjunto de par√¢metros de imagem |
| 9  |  Delimitador de unidade de acesso |
| 10 |  Fim da sequ√™ncia |
| 11 |  Fim do fluxo |
| ... |  ... |

Geralmente, o primeiro NAL de um fluxo de bits √© um **SPS**, esse tipo de NAL √© respons√°vel por informar as vari√°veis gerais de codifica√ß√£o como **perfil**, **n√≠vel**, **resolu√ß√£o** e outras.

Se pulamos o primeiro marcador de sincroniza√ß√£o, podemos decodificar o **primeiro byte** para saber qual **tipo de NAL** √© o primeiro.

Por exemplo, o primeiro byte ap√≥s o marcador de sincroniza√ß√£o √© `01100111`, onde o primeiro bit (`0`) √© para o campo **forbidden_zero_bit**, os pr√≥ximos 2 bits (`11`) nos dizem o campo `nal_ref_idc` que indica se este NAL √© um campo de refer√™ncia ou n√£o e os pr√≥ximos 5 bits (`00111`) nos informam o campo **nal_unit_type**, neste caso, √© um NAL de **SPS** (7).

O segundo byte (`bin√°rio=01100100, hex=0x64, dec=100`) de um SPS NAL √© o campo **profile_idc** que mostra o perfil que o codificador usou, neste caso, usamos o **[perfil alto](https://en.wikipedia.org/wiki/H.264/MPEG-4_AVC#Profiles)**. O terceiro byte tamb√©m cont√©m v√°rias flags que determinam o perfil exato (como restrito ou progressivo). Mas, no nosso caso, o terceiro byte √© 0x00 e, portanto, o codificador usou apenas o perfil alto.

![SPS binary view](/i/minimal_yuv420_bin.png "SPS binary view")

Quando lemos a especifica√ß√£o do bitstream H.264 para um SPS NAL, encontramos muitos valores para o **nome do par√¢metro**, **categoria** e uma **descri√ß√£o**, por exemplo, vamos olhar para os campos `pic_width_in_mbs_minus_1` e `pic_height_in_map_units_minus_1`.

| Nome do par√¢metro  | Categoria  |  Descri√ß√£o w  |
|---  |---|---|
| pic_width_in_mbs_minus_1 |  0 | ue(v) |
| pic_height_in_map_units_minus_1 |  0 | ue(v) |

> **ue(v)**: Inteiro sem sinal (unsigned integer) [Exp-Golomb-coded](https://ghostarchive.org/archive/JBwdI)

Se fizermos algumas contas com o valor desses campos, acabaremos com a **resolu√ß√£o*8. Podemos representar um `1920 x 1080` usando um `pic_width_in_mbs_minus_1` com o valor de `119 ( (119 + 1) * macroblock_size = 120 * 16 = 1920)`, economizando espa√ßo, em vez de codificar `1920`, fizemos isso com `119`.

Se continuarmos a examinar nosso v√≠deo criado com uma visualiza√ß√£o bin√°ria (ex: `xxd -b -c 11 v/minimal_yuv420.h264`), podemos pular para o √∫ltimo NAL que √© o quadro em si.

![h264 idr slice header](/i/slice_nal_idr_bin.png "h264 idr slice header")

Podemos ver seus primeiros 6 bytes: `01100101 10001000 10000100 00000000 00100001 11111111`. Como j√° sabemos, o primeiro byte nos informa sobre o tipo de NAL que √©, neste caso, (`00101`) √© um **IDR Slice (5)** e podemos inspecion√°-lo ainda mais:


![h264 slice header spec](/i/slice_header.png "h264 slice header spec")

Usando as informa√ß√µes da especifica√ß√£o, podemos decodificar qual tipo de slice (**slice_type**), o n√∫mero do quadro (**frame_num**) entre outros campos importantes.

Para obter os valores de alguns campos (`ue(v), me(v), se(v) ou te(v)`), precisamos decodific√°-los usando um decodificador especial chamado [Exponential-Golomb](https://ghostarchive.org/archive/JBwdI), este m√©todo √© **muito eficiente para codificar valores vari√°veis**, principalmente quando h√° muitos valores padr√£o.

> Os valores de **slice_type** e **frame_num** deste v√≠deo s√£o 7 (I slice) e 0 (o primeiro quadro).

Podemos ver o **bitstream como um protocolo**, e se voc√™ quiser ou precisar aprender mais sobre esse bitstream, consulte a [especifica√ß√£o ITU H.264](http://www.itu.int/rec/T-REC-H.264-201610-I). Aqui est√° um diagrama macro que mostra onde os dados da imagem (YUV comprimido) residem.

![h264 bitstream macro diagram](/i/h264_bitstream_macro_diagram.png "h264 bitstream macro diagram")

Podemos explorar outros bitstreams como o [VP9 bitstream](https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf), [H.265 (HEVC)](http://handle.itu.int/11.1002/1000/11885-en?locatt=format:pdf) ou at√© mesmo nosso **novo melhor amigo** [**AV1** bitstream](https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8
), [eles parecem semelhantes? N√£o](http://www.gpac-licensing.com/2016/07/12/vp9-av1-bitstream-format/), mas uma vez que voc√™ aprende um, pode facilmente aprender os outros.


> ### Pr√°tica: Inspecionar o H.264 bitstream
> Podemos [gerar um v√≠deo de um √∫nico quadro](https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#generate-a-single-frame-video) e usar o [mediainfo](https://en.wikipedia.org/wiki/MediaInfo) para inspecionar seu bitstream H.264. Na verdade, voc√™ pode at√© ver o [c√≥digo fonte que analisa o bitstream h264 (AVC)](https://github.com/MediaArea/MediaInfoLib/blob/master/Source/MediaInfo/Video/File_Avc.cpp).
>
> ![mediainfo details h264 bitstream](/i/mediainfo_details_1.png "mediainfo details h264 bitstream")
>
> Tamb√©m podemos usar o [Intel Video Pro Analyzer](https://software.intel.com/en-us/intel-video-pro-analyzer), que √© pago, mas h√° uma vers√£o de teste gratuita que limita o uso apenas aos primeiros 10 quadros, mas isso √© ok para fins de aprendizado.
>
> ![intel video pro analyzer details h264 bitstream](/i/intel-video-pro-analyzer.png "intel video pro analyzer details h264 bitstream")

## Revis√£o

Vamos notar que muitos dos **codecs modernos usam o mesmo modelo que aprendemos**. Na verdade, vamos olhar o diagrama de blocos do codec de v√≠deo Thor, ele cont√©m todos os passos que estudamos. A ideia √© que agora voc√™ deve ser capaz de entender melhor as inova√ß√µes e os artigos para a √°rea.

![thor_codec_block_diagram](/i/thor_codec_block_diagram.png "thor_codec_block_diagram")

Anteriormente, calculamos que precis√°vamos de [139 GB de armazenamento para manter um arquivo de v√≠deo com uma hora de dura√ß√£o em resolu√ß√£o 720p e 30fps](#chroma-subsampling) se usarmos as t√©cnicas que aprendemos aqui, como **previs√£o inter e intra, transforma√ß√£o, quantiza√ß√£o, codifica√ß√£o de entropia e outras** podemos alcan√ßar, assumindo que estamos gastando **0,031 bit por pixel**, o mesmo v√≠deo de qualidade percept√≠vel **requer apenas 367,82 MB em vez de 139 GB** de armazenamento.

> Escolhemos usar **0,031 bit por pixel** com base no exemplo de v√≠deo fornecido aqui.

## Como H.265 consegue uma melhor raz√£o de compress√£o comparado com H.264?

Agora que sabemos mais sobre como os codecs funcionam, fica f√°cil entender como os novos codecs s√£o capazes de entregar resolu√ß√µes mais altas com menos bits.

Vamos comparar AVC e HEVC, lembrando que quase sempre √© um equil√≠brio entre mais ciclos de CPU (complexidade) e taxa de compress√£o.

HEVC tem op√ß√µes de **parti√ß√µes** (e **sub-parti√ß√µes**) maiores e mais numerosas do que AVC, mais **dire√ß√µes/√¢ngulos de predi√ß√µes intra**, codifica√ß√£o de entropia melhorada e muito mais, todas essas melhorias tornaram o H.265 capaz de comprimir 50% mais do que o H.264.

![h264 vs h265](/i/avc_vs_hevc.png "H.264 vs H.265")

# Transmiss√£o online
## Arquitetura geral

![general architecture](/i/general_architecture.png "general architecture")

[TODO]

## Download progressivo e transmiss√£o adaptativa

![progressive download](/i/progressive_download.png "progressive download")

![adaptive streaming](/i/adaptive_streaming.png "adaptive streaming")

[TODO]

## Prote√ß√£o de conte√∫do

Podemos utilizar **um sistema simples de token** para proteger o conte√∫do. O usu√°rio sem um token tenta solicitar um v√≠deo e o CDN o impede, enquanto um usu√°rio com um token v√°lido pode reproduzir o conte√∫do. Isso funciona de maneira bastante semelhante √† maioria dos sistemas de autentica√ß√£o da web.

![token protection](/i/token_protection.png "token_protection")

O uso exclusivo deste sistema de tokens ainda permite que um usu√°rio baixe um v√≠deo e o distribua. Ent√£o, os sistemas de **gerenciamento de direitos digitais (DRM)** podem ser usados para tentar evitar isso.

![drm](/i/drm.png "drm")

Na vida real, em sistemas de produ√ß√£o, as pessoas frequentemente usam ambas as t√©cnicas para fornecer autoriza√ß√£o e autentica√ß√£o.

### DRM
#### Principais sistemas

* FPS - [**FairPlay Streaming**](https://developer.apple.com/streaming/fps/)
* PR - [**PlayReady**](https://www.microsoft.com/playready/)
* WV - [**Widevine**](http://www.widevine.com/)


#### O que?

DRM significa [gerenciamento de direitos digitais](https://sander.saares.eu/categories/drm-is-not-a-black-box/), √© uma **forma de fornecer prote√ß√£o de direitos autorais para m√≠dia digital**, como v√≠deo e √°udio digital. Embora seja usado em muitos lugares, [n√£o √© universalmente aceito](https://en.wikipedia.org/wiki/Digital_rights_management#DRM-free_works).

#### Por qu√™?

Os criadores de conte√∫do (principalmente est√∫dios) querem proteger sua propriedade intelectual contra c√≥pias para evitar redistribui√ß√£o n√£o autorizada de m√≠dia digital.

#### Como?

Vamos descrever de forma abstrata e gen√©rica uma forma simplificada de DRM.

Dado um **conte√∫do C1** (ou seja, um streaming de v√≠deo hls ou dash), com um **reprodutor P1** (como shaka-clappr, exo-player ou ios) em um **dispositivo D1** (como um smartphone, TV, tablet ou desktop/notebook) usando um **sistema DRM1** (widevine, playready ou FairPlay).

O conte√∫do C1 √© criptografado com uma **chave sim√©trica K1** do sistema DRM1, gerando o **conte√∫do criptografado C'1**.

![drm general flow](/i/drm_general_flow.jpeg "drm general flow")

O reprodutor P1, de um dispositivo D1, tem duas chaves (assim√©tricas), uma **chave privada PRK1** (essa chave √© protegida<sup>1</sup> e conhecida apenas por D1) e uma **chave p√∫blica PUK1**.


> **<sup>1</sup>protegida**: essa prote√ß√£o pode ser **via hardware**, por exemplo, essa chave pode ser armazenada dentro de um chip especial (somente leitura) que funciona como uma [caixa preta](https://en.wikipedia.org/wiki/Black_box) para fornecer a decodifica√ß√£o, ou **por software** (menos seguro), o sistema DRM fornece meios para saber qual tipo de prote√ß√£o um determinado dispositivo possui.


Quando o **reprodutor P1 deseja reproduzir** o **conte√∫do C'1**, ele precisa lidar com o **sistema DRM1**, fornecendo sua chave p√∫blica **PUK1**. O sistema DRM1 retorna a **chave K1 criptografada** com a chave p√∫blica do cliente **PUK1**. Em teoria, essa resposta √© algo que **apenas D1 √© capaz de decifrar**.

`K1P1D1 = enc(K1, PUK1)`

**P1** usa seu sistema local de DRM pode ser um [SOC](https://en.wikipedia.org/wiki/System_on_a_chip), um hardware ou software especializado, este sistema √© capaz de decifrar o conte√∫do usando sua chave privada PRK1, ele pode decifrar **a chave sim√©trica K1 do K1P1D1** e **reproduzir C'1**. No melhor dos casos, as chaves n√£o s√£o expostas atrav√©s da RAM.

 ```
 K1 = dec(K1P1D1, PRK1)

 P1.play(dec(C'1, K1))
 ```

![drm decoder flow](/i/drm_decoder_flow.jpeg "drm decoder flow")

# Como user o Jupyter

Certifique-se de ter o **docker instalado** e execute `./s/start_jupyter.sh` e siga as instru√ß√µes no terminal.

# Confer√™ncias

* [DEMUXED](https://demuxed.com/) - voc√™ pode [ver as apresenta√ß√µes dos √∫ltimos 2 eventos.](https://www.youtube.com/channel/UCIc_DkRxo9UgUSTvWVNCmpA).

# Refer√™ncias

O conte√∫do mais rico est√° aqui, √© onde todas as informa√ß√µes que vimos neste texto foram extra√≠das, baseadas ou inspiradas. Voc√™ pode aprofundar seu conhecimento com esses incr√≠veis links, livros, v√≠deos, etc.

Cursos Onlines e Tutoriais:

* https://www.coursera.org/learn/digital/
* https://people.xiph.org/~tterribe/pubs/lca2012/auckland/intro_to_video1.pdf
* https://xiph.org/video/vid1.shtml
* https://xiph.org/video/vid2.shtml
* https://wiki.multimedia.cx
* https://mahanstreamer.net
* http://slhck.info/ffmpeg-encoding-course
* http://www.cambridgeincolour.com/tutorials/camera-sensors.htm
* http://www.slideshare.net/vcodex/a-short-history-of-video-coding
* http://www.slideshare.net/vcodex/introduction-to-video-compression-13394338
* https://developer.android.com/guide/topics/media/media-formats.html
* http://www.slideshare.net/MadhawaKasun/audio-compression-23398426
* http://inst.eecs.berkeley.edu/~ee290t/sp04/lectures/02-Motion_Compensation_girod.pdf

Livros:

* https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/ref=sr_1_1?s=books&ie=UTF8&qid=1486395327&sr=1-1
* https://www.amazon.com/H-264-Advanced-Video-Compression-Standard/dp/0470516925
* https://www.amazon.com/High-Efficiency-Video-Coding-HEVC/dp/3319068946
* https://www.amazon.com/Practical-Guide-Video-Audio-Compression/dp/0240806301/ref=sr_1_3?s=books&ie=UTF8&qid=1486396914&sr=1-3&keywords=A+PRACTICAL+GUIDE+TO+VIDEO+AUDIO
* https://www.amazon.com/Video-Encoding-Numbers-Eliminate-Guesswork/dp/0998453005/ref=sr_1_1?s=books&ie=UTF8&qid=1486396940&sr=1-1&keywords=jan+ozer

Em material de integra√ß√£o:

* https://github.com/Eyevinn/streaming-onboarding
* https://howvideo.works/
* https://www.aws.training/Details/eLearning?id=17775
* https://www.aws.training/Details/eLearning?id=17887
* https://www.aws.training/Details/Video?id=24750

Especifica√ß√µes de fluxo de bits:

* http://www.itu.int/rec/T-REC-H.264-201610-I
* http://www.itu.int/ITU-T/recommendations/rec.aspx?rec=12904&lang=en
* https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf
* http://iphome.hhi.de/wiegand/assets/pdfs/2012_12_IEEE-HEVC-Overview.pdf
* http://phenix.int-evry.fr/jct/doc_end_user/current_document.php?id=7243
* http://gentlelogic.blogspot.com.br/2011/11/exploring-h264-part-2-h264-bitstream.html
* https://forum.doom9.org/showthread.php?t=167081
* https://forum.doom9.org/showthread.php?t=168947

Programas:

* https://ffmpeg.org/
* https://ffmpeg.org/ffmpeg-all.html
* https://ffmpeg.org/ffprobe.html
* https://mediaarea.net/en/MediaInfo
* https://www.jongbel.com/
* https://trac.ffmpeg.org/wiki/
* https://software.intel.com/en-us/intel-video-pro-analyzer
* https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8

Codecs n√£o-ITU:

* https://aomedia.googlesource.com/
* https://github.com/webmproject/libvpx/tree/master/vp9
* https://ghostarchive.org/archive/0W0d8 (era: https://people.xiph.org/~xiphmont/demo/daala/demo1.shtml)* https://people.xiph.org/~jm/daala/revisiting/
* https://www.youtube.com/watch?v=lzPaldsmJbk
* https://fosdem.org/2017/schedule/event/om_av1/
* https://jmvalin.ca/papers/AV1_tools.pdf

Conceitos de Codifica√ß√£o:

* http://x265.org/hevc-h265/
* http://slhck.info/video/2017/03/01/rate-control.html
* http://slhck.info/video/2017/02/24/vbr-settings.html
* http://slhck.info/video/2017/02/24/crf-guide.html
* https://arxiv.org/pdf/1702.00817v1.pdf
* https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors
* http://web.ece.ucdavis.edu/cerl/ReliableJPEG/Cung/jpeg.html
* http://www.adobe.com/devnet/adobe-media-server/articles/h264_encoding.html
* https://prezi.com/8m7thtvl4ywr/mp3-and-aac-explained/
* https://blogs.gnome.org/rbultje/2016/12/13/overview-of-the-vp9-video-codec/
* https://videoblerg.wordpress.com/2017/11/10/ffmpeg-and-how-to-use-it-wrong/

Sequ√™ncias de v√≠deo para testes:

* http://bbb3d.renderfarming.net/download.html
* https://www.its.bldrdoc.gov/vqeg/video-datasets-and-organizations.aspx

Diversos:

* https://github.com/Eyevinn/streaming-onboarding
* http://stackoverflow.com/a/24890903
* http://stackoverflow.com/questions/38094302/how-to-understand-header-of-h264
* http://techblog.netflix.com/2016/08/a-large-scale-comparison-of-x264-x265.html
* http://vanseodesign.com/web-design/color-luminance/
* http://www.biologymad.com/nervoussystem/eyenotes.htm
* http://www.compression.ru/video/codec_comparison/h264_2012/mpeg4_avc_h264_video_codecs_comparison.pdf
* https://web.archive.org/web/20100728070421/http://www.csc.villanova.edu/~rschumey/csc4800/dct.html (era: http://www.csc.villanova.edu/~rschumey/csc4800/dct.html)
* http://www.explainthatstuff.com/digitalcameras.html
* http://www.hkvstar.com
* http://www.hometheatersound.com/
* http://www.lighterra.com/papers/videoencodingh264/
* http://www.red.com/learn/red-101/video-chroma-subsampling
* http://www.slideshare.net/ManoharKuse/hevc-intra-coding
* http://www.slideshare.net/mwalendo/h264vs-hevc
* http://www.slideshare.net/rvarun7777/final-seminar-46117193
* http://www.springer.com/cda/content/document/cda_downloaddocument/9783642147029-c1.pdf
* http://www.streamingmedia.com/Articles/Editorial/Featured-Articles/A-Progress-Report-The-Alliance-for-Open-Media-and-the-AV1-Codec-110383.aspx
* http://www.streamingmediaglobal.com/Articles/ReadArticle.aspx?ArticleID=116505&PageNum=1
* http://yumichan.net/video-processing/video-compression/introduction-to-h264-nal-unit/
* https://cardinalpeak.com/blog/the-h-264-sequence-parameter-set/
* https://cardinalpeak.com/blog/worlds-smallest-h-264-encoder/
* https://codesequoia.wordpress.com/category/video/
* https://developer.apple.com/library/content/technotes/tn2224/_index.html
* https://en.wikibooks.org/wiki/MeGUI/x264_Settings
* https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming
* https://en.wikipedia.org/wiki/AOMedia_Video_1
* https://en.wikipedia.org/wiki/Chroma_subsampling#/media/File:Colorcomp.jpg
* https://en.wikipedia.org/wiki/Cone_cell
* https://en.wikipedia.org/wiki/File:H.264_block_diagram_with_quality_score.jpg
* https://en.wikipedia.org/wiki/Inter_frame
* https://en.wikipedia.org/wiki/Intra-frame_coding
* https://en.wikipedia.org/wiki/Photoreceptor_cell
* https://en.wikipedia.org/wiki/Pixel_aspect_ratio
* https://en.wikipedia.org/wiki/Presentation_timestamp
* https://en.wikipedia.org/wiki/Rod_cell
* https://it.wikipedia.org/wiki/File:Pixel_geometry_01_Pengo.jpg
* https://leandromoreira.com.br/2016/10/09/how-to-measure-video-quality-perception/
* https://sites.google.com/site/linuxencoding/x264-ffmpeg-mapping
* https://softwaredevelopmentperestroika.wordpress.com/2014/02/11/image-processing-with-python-numpy-scipy-image-convolution/
* https://tools.ietf.org/html/draft-fuldseth-netvc-thor-03
* https://www.encoding.com/android/
* https://www.encoding.com/http-live-streaming-hls/
* https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm
* https://www.lifewire.com/cmos-image-sensor-493271
* https://www.linkedin.com/pulse/brief-history-video-codecs-yoav-nativ
* https://www.linkedin.com/pulse/video-streaming-methodology-reema-majumdar
* https://www.vcodex.com/h264avc-intra-precition/
* https://www.youtube.com/watch?v=9vgtJJ2wwMA
* https://www.youtube.com/watch?v=LFXN9PiOGtY
* https://www.youtube.com/watch?v=Lto-ajuqW3w&list=PLzH6n4zXuckpKAj1_88VS-8Z6yn9zX_P6
* https://www.youtube.com/watch?v=LWxu4rkZBLw
* https://web.stanford.edu/class/ee398a/handouts/lectures/EE398a_MotionEstimation_2012.pdf
* https://sander.saares.eu/categories/drm-is-not-a-black-box/